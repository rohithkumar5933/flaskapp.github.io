{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdbf799-5929-4fbf-85f8-eef53bc1471c",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The main purpose of this project is for the submission of Machine Learning for Developers project Assignment. Secondly i have chosen this dataset as to how is the mobile specifications impact the price range of the mobile phones.\n",
    "## DataSet Information \n",
    "At the same time, this project will help to also find out and solve bob's problem. \n",
    "Bob has started his own mobile company. He wants to give tough fight to big companies like Apple,Samsung etc. He does not know how to estimate price of mobiles his company creates. In this competitive mobile phone market you cannot simply assume things. To solve this problem he collects sales data of mobile phones of various companies.Bob wants to find out some relation between features of a mobile phone(eg:- RAM,Internal Memory etc) and its selling price. But he is not so good at Machine Learning. So he needs your help to solve this problem.(context from kaggle.com) \n",
    "\n",
    "In this problem you do not have to predict actual price but a price range indicating how high the price is.\n",
    "\n",
    "##### This is a standard supervised classification problem. A classification problem where we have to predict the price range of mobile falls in 0 (low cost) ,1 (medium) ,2 (high),3 (very high) \n",
    "\n",
    "#### Below is the dataset attributes with description - Data Dictionary\n",
    "| Column | Description |\n",
    "| --- | --- |\n",
    "| battery_power |  Total energy a battery can store in one time measured in mAh |\n",
    "| blue |  Has bluetooth or not |\n",
    "| clock_speed |  speed at which microprocessor executes instructions |\n",
    "| dual_sim |  Has dual sim support or not |\n",
    "| fc |  Front Camera mega pixels |\n",
    "| four_g |  Has 4G or not |\n",
    "| int_memory |  Internal Memory in Gigabytes |\n",
    "| m_dep |  Mobile Depth in cm |\n",
    "| mobile_wt |  Weight of mobile phone |\n",
    "| n_cores |  Number of cores of processor |\n",
    "| pc |  Primary Camera mega pixels |\n",
    "| px_height |  Pixel Resolution Height |\n",
    "| px_width |  Pixel Resolution Width |\n",
    "| ram |  Random Access Memory in Mega Bytes |\n",
    "| sc_h |  Screen Height of mobile in cm |\n",
    "| sc_w |  Screen Width of mobile in cm |\n",
    "| talk_time |  longest time that a single battery charge will last when you are |\n",
    "| three_g |  Has 3G or not |\n",
    "| touch_screen |  Has touch screen or not |\n",
    "| wifi |  Has wifi or not |\n",
    "| price_range |  This is the target variable with value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost).|\n",
    "     \n",
    "     This dataset was found in Kaggle.com.\n",
    "   \n",
    "  Source:  The direct link to the csv file is : https://www.kaggle.com/iabhishekofficial/mobile-price-classification?select=train.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be5b34-8c7d-4afd-bca1-1312a50bbc68",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import Modules & Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18729fa3-e185-4bdb-b1fc-d284359afd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#loading the dataset\n",
    "\n",
    "data_table = pd.read_csv(\"data/dataset.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce52941-61ba-4231-b358-02f76e26b87a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:darkblue\"> Data Exploration & Preprossing Data </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a7e2c-71f3-4481-bf84-a21c3da917d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. Lets explore the dataset first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fe5bff-c518-49fe-8e89-56e6c4ae3861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0            842     0          2.2         0   1       0           7    0.6   \n",
       "1           1021     1          0.5         1   0       1          53    0.7   \n",
       "2            563     1          0.5         1   2       1          41    0.9   \n",
       "3            615     1          2.5         0   0       0          10    0.8   \n",
       "4           1821     1          1.2         0  13       1          44    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        188        2  ...         20       756  2549     9     7         19   \n",
       "1        136        3  ...        905      1988  2631    17     3          7   \n",
       "2        145        5  ...       1263      1716  2603    11     2          9   \n",
       "3        131        6  ...       1216      1786  2769    16     8         11   \n",
       "4        141        2  ...       1208      1212  1411     8     2         15   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        0             0     1            1  \n",
       "1        1             1     0            2  \n",
       "2        1             1     0            2  \n",
       "3        1             0     0            2  \n",
       "4        1             1     0            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d4fa11f-78e9-4921-a0a1-c51de952969d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n",
       "       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n",
       "       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n",
       "       'touch_screen', 'wifi', 'price_range'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summarise of all the columns we have: \n",
    "data_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e45a89c-2163-4307-97db-b61db51f69b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows the number of rows and columns\n",
    "data_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9afb53-5386-4600-bfff-b97eb7233648",
   "metadata": {},
   "source": [
    "We have 2000 rows and 21 columns. The last column is our target value, the value that we want to predict. In this project we will be predicting the range of mobile prices using the list of mobile phones specifications by applying some machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607c7c4d-1660-472d-9b6e-d24650364b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1238.518500</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>1.522250</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>4.309500</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>32.046500</td>\n",
       "      <td>0.501750</td>\n",
       "      <td>140.249000</td>\n",
       "      <td>4.520500</td>\n",
       "      <td>...</td>\n",
       "      <td>645.108000</td>\n",
       "      <td>1251.515500</td>\n",
       "      <td>2124.213000</td>\n",
       "      <td>12.306500</td>\n",
       "      <td>5.767000</td>\n",
       "      <td>11.011000</td>\n",
       "      <td>0.761500</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>439.418206</td>\n",
       "      <td>0.5001</td>\n",
       "      <td>0.816004</td>\n",
       "      <td>0.500035</td>\n",
       "      <td>4.341444</td>\n",
       "      <td>0.499662</td>\n",
       "      <td>18.145715</td>\n",
       "      <td>0.288416</td>\n",
       "      <td>35.399655</td>\n",
       "      <td>2.287837</td>\n",
       "      <td>...</td>\n",
       "      <td>443.780811</td>\n",
       "      <td>432.199447</td>\n",
       "      <td>1084.732044</td>\n",
       "      <td>4.213245</td>\n",
       "      <td>4.356398</td>\n",
       "      <td>5.463955</td>\n",
       "      <td>0.426273</td>\n",
       "      <td>0.500116</td>\n",
       "      <td>0.500076</td>\n",
       "      <td>1.118314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>501.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>851.750000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>282.750000</td>\n",
       "      <td>874.750000</td>\n",
       "      <td>1207.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1226.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1247.000000</td>\n",
       "      <td>2146.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1615.250000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>947.250000</td>\n",
       "      <td>1633.000000</td>\n",
       "      <td>3064.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1998.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "      <td>3998.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       battery_power       blue  clock_speed     dual_sim           fc  \\\n",
       "count    2000.000000  2000.0000  2000.000000  2000.000000  2000.000000   \n",
       "mean     1238.518500     0.4950     1.522250     0.509500     4.309500   \n",
       "std       439.418206     0.5001     0.816004     0.500035     4.341444   \n",
       "min       501.000000     0.0000     0.500000     0.000000     0.000000   \n",
       "25%       851.750000     0.0000     0.700000     0.000000     1.000000   \n",
       "50%      1226.000000     0.0000     1.500000     1.000000     3.000000   \n",
       "75%      1615.250000     1.0000     2.200000     1.000000     7.000000   \n",
       "max      1998.000000     1.0000     3.000000     1.000000    19.000000   \n",
       "\n",
       "            four_g   int_memory        m_dep    mobile_wt      n_cores  ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
       "mean      0.521500    32.046500     0.501750   140.249000     4.520500  ...   \n",
       "std       0.499662    18.145715     0.288416    35.399655     2.287837  ...   \n",
       "min       0.000000     2.000000     0.100000    80.000000     1.000000  ...   \n",
       "25%       0.000000    16.000000     0.200000   109.000000     3.000000  ...   \n",
       "50%       1.000000    32.000000     0.500000   141.000000     4.000000  ...   \n",
       "75%       1.000000    48.000000     0.800000   170.000000     7.000000  ...   \n",
       "max       1.000000    64.000000     1.000000   200.000000     8.000000  ...   \n",
       "\n",
       "         px_height     px_width          ram         sc_h         sc_w  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    645.108000  1251.515500  2124.213000    12.306500     5.767000   \n",
       "std     443.780811   432.199447  1084.732044     4.213245     4.356398   \n",
       "min       0.000000   500.000000   256.000000     5.000000     0.000000   \n",
       "25%     282.750000   874.750000  1207.500000     9.000000     2.000000   \n",
       "50%     564.000000  1247.000000  2146.500000    12.000000     5.000000   \n",
       "75%     947.250000  1633.000000  3064.500000    16.000000     9.000000   \n",
       "max    1960.000000  1998.000000  3998.000000    19.000000    18.000000   \n",
       "\n",
       "         talk_time      three_g  touch_screen         wifi  price_range  \n",
       "count  2000.000000  2000.000000   2000.000000  2000.000000  2000.000000  \n",
       "mean     11.011000     0.761500      0.503000     0.507000     1.500000  \n",
       "std       5.463955     0.426273      0.500116     0.500076     1.118314  \n",
       "min       2.000000     0.000000      0.000000     0.000000     0.000000  \n",
       "25%       6.000000     1.000000      0.000000     0.000000     0.750000  \n",
       "50%      11.000000     1.000000      1.000000     1.000000     1.500000  \n",
       "75%      16.000000     1.000000      1.000000     1.000000     2.250000  \n",
       "max      20.000000     1.000000      1.000000     1.000000     3.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe dataset\n",
    "data_table.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b8fac-e6dc-497a-abee-a04af04845d6",
   "metadata": {},
   "source": [
    "Not all column statistics makes some sense, certain things like average battery power a mobile phone has, or the maximum GB of internal memory makes sense. But some columns like blutooth, the statistics wont help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62f2a6b-95b4-4c0a-9250-b877d4c9a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   battery_power  2000 non-null   int64  \n",
      " 1   blue           2000 non-null   int64  \n",
      " 2   clock_speed    2000 non-null   float64\n",
      " 3   dual_sim       2000 non-null   int64  \n",
      " 4   fc             2000 non-null   int64  \n",
      " 5   four_g         2000 non-null   int64  \n",
      " 6   int_memory     2000 non-null   int64  \n",
      " 7   m_dep          2000 non-null   float64\n",
      " 8   mobile_wt      2000 non-null   int64  \n",
      " 9   n_cores        2000 non-null   int64  \n",
      " 10  pc             2000 non-null   int64  \n",
      " 11  px_height      2000 non-null   int64  \n",
      " 12  px_width       2000 non-null   int64  \n",
      " 13  ram            2000 non-null   int64  \n",
      " 14  sc_h           2000 non-null   int64  \n",
      " 15  sc_w           2000 non-null   int64  \n",
      " 16  talk_time      2000 non-null   int64  \n",
      " 17  three_g        2000 non-null   int64  \n",
      " 18  touch_screen   2000 non-null   int64  \n",
      " 19  wifi           2000 non-null   int64  \n",
      " 20  price_range    2000 non-null   int64  \n",
      "dtypes: float64(2), int64(19)\n",
      "memory usage: 328.2 KB\n"
     ]
    }
   ],
   "source": [
    "#check if there is any null values in our dataset\n",
    "data_table.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680282e-8415-445b-843f-58b3e55945bb",
   "metadata": {},
   "source": [
    "We do not have any null values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8da4111-ce88-4495-94fe-80df6b114580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_range\n",
      "0    500\n",
      "1    500\n",
      "2    500\n",
      "3    500\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_table.groupby('price_range').size())\n",
    "#data set is balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "267de7df-6d9b-49fe-be1f-d5b96cabd47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#scatter plot\n",
    "# lets see if ram increases , does the price range increase too??\n",
    "data_table.plot(kind='scatter',x='price_range', y='ram', sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5796937a-6586-4cf8-b9bf-9bfe5e68cc77",
   "metadata": {},
   "source": [
    "Above we can see that <span style=\"color:green\"> ram is directly proportional to price range.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a96da5b-ea26-4be9-b25b-6a8871175da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x936 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "fig = plt.subplots (figsize = (11, 13))\n",
    "sns.barplot(x='price_range',y='battery_power',data=data_table,ci= None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02819c-77c8-4eed-bfc2-22fb3d16cfca",
   "metadata": {},
   "source": [
    "##### lastly lets see iff internal memory storage affects the price of a mobile phone ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2012dade-e218-416b-bed9-912f52ee0164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='price_range', ylabel='int_memory'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x936 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "fig = plt.subplots (figsize = (11, 13))\n",
    "sns.lineplot(x=\"price_range\", y=\"int_memory\",\n",
    "             data=data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ccdd7f-1513-4b9a-a745-4c7ed69f92a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find if there is any high correlation between features to price_range\n",
    "#find the most influential feature between features\n",
    "import seaborn as sns\n",
    "fig = plt.subplots (figsize = (20, 10))\n",
    "sns.heatmap(data_table.corr(),annot=True, annot_kws={\"size\": 5},cmap = \"Purples\")\n",
    "plt.title('Correlations between features')\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95b56b-622f-42d2-8821-049a1b281614",
   "metadata": {},
   "source": [
    "=> we can summarise that not much features have high correlation to price range except ram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d271e04-ead2-4bbe-9265-38f2a4090628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "battery_power    0.200723\n",
       "blue             0.020573\n",
       "clock_speed     -0.006606\n",
       "dual_sim         0.017444\n",
       "fc               0.021998\n",
       "four_g           0.014772\n",
       "int_memory       0.044435\n",
       "m_dep            0.000853\n",
       "mobile_wt       -0.030302\n",
       "n_cores          0.004399\n",
       "pc               0.033599\n",
       "px_height        0.148858\n",
       "px_width         0.165818\n",
       "ram              0.917046\n",
       "sc_h             0.022986\n",
       "sc_w             0.038711\n",
       "talk_time        0.021859\n",
       "three_g          0.023611\n",
       "touch_screen    -0.030411\n",
       "wifi             0.018785\n",
       "price_range      1.000000\n",
       "Name: price_range, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.corr()['price_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ac0fd4-5008-4b33-b883-09271f303eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>794</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>106</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1222</td>\n",
       "      <td>1890</td>\n",
       "      <td>668</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1965</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>915</td>\n",
       "      <td>1965</td>\n",
       "      <td>2032</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1911</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.7</td>\n",
       "      <td>108</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>868</td>\n",
       "      <td>1632</td>\n",
       "      <td>3057</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0.1</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>336</td>\n",
       "      <td>670</td>\n",
       "      <td>869</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.9</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>754</td>\n",
       "      <td>3919</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
       "0               842     0          2.2         0   1       0           7   \n",
       "1              1021     1          0.5         1   0       1          53   \n",
       "2               563     1          0.5         1   2       1          41   \n",
       "3               615     1          2.5         0   0       0          10   \n",
       "4              1821     1          1.2         0  13       1          44   \n",
       "...             ...   ...          ...       ...  ..     ...         ...   \n",
       "1995            794     1          0.5         1   0       1           2   \n",
       "1996           1965     1          2.6         1   0       0          39   \n",
       "1997           1911     0          0.9         1   1       1          36   \n",
       "1998           1512     0          0.9         0   4       1          46   \n",
       "1999            510     1          2.0         1   5       1          45   \n",
       "\n",
       "      m_dep  mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  \\\n",
       "0       0.6        188        2  ...         20       756  2549     9     7   \n",
       "1       0.7        136        3  ...        905      1988  2631    17     3   \n",
       "2       0.9        145        5  ...       1263      1716  2603    11     2   \n",
       "3       0.8        131        6  ...       1216      1786  2769    16     8   \n",
       "4       0.6        141        2  ...       1208      1212  1411     8     2   \n",
       "...     ...        ...      ...  ...        ...       ...   ...   ...   ...   \n",
       "1995    0.8        106        6  ...       1222      1890   668    13     4   \n",
       "1996    0.2        187        4  ...        915      1965  2032    11    10   \n",
       "1997    0.7        108        8  ...        868      1632  3057     9     1   \n",
       "1998    0.1        145        5  ...        336       670   869    18    10   \n",
       "1999    0.9        168        6  ...        483       754  3919    19     4   \n",
       "\n",
       "      talk_time  three_g  touch_screen  wifi  price_range  \n",
       "0            19        0             0     1            1  \n",
       "1             7        1             1     0            2  \n",
       "2             9        1             1     0            2  \n",
       "3            11        1             0     0            2  \n",
       "4            15        1             1     0            1  \n",
       "...         ...      ...           ...   ...          ...  \n",
       "1995         19        1             1     0            0  \n",
       "1996         16        1             1     1            2  \n",
       "1997          5        1             1     0            3  \n",
       "1998         19        1             1     1            0  \n",
       "1999          2        1             1     1            3  \n",
       "\n",
       "[2000 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5048491d-f838-4b6d-ae2d-04595b2b8b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_table.hist(bins=50,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af066127-bf4a-4eda-89cd-5f24dba5f6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI0AAAq+CAYAAAAcGaBqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAB5WklEQVR4nOzdX6jn913n8dd7k1AD2rXiKDETmCIRps1iSkMIOBcblW2wy6ZeCAms7cVApNSiICypc7F6cSAX/lm6YKHuSNNdNyWg0GAsbO1OLwZqw9StjemxGGjWxoZ2VlAjSEjjZy/mWzzT1+nMmeYkMzqPB/w43/P+fT/n9/ndHZ78ft/vrLUCAAAAAHv9q6u9AQAAAACuPaIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUG682hu4nO///u9fx44du9rbAAAAAPgX43Of+9z/W2sdudQ513w0OnbsWM6dO3e1twEAAADwL8bM/N/LnePraQAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAOXA0mpkbZub/zMwfbL9/38x8cmb+Yvv5pj3nfmBmnp2ZL83MO/bM3z4zT2/PfXBm5nDfDgAAAACH4Uo+afQLSXb3/P5wkk+ttW5P8qnt98zMW5I8kOStSe5L8lszc8O25kNJHkpy+/a471XtHgAAAIDXxIGi0cwcTfLOJP9tz/j+JI9ux48medee+cfWWi+ttb6c5Nkkd8/MLUneuNb6zFprJfnonjUAAAAAXEMO+kmj/5LkPyX5xz2zH1xrvZAk288f2Oa3JvnKnvOe32a3bsffOi8z89DMnJuZc+fPnz/gFgEAAAA4LDde7oSZ+fdJvr7W+tzM/NsD/M39rlO0LjHv4VofTvLhJLnrrrv2PQcAAK4nxx5+8jte+9wj7zzEnQBwvbhsNEryY0n+w8z8VJLvSvLGmfkfSb42M7estV7Yvnr29e3855Pctmf90SRf3eZH95kDAACXcanwc+zhJ4UhAA7dZb+ettb6wFrr6FrrWC5c4Pp/r7X+Y5InkrxnO+09ST6+HT+R5IGZecPMvDkXLnj91PYVthdn5p7trmnv3rMGAAAAgGvIQT5p9O08kuTxmTmZ5C+T/EySrLWemZnHk3wxyTeSvG+t9cq25r1JPpLk5iSf2B4AAAAAXGOuKBqttT6d5NPb8V8n+Ylvc95Okp195ueS3HGlmwQAAADg9XXQu6cBAAAAcB0RjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgXDYazcx3zcxTM/OnM/PMzPzqNv+Vmfmrmfn89vipPWs+MDPPzsyXZuYde+Zvn5mnt+c+ODPz2rwtAAAAAF6NGw9wzktJfnyt9fczc1OSszPzie2531xr/drek2fmLUkeSPLWJD+U5I9m5kfWWq8k+VCSh5L8cZI/THJfkk8EAAAAgGvKZT9ptC74++3Xm7bHusSS+5N8bK310lrry0meTXL3zNyS5I1rrc+stVaSjyZ516vaPQAAAACviQNd02hmbpiZzyf5epJPrrU+uz318zPzhZn5nZl50za7NclX9ix/fpvduh1/63y/13toZs7NzLnz588f/N0AAAAAcCgOFI3WWq+ste5McjQXPjV0Ry581eyHk9yZ5IUkv76dvt91itYl5vu93ofXWnette46cuTIQbYIAAAAwCG6orunrbX+Jsmnk9y31vraFpP+MclvJ7l7O+35JLftWXY0yVe3+dF95gAAAABcYw5y97QjM/O92/HNSX4yyZ9v1yj6pp9O8mfb8RNJHpiZN8zMm5PcnuSptdYLSV6cmXu2u6a9O8nHD++tAAAAAHBYDnL3tFuSPDozN+RCZHp8rfUHM/PfZ+bOXPiK2XNJfi5J1lrPzMzjSb6Y5BtJ3rfdOS1J3pvkI0luzoW7prlzGgAAAMA16LLRaK31hSRv22f+s5dYs5NkZ5/5uSR3XOEeAQAAAHidXdE1jQAAAAC4PohGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoN17tDQAAAP/kR3/1f+Vv/+HlK1537OEnr+j8f33zTfnT//zvrvh1ALh+iEYAAHAN+dt/eDnPPfLO1/x1rjQyAXD98fU0AAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiXjUYz810z89TM/OnMPDMzv7rNv29mPjkzf7H9fNOeNR+YmWdn5ksz844987fPzNPbcx+cmXlt3hYAAAAAr8ZBPmn0UpIfX2v9aJI7k9w3M/ckeTjJp9Zatyf51PZ7ZuYtSR5I8tYk9yX5rZm5YftbH0ryUJLbt8d9h/dWAAAAADgsl41G64K/3369aXusJPcneXSbP5rkXdvx/Uk+ttZ6aa315STPJrl7Zm5J8sa11mfWWivJR/esAQAAAOAacqBrGs3MDTPz+SRfT/LJtdZnk/zgWuuFJNl+/sB2+q1JvrJn+fPb7Nbt+Fvn+73eQzNzbmbOnT9//greDgAAAACH4UDRaK31ylrrziRHc+FTQ3dc4vT9rlO0LjHf7/U+vNa6a61115EjRw6yRQAAAAAO0RXdPW2t9TdJPp0L1yL62vaVs2w/v76d9nyS2/YsO5rkq9v86D5zAAAAAK4xB7l72pGZ+d7t+OYkP5nkz5M8keQ922nvSfLx7fiJJA/MzBtm5s25cMHrp7avsL04M/dsd0179541AAAAAFxDbjzAObckeXS7A9q/SvL4WusPZuYzSR6fmZNJ/jLJzyTJWuuZmXk8yReTfCPJ+9Zar2x/671JPpLk5iSf2B4AAAAAXGMuG43WWl9I8rZ95n+d5Ce+zZqdJDv7zM8ludT1kAAAAAC4BlzRNY0AAAAAuD6IRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQLhuNZua2mTkzM7sz88zM/MI2/5WZ+auZ+fz2+Kk9az4wM8/OzJdm5h175m+fmae35z44M/PavC0AAAAAXo0bD3DON5L80lrrT2bme5J8bmY+uT33m2utX9t78sy8JckDSd6a5IeS/NHM/Mha65UkH0ryUJI/TvKHSe5L8onDeSsAAAAAHJbLftJorfXCWutPtuMXk+wmufUSS+5P8rG11ktrrS8neTbJ3TNzS5I3rrU+s9ZaST6a5F2v9g0AAAAAcPiu6JpGM3MsyduSfHYb/fzMfGFmfmdm3rTNbk3ylT3Lnt9mt27H3zrf73UemplzM3Pu/PnzV7JFAAAAAA7BgaPRzHx3kt9L8otrrb/Lha+a/XCSO5O8kOTXv3nqPsvXJeY9XOvDa6271lp3HTly5KBbBAAAAOCQHCgazcxNuRCMfnet9ftJstb62lrrlbXWPyb57SR3b6c/n+S2PcuPJvnqNj+6zxwAAACAa8xB7p42SU4n2V1r/cae+S17TvvpJH+2HT+R5IGZecPMvDnJ7UmeWmu9kOTFmbln+5vvTvLxQ3ofAAAAAByig9w97ceS/GySp2fm89vsl5M8ODN35sJXzJ5L8nNJstZ6ZmYeT/LFXLjz2vu2O6clyXuTfCTJzblw1zR3TgMAAAC4Bl02Gq21zmb/6xH94SXW7CTZ2Wd+LskdV7JBAAAAAF5/V3T3NAAAAACuD6IRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgHLj1d4AAADwT77n+MP5N48+/Dq8TpK88zV/HQD++RKNAADgGvLi7iN57pHXPuYce/jJ1/w1APjnzdfTAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACActloNDO3zcyZmdmdmWdm5he2+ffNzCdn5i+2n2/as+YDM/PszHxpZt6xZ/72mXl6e+6DMzOvzdsCAAAA4NU4yCeNvpHkl9Zax5Pck+R9M/OWJA8n+dRa6/Ykn9p+z/bcA0nemuS+JL81Mzdsf+tDSR5Kcvv2uO8Q3wsAAAAAh+Sy0Wit9cJa60+24xeT7Ca5Ncn9SR7dTns0ybu24/uTfGyt9dJa68tJnk1y98zckuSNa63PrLVWko/uWQMAAADANeSKrmk0M8eSvC3JZ5P84FrrheRCWEryA9tptyb5yp5lz2+zW7fjb53v9zoPzcy5mTl3/vz5K9kiAAAAAIfgwNFoZr47ye8l+cW11t9d6tR9ZusS8x6u9eG11l1rrbuOHDly0C0CAAAAcEhuPMhJM3NTLgSj311r/f42/trM3LLWemH76tnXt/nzSW7bs/xokq9u86P7zF9Xxx5+8jta99wj7zzknQAA8Fr5Tv/nS/zfBwDfdNlotN3h7HSS3bXWb+x56okk70nyyPbz43vm/3NmfiPJD+XCBa+fWmu9MjMvzsw9ufD1tncn+a+H9k4O6Nv9E3Ds4Sf9gwAA8C/Epf6v838fABzMQT5p9GNJfjbJ0zPz+W32y7kQix6fmZNJ/jLJzyTJWuuZmXk8yRdz4c5r71trvbKte2+SjyS5OckntgcAAAAA15jLRqO11tnsfz2iJPmJb7NmJ8nOPvNzSe64kg0CAAAA8Pq7orunAQAAAHB9EI0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAEB57LHHcscdd+SGG27IHXfckccee+xqbwkAeJ3deLU3AADAteWxxx7LqVOncvr06Zw4cSJnz57NyZMnkyQPPvjgVd4dAPB68UkjAAAusrOzk9OnT+fee+/NTTfdlHvvvTenT5/Ozs7O1d4aAPA6Eo0AALjI7u5uTpw4cdHsxIkT2d3dvUo7AgCuBtEIAICLHD9+PGfPnr1odvbs2Rw/fvwq7QgAuBpEIwAALnLq1KmcPHkyZ86cycsvv5wzZ87k5MmTOXXq1NXeGgDwOnIhbAAALvLNi12///3vz+7ubo4fP56dnR0XwQaA64xPGgEAAABQfNIIAICLPPbYYzl16lROnz6dEydO5OzZszl58mSS+LQRAFxHfNIIAICL7Ozs5PTp07n33ntz00035d57783p06ezs7NztbcGALyORCMAAC6yu7ubEydOXDQ7ceJEdnd3r9KOAICrQTQCAOAix48fz9mzZy+anT17NsePH79KOwIArgbRCACAi5w6dSonT57MmTNn8vLLL+fMmTM5efJkTp06dbW3BgC8jlwIGwCAi3zzYtfvf//7s7u7m+PHj2dnZ8dFsAHgOiMaAQBQHnzwQZEIAK5zvp4GAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAACUG6/2BgAAuLqOPfzkd7TuuUfeecg7AQCuJaIRAMB17lLx59jDT4pDAHCd8vU0AAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAAAAFNEIAAAAgCIaAQAAAFBEIwAAAACKaAQAAABAEY0AAAAAKKIRAAAAAEU0AgAAAKCIRgAAAAAU0QgAAACAIhoBAAAAUEQjAAAAAIpoBAAAAEARjQAAAAAoohEAAAAARTQCAAAAoIhGAAAAABTRCAAAAIAiGgEAAABQRCMAAAAAimgEAAAAQBGNAAAAACiiEQAAAABFNAIAAACgiEYAAPD/27v3OMnyur7/74/MBhddFhaIPwSXIQYVcAVlQVAxixIgbBLgJwhGBUSDGG/4EOP40who0DEkknhBRUJ2vaEEgqKDAqJcglx2QWB3QZTAACsEEHQFWXGB7++Pc5qp6U91T3dPX2eez8djHlN9uqrOOVWnzuVVp6oBgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKA5tNcTAAAAW3WXJ784111/w6Zvd/jIsU1d//xzz8mbnni/TY8HAA4y0QgAgAPruutvyPGjl+74eDYbmQDgTODjaQAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0h/Z6AnbCXZ784lx3/Q2bvt3hI8c2fZvzzz0nb3ri/TZ9OwAAAID97IyMRtddf0OOH710V8a1ldAEAAAAsN/5eBoAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQHPKaFRVz6qqD1TV1QvDnlRVf1lVb5z/PXDhdz9UVW+vqrdV1f0Xht+tqq6af/czVVXbPzsAAAAAbIeNnGl0WZIHLBn+tDHGXed/L0ySqrpTkkckufN8m6dX1Y3m6/9CkscmucP8b9l9AgAAALAPnDIajTFekeTDG7y/ByX5zTHGx8cY70zy9iT3qKpbJ7npGOPVY4yR5FeSPHiL0wwAAADADjud7zT6rqp68/zxtZvPw26T5D0L17l2Hnab+fLq4UtV1WOr6sqquvKDH/zgaUwiAAAAAFux1Wj0C0k+P8ldk7wvyX+Zhy/7nqKxzvClxhjPGGNcPMa4+Fa3utUWJxEAAACArdpSNBpjvH+M8ckxxqeS/HKSe8y/ujbJ5y1c9bZJ3jsPv+2S4QAAAADsQ1uKRvN3FK14SJKVv6z2giSPqKobV9XtM33h9evGGO9L8pGquuf8V9MemeR3TmO6AQAAANhBh051hap6dpJLktyyqq5N8sQkl1TVXTN9xOx4km9PkjHGNVX1nCRvSfKJJN85xvjkfFffkekvsZ2b5PfnfwAAAADsQ6eMRmOMb1gy+L+vc/2nJHnKkuFXJvniTU0dAAAAAHvidP56GgAAAABnKNEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKA5tNcTAAAAW3XeHY/kosuP7MJ4kuTSHR8PAOwnohEAAAfWR956NMeP7nzMOXzk2I6PAwD2Gx9PAwAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAmkN7PQE74bw7HslFlx/ZpXElyaW7Mi4AAACA3XJGRqOPvPVojh/dnZBz+MixXRkPAAAAwG7y8TQAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAmkN7PQEAAMDJDh85tuPjOP/cc3Z8HAAcbKIRAADsI8ePXrrp2xw+cmxLtwOA9fh4GgAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0JwyGlXVs6rqA1V19cKwC6rqJVX1F/P/N1/43Q9V1dur6m1Vdf+F4Xerqqvm3/1MVdX2zw4AAAAA22EjZxpdluQBq4YdSfLSMcYdkrx0/jlVdackj0hy5/k2T6+qG823+YUkj01yh/nf6vsEAAAAYJ84ZTQaY7wiyYdXDX5Qksvny5cnefDC8N8cY3x8jPHOJG9Pco+qunWSm44xXj3GGEl+ZeE2AAAAAOwzW/1Oo88ZY7wvSeb///E8/DZJ3rNwvWvnYbeZL68evlRVPbaqrqyqKz/4wQ9ucRIBAAAA2Krt/iLsZd9TNNYZvtQY4xljjIvHGBff6la32raJAwAAAGBjthqN3j9/5Czz/x+Yh1+b5PMWrnfbJO+dh992yXAAAAAA9qGtRqMXJHnUfPlRSX5nYfgjqurGVXX7TF94/br5I2wfqap7zn817ZELtwEAAABgnzl0qitU1bOTXJLkllV1bZInJjma5DlV9a1J3p3kYUkyxrimqp6T5C1JPpHkO8cYn5zv6jsy/SW2c5P8/vwPAAAAgH3olNFojPENa/zqa9e4/lOSPGXJ8CuTfPGmpg4AAACAPbHdX4QNAAAAwBlANAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaA7t9QQAALA77vLkF+e662/Y9O0OHzm2qeuff+45edMT77fp8QAA+4toBABwlrju+hty/OilOz6ezUYmAGB/8vE0AAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgObTXEwAAAKfj8JFjOz6O8889Z8fHAQD7jWgEAMCBdfzopZu+zeEjx7Z0OwA42/h4GgAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSH9noCdsrhI8d2ZTznn3vOrowHAAAAYDedkdHo+NFLN32bw0eObel2AAAAAGciH08DAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACA5tBeTwAAALvjvDseyUWXH9mF8STJpTs+HgBgZ4lGAABniY+89WiOH935mHP4yLEdHwcAsPN8PA0AAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoDmtaFRVx6vqqqp6Y1VdOQ+7oKpeUlV/Mf9/84Xr/1BVvb2q3lZV9z/diQcAAABgZ2zHmUb3GWPcdYxx8fzzkSQvHWPcIclL559TVXdK8ogkd07ygCRPr6obbcP4AQAAANhmO/HxtAcluXy+fHmSBy8M/80xxsfHGO9M8vYk99iB8QMAAABwmg6d5u1HkhdX1UjyS2OMZyT5nDHG+5JkjPG+qvrH83Vvk+Q1C7e9dh7WVNVjkzw2SS688MLTnMSTHT5ybEu/O3700m2dDgAAds56+3Wn+r39PgCYnG40+soxxnvnMPSSqvqzda5bS4aNZVec49MzkuTiiy9eep2tshMAAHDms88HAKfvtD6eNsZ47/z/B5I8P9PHzd5fVbdOkvn/D8xXvzbJ5y3c/LZJ3ns64wcAAABgZ2w5GlXVZ1XVeSuXk9wvydVJXpDkUfPVHpXkd+bLL0jyiKq6cVXdPskdkrxuq+MHAAAAYOeczsfTPifJ86tq5X5+Y4zxB1V1RZLnVNW3Jnl3koclyRjjmqp6TpK3JPlEku8cY3zytKYeAAAAgB2x5Wg0xnhHkrssGf6hJF+7xm2ekuQpWx0nAAAAALvjtL7TCAAAAIAzk2gEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAzaG9ngAAAHbP4SPHdnwc5597zo6PAwDYeaIRAMBZ4vjRSzd9m8NHjm3pdgDAwefjaQAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAzaG9ngAAAODUDh85tuXfHz966XZPDgBnAdEIAAAOAOEHgN3m42kAAAAANKIRAAAAAI1oBAAAAEAjGgEAAADQiEYAAAAANKIRAAAAAI1oBAAAAEAjGgEAAADQiEYAAAAANKIRAAAAAI1oBAAAAEAjGgEAAADQiEYAAAAANKIRAAAAAI1oBAAAAEAjGgEAAADQiEYAAAAANKIRAAAAAI1oBAAAAEAjGgEAAADQiEYAAAAANKIRAAAAAI1oBAAAAEAjGgEAAADQiEYAAAAANKIRAAAAAI1oBAAAAEAjGgEAAADQiEYAAAAANKIRAAAAAI1oBAAAAEAjGgEAAADQiEYAAAAANIf2egIAgMnhI8e2dLvjRy/d5ikBAADRCAD2jbXiz+Ejx4QhAAB2nY+nAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQCMaAQAAANCIRgAAAAA0ohEAAAAAjWgEAAAAQHNorycAAM4md3nyi3Pd9Tds+naHjxzb9G3OP/ecvOmJ99v07QAAIBGNAGBXXXf9DTl+9NJdGddWQhMAAKzw8TQAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoDu31BADA2eS8Ox7JRZcf2aVxJcmluzIuAADOPKIRAOyij7z1aI4f3Z2Qc/jIsV0ZDwAAZyYfTwMAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaA7t9QQAwNnm8JFjuzKe8889Z1fGAwDAmUk0AoBddPzopZu+zeEjx7Z0OwAAOB0+ngYAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAADNob2eAABgcvjIsS397vjRS3dicgAAOMuJRgCwT4g/AADsJz6eBgAAAEAjGgEAAADQiEYAAAAANKIRAAAAAI1oBAAAAEAjGgEAAADQiEYAAAAANKIRAAAAAM2hvZ4AAHbW4SPHtnzb40cv3cYpAQAADhLRCOAMt174OXzkmDAEAAAs5eNpAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0Bza6wkAYHvc5ckvznXX37Dp2x0+cmxT1z//3HPypifeb9PjAQAADhbRCOAMcd31N+T40Ut3fDybjUwAAMDB5ONpAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0Bza6wkAYHucd8cjuejyI7swniS5dMfHAwAA7C3RCOAM8ZG3Hs3xozsfcw4fObbj4wAAAPaej6cBAAAA0IhGAAAAADSiEQAAAACN7zQCtlVVtWFjjD2YEgAAAE6HM42AbbMsGK03HAAAgP3LmUbAtrvzne+cF77whXngAx+Ya665Zq8nB2BbbfUvCO7GXzcE2CnWfXB2Eo2AbXXhhRfm6quvTpJcffXVud3tbpd3v/vdezxVANtnvQOgw0eOOUCCTToTY8SZOE/WfXB2Eo2ALVu2Q/Tu91x70vB3v+fadl07FTtnqzupm3H+uefs+DgAOLPc5ckvznXX37Ct97nWNu/8c8/Jm554v20d12atta+z3+PK6TxPm90H2Q/PE3BqohGwZat3euqnkoxP5b0//ZDc9KH/MX/73B9JxqeWXpftt5XHeL/vvMJeceAE2+u662/Yte3NbryBcqbyPAGriUbAtrngggvy4Q9/ODfccEM+9OwfPGk4wEHiwAkAQDQCttGHPvSh3OIWt8iHP/zhTw+74IIL8qEPfWgPpwoA2Gvn3fFILrr8yC6NK0mcRQuwHUQjYFutBCIfewIOMge4sL0+8tajzt47AKz7gNVEIwBgxxzUvyDkABe2324t67v1Bxu2+t1nW3kcduu7z6z7gNVEI+CUdmunyJfBwsF0tv1VJGDzzsQ/1uC7z4Czwa5Ho6p6QJL/luRGSZ45xji629MAbM5u7RTZIYKD6Uw9cDrTzoqA/Wq919p6v9vroHSmfpTrbF/3VVUbNsbYgymB/WFXo1FV3SjJzyf550muTXJFVb1gjPGW3ZwO9r+D+nGGM9Vu7RT5bDscTGfigdNWtyf7/cwI2I8O6mvmTPwo19m+7lsWjFaGC0dsp9N5Te/2a223zzS6R5K3jzHekSRV9ZtJHpRENDoLXXT5RWv+bjoo2Mp9rn3QctWjrtranbJrO0XONNoZp3pc9/O7uBwMW12/nikHGcDZ62w6K2er+xMHcT2/GIjWCkmwEWsd8271eHe6z+XHvDt1vFu7WUyr6qFJHjDG+Lb5529O8uVjjO9adb3HJnlsklx44YV3e9e73rVr07hfrRdYdsJuBJbdDAS79R0YZ+LzlOzuDtFufVfJbj5XguXWnamvqbPJmXjmqHk6YT/PE+wlr6n940zdlzgT92XPxHna78dRVfX6McbF615nl6PRw5Lcf1U0uscY47vXus3FF188rrzyyt2aRAAAAM5CK2cVLTvTyMfTOBNtJBrt9sfTrk3yeQs/3zbJe3d5GgAAAGApH0mDEz5jl8d3RZI7VNXtq+ofJXlEkhfs8jQAAADASdY6m8hZRpzNdvVMozHGJ6rqu5K8KMmNkjxrjHHNbk4DAAAALCMQwcl2++NpGWO8MMkLd3u8AAAAAGzcbn88DQAAAIADQDQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoKkxxl5Pw7qq6oNJ3rULo7plkr/ahfHspjNxnpIzc77M08Fgng4G83RwnInzZZ4OBvN0MJing+NMnC/zdDCYp9NzuzHGrda7wr6PRrulqq4cY1y819Oxnc7EeUrOzPkyTweDeToYzNPBcSbOl3k6GMzTwWCeDo4zcb7M08Fgnnaej6cBAAAA0IhGAAAAADSi0QnP2OsJ2AFn4jwlZ+Z8maeDwTwdDObp4DgT58s8HQzm6WAwTwfHmThf5ulgME87zHcaAQAAANA40wgAAACARjQ6gKrqcFVdvWT4y6pq33zLOmefqvqeqnprVf36Xk8LcHBV1Y32ehoAgP2vql5YVTebL3/6WKSq/nVVHdnjyTsj7Gk0Wit+rHP9R1fV5y78/PiqusnOTB1bUVVPqqonbOF2l1TV7+3ENJ2OUy2jpzG/m1r2F273J5u9zS77d0keOMb4xr2ekPXs17i1kefXem/jqup4Vd1yr6fjVNZaj1TV51bVc+fLO7KOnNdF/2YLt3lrVf1yVV1TVS+uqnPXuO4/rao/rKo3VdUbqurza/LUqrq6qq6qqofP172kqv64qn4jyVVVdaP5eldU1Zur6tvn6926ql5RVW+c7+Pep/1AkKr66Cavf8qd8fWW2/20LquqZ1bVnZYMf3RV/dx8+cGL19nLN+rm19BZ+8bv4vPC5lTVzarq323geh+d/9/wtme+7lcs/Py4qnrk1qd2axbncb8dX2z08d/kfZ71r4cxxgPHGH8z//jpY5ExxgvGGEd3c1qq6seq6r67Oc7dcNA2OI9O8rkLPz8+yaZ2OA7Cu5dVdWgDVztUVZfPO9LPXb3jtbjzV1UPrarL5su3qqrnzTvhV1TVV27v1LOTxhhfcepr7Y2q+sUk/yTJC6rqP1TV/5gPCN9cVV+319O3ymnHrZ3Yad/g8/v4bHK9t1c2uC5jDWOM944xHrrDozmcZFPRaHaHJD8/xrhzkr9JstZr/Nfn690lyVckeV+S/zfJXZPcJcl9kzy1qm49X/8eSX54jHGnJN+a5Loxxt2T3D3Jv62q28/T+6Ixxsp9vHEL078t5oD2Z6u3x1V196r6kzmWva6qzturadwp27Az/vjsk3XZGOPbxhhvOcXVHpykhaXdshBrn57kDUn+e1VdOYfbJy9c73hV/URVvXr+/ZdV1Yuq6v9U1eP2avrPdFX1WVV1bH7NX11VD9/oeqCmsyS+ZL78p1X1o/PlH6+qb9vmSb1Zpn2gnXBJpvV8kmSM8YtjjF/ZoXGt52bZ5Dzu4vHhzbJzj/+O2evj56r691X1PfPlp1XVH82Xv7aqfm1e791y1bHI9+12UKuqG40xfnSM8Yc7cd/bfZ+bsR+iUYsfVfWjc9C4uqqeMR+cPTTJxUl+vaZ3F783U0D646r64ySpqvvNG8k3VNX/rKrPnocfn+/zfyc5UlVvWBl5Vd2hql6/1sTNt/2peWX/uqr6p/Pw21XVS+fpfmlVXVjTu6LvmKf3ZlX1qar66vn6r6zpHdfPqqpnzfP3p1X1oPn3j56n+XeTvHgDj9sXJnnGGONLkvxtNr4C+m9JnjbvhH9dkmdu8HZLVdUj58fgTVX1q6t+d9eqes38++dX1c3n4e2d51W3u/v82PyTNcb5z+Zl4I3z9c6r6Z2EV8zjeUtV/WLNB/TrLBd3q6qXV9Xra9qhuvXC8DdV1auTfOeS8f9wVb2tqv4w0/Nw0juO80rr+Hz58Pzcv2H+t6HoU1V3npe3N86P3x3m4Yvv/Ly8qp5TVX9eVUer6hvn21y1+jHdDWOMxyV5b5L7JPnsTAd7F83L6B/t9vSspU7eoHx/Vf32/Bi/pk7stJ105kdN66LD1XfaP2+NcXzr/Ly8rKazMTa0wVr1/L6spnXin9V0im3VtME8ab231v3UtN56/fxau8d8f++oqn89X2etszg2tGzVknXgPPyyqvrpefqeWlV/UVW3mn/3GVX19jrF2T914kD8mfNj/+tVdd+qetV8f/dY43a3qOmslz+tql9KUgu/+6aF19Qv1bzxnR+r/zK/Pl+6Mq2btZFprqoLli1vs7tU1R/N1/23C/e57KPIS7cja0zXqQ5Gjia59/y4fN8mZvmdY4w3zpdfnyk+rR73eUluM8Z4fpKMMf5+jPGxJF+V5NljjE+OMd6f5OWZolCSvG6M8c758v2SPLKq3pjktUlukSlWXZHkW6rqSUkuGmN8ZBPTvRNWb4+/K8lvJfneOZbdN8n1OzkBtTxenV/TtmplO/XslWVrnft5Sk3bv9dU1efMw5a+2VQnn4Xz+fNtrqjpXdbFs5Y+u7a4LtuOeauqr6+qn54vf29VvWNhmv/3fHlxG/4tNa37Xp5kZV6/Ism/zrROe2Od2MY+bF6v/HntzhlvX5jkV8YYX5rk+8cYFyf5kiT/bNX65D1jjHsleWWSy5I8NMk9k/zYLkzjhtRpRJbZ51bVH8zrzP+0axO+tgckee8Y4y5jjC9O8gfZ+HrgFZnWwzdN8onMy12mdeUrt3k6jyb5/Hk5ftq83XtDTdv3Nbclyfr751V1OMnjknzffN/3roV9qfk19rSa9tXfOt/X/5qfv/+4cD9Lt9VbncckT82SddA8rsXjw4fVJo8X1nmM3jzfz1Orb8MXH/+n1tpn3f7ewn3+XFU9euH+l71GNvR6qGnf77KFcX7fPHzZWcGX1AbO/p1v/wMLw588D1vZZz7lWckb8IokK+vYizM9p+dk1Wtk8VhkjPG0LY5rqVr7TaLVy9FlNXWLpc/Xeo/jknGe9BzMw357XhavqarHLlz3o7V8G77m9nnZ87amMcae/cu0kzmSfOX887OSPCHJBQvX+dUk/2q+/LIkFy/87niSW86Xb5lpgfqs+ecfTPKjC9f79wu3++Mkd50v/0SS715nGo9netczSR6Z5Pfmy7+b5FHz5cck+e358h8kuXOSf5lpx/aHk9w40w72yvi+ab58syR/nuSzMp1Fde3ivJ/icXv3ws9fk+S3Fx+fJB9d+P1Dk1w2X/5ApndlV/79ZZLztvj83TnJ2xaegwuSPCnJE+af35zkn82XfyzJf50vvzbJQ+bLn5npncZLkvxepncoXp/kwnXG+7sLy8xnJzk03/7vM8WAGyV5yTzfS5eLJOck+ZMkt5qHPzzJs5ZM91OTXL0w7rtletHeJMlNk7w90zK7+NjfMsnx+fJNknzmfPkOSa5ceA6vXmcefzbJN86X/1GScxef13l+/ybJrTMtX3+Z5Mnz77535bHeg9f08Xn+X5/kDnsxDZuczp9N8sSF19Eb58ufXo7nn6+en7PDST6V5J7r3Pfnzvd/wbycvTLJz21wuhaf3+uS3DZT3H91kq9anPZT3M9I8i/my8/PFKLPyXxWxjz8sUl+ZL584yRXJrn9RpetrL0OvCzTa/lG889PTPL4+fL9kjxvA4/D4Uw7zhfN8//6TNuHSvKglXEtud3P5MR6/9L5cbhlkjvO03vO/LunJ3nkwmO18lr70Y0+V1uZ5lMsb29Kcu48ve+Zl6PDmdcT8/Oysv1Zuh1ZY7qOZIrfN820TXrRPPyPMx2Afvp+Nzmvi+vFJyR50pLr3TTJtUuG/9ckj1n4+VczHZCfNC1Jnpfk/uu8zv5tpvXxI7djvXAaz/vq7fFLk7xqD6Zj2f7UP8+0/nhEkj84xX2MnNjf+k85sX74jZxY/1yY5K3z5UevvF4yvea/Yb78uGzTumw75i3J/5Pkivnyc+fXwW2SPCrJT87DX5bpQOTWSd6d5FaZtr2vWpjHy5I8dOF+X5bkv8yXH5jkD3fhOX7nws+Py/TmxZuTfDDJIxYe19vMlx+T5JcXbvPuJDfbq9fLqvn5ulXTdn6SdyS5+/zzTZMcWuO2j56ve36m/ch3Jfm8PZ6fL0jyziQ/leng9qJscD2QKRL91rwcPWle7m6y+Hxv83K0sl05lOSm8+VbZtqnXfnL2ouv4Y3unz8pJ+87ffrn+fXyU/Pl7810YL+yn3FtpjcF1txWn8Y8XpL110H/fmH+N3W8sMa4r07yFfPlo1m1r79q2r4u0/HKjZJ8zvz6vHX6tvDn5mX+Hy17jWzm9ZDpOOYlCz/fbP5/rWOzv0ty+3n4WvuN98v05+Frfox/L8lX58R+0V3n2zwn877LFp7Tc+Z5PC/JH2Y6CeJe8+U75eQmsHj50dnift0ay9WybdGnl6N5+GWZjkHXer6WPo5rjPOk52AedsH8/7nz8naL+ee1tuFrbZ+XPm9rzf9+ONPoPWOMV82Xfy1TMbxPVb22qq7KtAN25w3czz0zLTSvmsvyo5LcbuH3v7Vw+ZmZ3qW8UaYX/2+c4r6fvfD/vebL91q43a/O051MB4hfPf/7yXn43TPtpCTTE3RknsaXZXphXjj/7iVjjA+fYlpWjE38/JkLlz8jyb3GGHed/91mbP1d2q9J8twxxl8lyeK0V9X5mVZEL58HXZ7kq2vtd56TaWPxjEwL/LvXGe+rkvx0Te9U3myM8Yl5+OvGGO8YY3wy03P1VVl7ufjCJF+c5CXz8B9Jctsl033S2VOZdgSeP8b42Bjjb5O84BSP0TlJfnlelv9nNn5q+6uT/H9V9YNJbjfGWPbu1BVjjPeNMT6e5P/kxBlqV2XJu/67rNKXyf3oqzI/x2OMP0pyi3kZWM+7xhivWef390jy8jHGh8cYN2R63rfidWOMa8cYn8oUeA9v4rb/kClgJ9Py8PJ5WhaXjbXO4kg2tmyttQ5Mkv85vw6TaaO68p0Gj0nyPzY4D+8cY1w1z/81SV46pq3cesv3V2fajmSMcSzJX8/DvzbTjtIV8/x+babAnEwRcGX7sLIN2qpTTfN6y9vvjDGun9enf5xpOVrLetuR1Va2SV+V5Fimd+dukuTwGONtW53RjZjXkddW1YOTpKpuPI/7FUkePr/bdqt5+l635C5elOQ75ncTU1VfUNPZCbdL8oExxi8n+e9Jvmwn52MDVq/r/nbJsN3Q9qfGGC/JtPz9fJJTfczlHzLtNCYnnz123yQ/Ny9vL0hy0+pngNwrJ9Z1q/epTmddtmLL8zbG+L+ZlvvzMp0d+huZlrl7p5/F8eVJXjbG+OAY4x9y8r7jMv9r/n/p2XY74O+SpKaPaT4hydeO6Qy3Yzl5X+/j8/+fWri88vN++djwVUnuW9NZsffOtA573xjjimRafyzs3y3z0jHGdWOMv0/ylpy8z7/rxhh/nhNvLP5kkodk4+uBKzJFy3tnWj/+aaYovuYnIbZJJfmJqnpzpoPv22SKF6ttdP/8VFb2ma9Kcs3CfsY7Mr0219tWn4711kErr/FNHS8sG0lNX8R83hhj5fspT3V8ud5Zt8t8YdZ+jWz09fCOJP+kqn62qh6Q5G9PcWy2kbN/7zf/+9NMIfuLcmJ/8p3jFGclb8S8D3s8ybdkinivzPTJhs9P8tat3OcWLesWyfJtxVrP13r738ssPgdJ8j1V9aYkr8n0ulm57Vrb8LW2z+s9b81+2HAsix1Pz3TWxntqOv38M9utusoUXb5hjd//3cLl52V69/uPkrx+jPGhTUzjWhuAleGvzFTxPjdTof6BTJXwFQvT+XWrd9ar6stXTeOpXFhV9xpjvDrJNyT530n+1cLv319Vd8x0JtBDkqyEoRdnOnX+qfN477rwYt6srYSBWud378v0XH9ppncglhpjHK2qY5nekXlNnfiysWXL0tLloqouyrTButeq4Tdbcj9tEpYM+0ROfNxzcXn9viTvz3SGx2dkOhvqlMYYv1FVr810tsSLqurb5oPMRat3BBd3Evf6tb2ynD0+Sarq5mOMv173Fntj2fI4cvLzmZz8nJ7qdbreMr4Zi8/vJ7O55/SGOVYkC8vGGONTdeJ7hirTWZYvWrxhVV2SrS1bi6+LTz9G83r8/VX1NZkOyDb6PVJbXb6XvT4ryeVjjB/awHhP52D/VNO87ABorPp/I9OxdDuyhpWDkXdkekfzltmdg5EV35zkl6rqx5LckORhmc5+u1ems6tGpnfo/m9VfdGq2z4z007PG6qqMp1N8eBM29QfqKobknw0J6LkXlm9PX5Nkm+vqruPMa6Yd8ivP8UB8HZoy1BNH9O+Y6aPxVyQ6R39tSyuNxbXOStvNp305sX0lGzI6azLVpzuvL0608HG2zLtpz0m0zL4/RsY13pW5m2r87VVN820nr1u/gjCv8gUkA+MMcafV9XdMu3L/WSm/YatPPbJ7j/+TU1/qOfDY4xfmz/+8dhMHxk65XpgjPEPVfWeJF+f5Mcznen2n+d/O+kb53HdbYxxQ01frbDsmGtD++cbcKqYuZlt9VbGm/RlZWV/ZVPHC2vY7P7fWtdfax90veOuDb0exhh/XVV3SXL/TGchf33mffU1LO7zrrXfeP9MZ23+0qrhh5dM11Y/npZMx9JPyLT+virJT2c6jh+b2B6drrX21ZYdG6z1fC19HNfx6fue99Hvm2mb/LGqellOLB9rbcPXUlnyvK1lP5xpdGFVrbwQV+JHkvxVTZ8lXfwS0I9kOi1t2c+vSfKVdeI7h25SVV+wbIRzhX1Rkl/Ixt71fvjC/6+eL/9JplOik2mluzLdr810Cuen5vG8Mcm358S7WS9K8t3zDnCq6ks3MP5l3prkUfO7AxfM87LoSKba+EeZVvYrvifJxfNnF9+SKXBt1UuTfH1V3SJJquqClV+MMa5L8td14jP+35zpbIe13nlOpo/EXJrpXY9L1hppVX3+/G7+T2U6pW/lQOMeVXX7eUfy4Zmek7WWi7cludXKsldV51TVncf0zfvXVdVKOV59gPuKJA+pqnPnHYCVUHc807sjycnL7PmZKvOn5sdgQ5/Nrunz4u8YY/xMpndmvuQUN9lv/mOSm9f0mek3ZXo3YD96RebneF7m/mpeRo9nPnuhqr4s0+m3G/W6TN8vcfM50Gz3l4CvXg9u1dKzODZx+7XWgcs8M9M7Ms9ZOANpJyw+n/8iyc3n4S9N8tCq+sfz7y6Yz1ZJpu3gymv232T9+djO6bskJ5a3JHlQVX3mvD69JCfOTl1mw9uR+WyJlYOR12TaFj0hJ7ZJm16exhjHx/SdHSs//+cxxpPWuO5fjDG+ZozxJWOMu43pbNAxxviBMcYXj+l7z35rvu7Lxhj/cuG2nxpj/H/zdb54jHGf+Z3Uy+efv3SMce9V78DthdXb45/NtA362Xn995Js7M2v07Vsf+r75un7hiTPWnm9b9LKmwBJpjebllznNTmxrnvEkt8vs5ll73TnbeVgY+Usjvsk+fi8r7LotUkuqen70c7JFDm3Mr07aozxpkzzcU2mszlftf4t9p85snxsjPFrmeLIPTNHlvn359XB+mMKFyV53Xz2wA9neuN4M+uBVyZ5/3yGxysznc2y3d9nlJy8HJ+f6azNG6rqPln77JS/yQb2z3P6r5H1ttWbsZXp2NTxwrI7mN8c/UhV3XMetGxduDhta511+64kd5qPkc7PdMZVkvxZTvM1UtN3Sn7GGON5Sf5Dki87xbHZorX2G1+U5DF14jugbrPyHG6zV2b6+N6r5zOz/j478xpZz1rdYpm1nq/T2f8+P8lfz8HoizKtN09lre3zpp63/bAyXtnZ+qUkf5Epftw8U0E8npN3nC9L8otVdX2md4iekeT3q+p9Y4z71PQlYc+uqhvP1/+RTN/1sMyvZ/oLLhv50ukb13TWx2dkWkCSKb48q6p+INO7n9+SJGOMj8/vFqx8fOWV822umn/+8Uzf5/DmeYf/eKbvP9qwMcbxLP+Y0yUL13lups/ur77tX+VEBDstY4xrquopSV5eVZ/MtANzfOEqj8r0fN0k07vc3zIPX/bO88p9vr+q/lWm5/UxY4zXLhn14+eN2ycznYL5+5mWh1dn+vzwRZlWxM+fz6x4dFYtF/M7XA9N8jPzCvlQpuflmnk6n1VVH8v0glqc5zdU1W9lioHvyomV1X9O8pyq+uac/KXPT0/yvKp6WKaPnGz0bLKHJ/mmmt5J/7/ZR19euZ4xxuGFHx+1V9OxCU9K8j/mg72P5cQ0Py8nTh29ImuvR5oxxl9W1U9kOvh4b6ZldPWByek4ab13Gvez1lkcG7V0HbiGF2QK9Bv9aNpWPTnTa/0NmU7zfneSjDHeUlU/kuTFc1S+IdM7bO/K9Jq8c01/EOG6bNP6cQ1PyvLlLZl2FI9l+ojGj48x3lvTu3TLbHY78spMH2P5WFWtPhh5c5JPzAc1l41t/uLIs8SnxvTlm4uuyMZ25rbT6v2pl2Q6Zf4eY4yPVNUrMu0XPXGT9/s9SX5+Xm4PZdq+rp7fxyf5tar6/kzL8UbWeZtZl53uvL0y02n8rxhjfHLeT/uz1VcaY7yvpjPcX53pDbc35MSbPb+Z6ePm35OT3xzaFfO+32KsffQa1zu8cPmyTPvO7Xf7wEWZvlj8U5nWyd+R6Z3vn63py3Kvz/SO+kfXvov9Yz5rYNmZAxtaD4wx/kOmg/iMMd6b7TtrefV4PlTTH2i4OtN66ouq6spM+7XtNbFwu43sn/9ukufW9IXa372FaVtvW72Z+1mcx+sznfF/qtt8cAvHC8t8a6b1xN9lOvvvpHXhqmn7/Uzb4JPOuk2SqnrO/Lu/yHR8tXJG2kqIXHyNbMZtMu2HrJw4snJW15rHZguW7jeOMV5c06dbXj0NzkeTfFOm47RtM8Z4aaav/Vj5+QsWLh9e4/JlWVgHboNl3WLpsr7O83U6+99/kORx8/b4bTnRG9bz+CzZPq/zvH1g2Z2sfNnZWaemb/M/f15Jr3e945k+KvdXuzJhbElN73w8YfFdatgrVfXZY4yPzu8oPD/TlyY+f6+na6/U9FeJnjbG2I2/LrQpVfXRMcZn7/V0HHRV9fM58Rd/Vvy3McZOh8I9NYe931s88+psnI75zaHr548JPCLTl24+aJvu+3D2wWMMcCor+3/z5SNJbj3G+N49niy2wUHdFm3X9nk/nGm066rq+Zm+OOtr9npagDPSk2r6rq3PzHQ242/v7eTsnXmn6Tuy8e8y4gAaY3znXk/DXlh99sdZ7G6Zviy7Mn2U5TF7Ozmw/9T03S8/tWrwO8cYD9mL6WFHXFpVP5TpGPtdmf56F+ylbdk+n7VnGq02h6TV31vyg2PjX1LFNquqb8n0ZzkXvepMOjixA3GwzR9bvfGqwd88xrhq2fXPtPHvlbNh3bBZ1iVs1Jm83jiT5+1sZL22P9kGH4yza60PD46avnB99V/s/vgY48v3YnqWEY0AAAAAaPbDX08DAAAAYJ8RjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKD5/wGn22anuDB79AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x3600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if theres any outliers\n",
    "data_table.plot(kind = 'box',figsize=(20,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92cdcb02-fdf2-401c-b2c0-080c1cad6342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x3600 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_table['fc'].plot(kind='box',subplots= True,sharex = False,sharey = False,figsize = (20,50),layout = (5,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cfb36-131f-40f1-a220-aa0a97ad3557",
   "metadata": {},
   "source": [
    "As you can see from the boxplot , we can identify that there are 3 outliers in front-camera and 2 outliers in pixel height.. \n",
    "box plot does not make sense to binary columns like touch_screen,wifi,three_g,four_g,bluetooth.\n",
    "\n",
    "Since the heat map tells that fc and pc are correlated, i do not want to remove the outliers so that i do not mislead the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b746da8-bf75-44e0-bd92-0e161393075c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "battery_power    0.200723\n",
       "blue             0.020573\n",
       "clock_speed     -0.006606\n",
       "dual_sim         0.017444\n",
       "fc               0.021998\n",
       "four_g           0.014772\n",
       "int_memory       0.044435\n",
       "m_dep            0.000853\n",
       "mobile_wt       -0.030302\n",
       "n_cores          0.004399\n",
       "pc               0.033599\n",
       "px_height        0.148858\n",
       "px_width         0.165818\n",
       "ram              0.917046\n",
       "sc_h             0.022986\n",
       "sc_w             0.038711\n",
       "talk_time        0.021859\n",
       "three_g          0.023611\n",
       "touch_screen    -0.030411\n",
       "wifi             0.018785\n",
       "price_range      1.000000\n",
       "Name: price_range, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.corr()['price_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fce4ba5-f7df-48b7-afa7-dd53986bf5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_table.iloc[:,0:20] #independenet columns \n",
    "Y = data_table.iloc[:,-1] # target column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d78a179-0037-41ab-8574-0068325a9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "validation_size = 0.30\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e74ff93f-13d4-41f7-bb1a-6495f7c45bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Accuracy: 0.9333333333333333 at K =  17\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAGDCAYAAABuj7cYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABPoElEQVR4nO3dd3hUZfrG8e+TkBACQaqIgAYLKFYggF1YFTUg2EBQkbUhioqr7uq6ri6u7q7uqj8bgj2iLkVFELFCstgQQalCpAQUpddASEh5f3/MBENIz8ycmcn9ua5cZE55zzMnZ5KbU97XnHOIiIiISHiI8boAEREREfmNwpmIiIhIGFE4ExEREQkjCmciIiIiYUThTERERCSMKJyJiIiIhBGFMxGRKGFmr5nZw17XISK1o3AmIjVmZhlmts3M6ntdSzgzs9Vmdm6J14P8++1sL+sSkfCkcCYiNWJmycCZgAP6hXjb9UK5vUAys6HAc0Af59z/vK5HRMKPwpmI1NQ1wGzgNWBoyRlm1s7M3jWzTWa2xcyeLTHvRjNbambZZvaDmXXxT3dmdlSJ5fZdojOznma21szuMbP1wKtm1tTMpvm3sc3/fdsS6zczs1fN7Ff//Pf80xeb2UUlloszs81mdnLpN+ivs2+J1/X8y3YxswQze8P//rab2bdm1qqiHWZmw4DHgfOdc1+Vs0y52/S/nmRm681sh5nNMrPjymnn92b2Ralp+/axmdU3s/+Y2U9mtsHMxphZg4rqF5HQUDgTkZq6BnjT/3V+cTAxs1hgGrAGSAbaAOP98wYAf/Ov2xjfGbctVdzeIUAz4HBgGL7fX6/6Xx8G7AGeLbH8OCAROA44GHjSP/114OoSy6UC65xz88vY5n+BwSVenw9sds59hy+QHgS0A5oDw/01lOdm4O/AOc65uRUsV9E2AT4Ejva/p+/w7f+aeBToAJwMHIXv5/RADdsSkQCK2EsDIuIdMzsDXyia6JzbbGYrgSvxBaDuwKHAH51zBf5Vis/g3AA85pz71v96RTU2WwQ86JzL87/eA7xToqZHgHT/962BC4Hmzrlt/kWKLyG+AfzVzBo753YCQ/AFubK8BXxvZonOuRz/e3zLPy8fXyg7yjm3EJhXSf3n+etbVMlyFW0T59wrJd7z34BtZnaQc25HJe3uY2YG3Aic6Jzb6p/2D/92/lzVdkQkOHTmTERqYijwiXNus//1W/x2abMdsKZEMCupHbCyhtvc5JzLLX5hZolmNtbM1pjZTmAW0MR/5q4dsLVEMNvHOfcr8CVwmZk1wRfiyjz75JxbASwFLjKzRHxn+oqD0jjgY2C8/9LpY2YWV0H9w/GdqXrJH47KVNE2zSzWzP5lZiv973m1f7UWFWy3LC3xnVWc578kux34yD9dRDymM2ciUi3++5IGArH++78A6uMLRicBPwOHmVm9MgLaz8CR5TSdgy8wFDsEWFvitSu1/F1AR6CHc269/56x7wHzb6eZmTVxzm0vY1tp+M7i1QO+ds79Ut775bfLjDHAD/7whHMuHxgFjPI/HDEdyAReLqedjcA5+M7gjcZ3mbNa28R3Fq0/cC6+YHYQsM3/nkvbTYn9aWaHlJi3Gd+Zx+Mqee8i4gGdOROR6roYKAQ64btf6WTgWOBzfPeSzQHWAf8ys4b+G+dP96/7EnC3mXU1n6PM7HD/vPnAlf6zQxcAlXUzkYQvYGw3s2bAg8UznHPr8N2bNdr/4ECcmZ1VYt33gC7ASHz3oFVkPNAbX5jad3nRzHqZ2Qn+M3U78V3mLKyoIf9Zu98BF5jZkxUsWuY28b3nPHz36SUC/6igjQXAcWZ2spkl4LvXr7iOIuBF4EkzO9j/ftqY2fkV1S8ioaFwJiLVNRR41Tn3k3NuffEXvpvxr8J3FucifDeZ/4Tv7NcVAM65ScAj+AJHNr6Q1Mzf7kj/etv97bxXSR3/BzTAdxZoNr7LciUNwReYluE7a3VH8QznXPH9au2BdyvaiD/ofQ2cBkwoMesQ4G18wWwpvjNib1RSM865n/EFtMvN7J/V3Obr+B60+AX4Ad/7Lm87PwIPAZ8By/ntvr9i9+C752+2/xLpZ/jORIqIx8y50lcKRESin5k9AHRwzl1d6cIiIiGke85EpM7xXwa9Ht/ZNRGRsKLLmiJSp5jZjfgeGPjQOTfL63pERErTZU0RERGRMKIzZyIiIiJhROFMREREJIxE1QMBLVq0cMnJyV6XISIiIlKpefPmbXbOHTAyR1SFs+TkZObOrWg8YREREZHwYGZrypquy5oiIiIiYUThTERERCSMKJyJiIiIhBGFMxEREZEwonAmIiIiEkYUzkRERETCiMKZiIiISBiJqn7OREREJPjSs9JJHdefXJe9b1qCJTF9yBR6te/lYWXRQeFMREREqiw9K52+aQPJTZsKq3vum56bnEHfogFMGzpRAa2WdFlTREREqqQ4mOWkTdovmAGwuic5aZPomzaQ9Kx0T+qLFgpnIiIiUiWp4/qXHcyK+QNa6rj+Ia0r2iiciYiISJXkuuzyg1mx1T33uxdNqk/hTERERCSMKJyJiIhIpZZuWup1CXWGwpmIiIiUaW/hXiYsnkDP13rSaXQn4mkIyRkVr5ScQYIlhaS+aKWuNERERGQ/Ofk5/OPzf/DSdy+xYfcG2jdpz6PnPkqH5h24yg0o/6GA5AzihlzC5METQ15zNFE4ExEREYpcEau2reKoZkdRP7Y+4xePp3ub7tyccjPnH3U+Mea72DZt6EH0pYyAlpxB/WsuIy9mO/en388JrU6gTeM23ryZCGfOOa9rCJiUlBQ3d+5cr8sQERGJGJt2b+LV+a8yZu4Ydufv5qc7fqJ+vfrsyd9Dg7gGZa5T0QgB2Xuzuerdq2gU34j3rniPHm17hOqtRBwzm+ecSyk9XWfORMQTGv5FJLCq+5n6YdMP/OPzfzDph0nsLdzL2Yefzc0pN+87Q1ZeMAPo1b4Xex7YWe782dfPpt/4fpz92tm8fsnrDDxuYC3eWd2jcCYiIafhX0QCq6qfqey8bHILcmnZsCWbdm/i/R/fZ1iXYQxPGc5xBx8XsHqOO/g45twwh6vevYo2Sbq0WV26rCkiIVXh8C8AyRkkDlVAE6mqqnymEoZezvkdz2Dm6pkMPWkoz6Q+g3OOnPwcGsY3DEmdby58k74d+nJQwkEh2V4kKO+yprrSEJGQ0vAvIoFVlc9UbtrbTFk6nYuPuZghJw0BwMxCFsyytmVx7ZRrOfXlU1mxdUVIthnJFM5EJKQ0/ItIYFX1M0VsPq9f8jrd23QPRVn7ad+0PZ8M+YSNuzfS/cXuzFg1I+Q1RBKFMxEREQm6nsk9mXPjHNo0bsP5b5zP6G9He11S2FI4ExERiVC79u7yuoRqOaLpEXx13Vf06dCH+rH1vS4nbCmciUhIJViShn8RqQXnHBt2bQDwdXtRGBdRn6mk+km8d8V7XN/legA+W/UZm3Zv8riq8KJwJiIhNX3IFBKHDij/j4n/ac3pQ6aEtC6RcLczbyejvx3NiWNO5KzXzsI5R2JcIpOvnBhxnykzA3xn/ga9PYhuL3ZjwfoFHlcVPhTORCSkerXvxeQhb8JVfQ/8Y6JuNEQOkLk5k5un3UybJ9owYvoI4mLi+ONpf6TQFQJw8TEXM21oOQEtzD9TjeIb8dHVH1FQVMDpr5zO5KWTvS4pLKgTWhEJuR25OyBuN/G/78tedu+bnmBJTNMIASLkFeSRX5RPo/hGLNywkNcWvMYVx13BzSk3071N931nnor1at+LaUMnkhrT74ARAsL9M5VyaArf3vgtl0y4hEsnXspDPR/i/rPuP+A91iXqhFZEQu6c189h5daVrLx9JbExsfvNm712Nq0btebwJod7VF3NaUgqKa26x0TWtizGzhvLK9+/wm3db+OvZ/+V/MJ8svdm06xBs1CWHnK5BbncNO0mDqp/EE9f+PS+6dH8udLYmiISFpZvWc7MrJk83OvhA4LZjtwdnDfuPH7X/ndMGRQ+98dUhYakktKqc0xMXz6dZ+c8y0crPsLM6NexHz2TfevExcZFfTADSKiXwGv9X6PIFQGwaMMilmxawvVv31b3PlfOuaB9ARcAmcAK4N4y5jcFJgMLgTnA8f7pCf7XC4AlwKiqbK9r165ORMJbfmG+m7psqlufvb7M+Y998Zjjb7ipy6aGuLKam7lqpkt8sIUjOd2BO/ArOd0lPtjCzVw10+tSJUSqckw0KHFM9PtvP9f6P63dAzMfcD/v+Nnj6r1XWFTokv8v2fGXhlH9uQLmujLyTNAua5pZLPAjcB6wFvgWGOyc+6HEMv8GdjnnRpnZMcBzzrlzzHehuaFzbpeZxQFfACOdc7Mr2qYua4pEvvzCfE4eezI5+TksuWUJiXGJXpdUqQYPNSb31akV99KenEHCtf3Y88DOkNUl3qnqMVH/2ovIfSCbDbs20KxBM+Ji40JWY7hLGJVE3mvvR/XnyouxNbsDK5xzq5xze4HxQOnB8joBMwCcc8uAZDNr5Q+UxT3rxfm/oufmOJE66r1l73H/zPvZk7+n3GXiYuMYnTqa1dtX88/P/xnC6mpOQ1JJaVU9JvL8f+paNWqlYFZKHrvq7OcqmOGsDfBziddr/dNKWgBcCmBm3YHDgbb+17FmNh/YCHzqnPumrI2Y2TAzm2tmczdtUid2IuHs8a8fZ+KSidSvV3HP4Gcnn83wrsNpntg8RJWJiISPYD4QUNYzsKXPfv0LeMofwhYB3wMFAM65QuBkM2sCTDaz451ziw9o0LkXgBfAd1kzYNWLSEAt2biEL376gv+c9x9fr+aVeL7v8yGoSkQk/ATzzNlaoF2J122BX0su4Jzb6Zy71jl3MnAN0BLIKrXMdiAD38MFIhKhxs4bS3xsPENPHlrldZxzjF88nvcz3w9iZbWnIamkNB0TtVeX92Eww9m3wNFm1t7M4oFBwNSSC5hZE/88gBuAWc65nWbW0n/GDDNrAJwLLAtirSISRLv37ub1Ba8zoNMAWiS2qPJ6Ra6If3/1b4Z/MJzsvPC9r2T6kCnUG3JxRA2fI8FT5IoYeMIlcGUfHRO1UJeHegtaOHPOFQC3Ah8DS4GJzrklZjbczIb7FzsWWGJmy4ALgZH+6a2BdDNbiC/kfeqcmxasWkUkuLbnbqf3kb25OeXmaq0XGxPL6NTRrMtex98y/hac4gJg/a71FMTuoN7V/cscPocr+/BY6t+isz8m2c/uvbsZOGkgry98nfOPPTMih1QKF8WjHpS3D2OuuojJQ96Myn0Y1E5onXPTgemlpo0p8f3XwNFlrLcQ6BzM2kQkdNo0bsPEARNrtG6Ptj24scuNPPXNUww9eSgntjoxwNXVXsbqDM487EzuP+t++scdOHxOmyatSW6S7F2BEjJf/vwlUzKn8ETvJ7jjlDvIWJ0RkUMqhYvyhqWKI5F8cnhv2Xv0PrK3hxUGh4ZvEpGgytqWRV5hHse0OKbGbWzJ2cIxzx1Dx+YdmXXtrCo9UBBKzjly8nNoGN+wzPlFrijsapbA2rR7Ey0btgR8x3z7pu09rij63fPpPUxeNpk5N86hSUITr8upES/6ORMR4V9f/IuuL3Rl195dlS9cjuaJzRmdOpqRPUZiZT4IHnp7C/dy/ZTrydyciZmVG8yAfcHs+W+f56b3byKa/lMs8NJ3L5H8VDKfr/kcQMEsRP5xzj/49sZvIzaYVUThTESCZmfeTt5c9CaDjhtEo/hGtWprwHEDGHDcAHwDiHjvjo/u4JX5r/Dduu+qvM76Xet54bsXeGbOM0GsTEKloKiAkR+O5Mb3b+SMw87g+IOP97qkOiU2JpaDEg4ityCXP3z0B37e8XPlK0UIhTMRCZo3F77J7vzd3JRyU8Da/M9X/+Guj+8KWHs18eK8F3l+7vP88bQ/MviEwVVe78GeD9K/Y3/u/PhOZmbNDGKFEmzb9mwj9c1Unp7zNH845Q98cOUHNG3Q1Ouy6qSfd/zMy9+/zCUTLqlw9JFIonAmIkHhnOP5uc/T+ZDOdDu0W8Da/WXnLzw5+0nm/DInYG1Wx5c/fcmI6SO44KgL+Oc51RteKsZiGHfJODq26MiASQPI2pZV+UoSlt5Y+AYZqzN4ud/LPHH+E9SLCerzdVKBo5sfzRuXvsG8dfMYNm1YVNw2oHAmIkGRtT2L5VuXMzxleEAvRY7qNYpDGh3CzR/cTGFRYcDararHvnqMw5sczluXvkVsTGy110+qn8SUQb5+mT5b9Vmgy5Mg25nnG2D71u63Mn/4fK7rfJ3HFQlAv479+Huvv/PGwjd44usnvC6n1vS0pogEzbY926hfrz6JcYkBbXf84vEMfmcwz6U+xy3dbglo25XJLchlw64NHN7k8Fq1s3XPVpo1aBagqiTYnHM88fUTPPbVY8y+frZu+g9DzjkGvj2QGatmsPL2lRFxmVlPa4pIyBSf0WraoGnAgxnAFcddwTntz+H+mfeze+/ugLdfmnOOJ79+ku2520mol1DrYAbsC2bpWen83+z/q3V7Ejy5Bbn8fsrvufvTuznzsDM5uOHBXpckZTAzXu3/KrNvmB0RwawiCmciEnDPznmW7i92D9qQS2bG832e58OrPqywC4tAeeLrJ7jzkzsZt2BcwNtOW5DGHz7+Q9iPH1pXrcteR6+0Xry+4HVG9RzFxAETQ3LMSc00im9Eh+YdcM7xyvevhPWwbxVROBORgHLOMWbeGGJjYkmqH7wBiY9ufjQ92vYACOoTWp+s/IQ/ffYnLjv2Mm7tfmvA23++z/N0bd2Vq969iqWblga8famdh2c9zMINC3l7wNs8cPYD6kw4QizZtIRh7w9jyOQhFLkir8upNh1lIhJQs9bMYtnmZQzvOrzyhQNgVMYoerzUg/zC/IC3vWLrCq54+wqOa3kcr138WlD6WGsQ14DJV0ymQVwD+o/vz/bc7QHfhlRfceB/7LzH+OaGb7is02UeVyTVcfzBx/N478eZkjmFh/73kNflVJvCmYgE1Jh5Y2iS0ISBxw0MyfY6t+7Moo2LeOqbpwLe9i0f3EKMxfDeoPdq3YluRdod1I53Br7D6u2rGTt3bNC2UxPpWek0eKgxNsr2fTV4qDHpWelel1Yr5b2vGatmcP/M++nxUg+y87JpGN9QnctGqNt73M7Qk4Yy6n+jeHfpu16XUy16WlNEAmbj7o20faItt3S7hf+74P9Ctt1+/+3HzKyZLB2xlHYHtQtYu+uy17F6+2pObXdqwNqsyHfrvqPzIZ3DZhSE9Kx0+qYNJCdtEqzu+duM5AwShw5g2tCJETl4d0XvK/bqfhTWy+aGzjfwXJ/niI+N96xOqb3cglzOfu1slm1exuqRq8PuQQE9rSkiQdcovhHP93k+5N1bPHXBUxS5Iv7w8R8C0t6nKz+lsKiQ1kmtQxbMALq07oKZkbUtiw9+/CBk2y1LuQEGYHVPctIm0TdtYMSdQavsfRW+MZW4wiYMPn6wglkUSKiXwOQrJjPh8glhF8wqonAmIgGTGJfI9V2up0PzDiHdbvum7fnLmX/hwxUfsmb7mlq1NXnpZHq/0Tsol0mr6s5P7mTApAHVGrcz0FLH9S87wBTzB7TUcf1DWldtVeV95Y+bTJ83Lg5lWRJEhyYdygVHXQDAN2u/oaCowOOKKqdwJiIB8dXPX/H0N097Nrbd3afdzQ+3/FCrPsgWb1zMkMlD6N6me8jP/pU0tu9YWiS24OLxF7Nx90ZPash12eUHmGKre/qWiyDR+r6kcj9s+oHTXjmNP336J69LqZTCmYgExONfP87fZ/3ds64G6terz+FNDsc5V6MuKbbu2Ur/8f1pXL8xk6+YTEK9hCBUWTUHNzyYyVdMZlPOJgZMGhCUJ1FF6ppOLTsxotsInpz9JK8veN3rciqkcCYitfZr9q9MWTaFa0++lvr16ntay7+++Bedx3Zm5daV1Vrv9+/9nrU71/LuFe9yaNKhQaqu6roe2pWX+73MrDWz+M9X/wnptrft2RbS7YmEyuO9H6dXci+GvT+MOb/M8bqccimciUitvfzdyxS6QoZ1HeZ1KQw9eSjxsfHc/tHtVOdp9L+e9VfSLk7jlLanBLG66rnyhCt569K3uL3H7SHbpnOOU14+BQrjIDmj4oWTM0iw4HU0HAwJlhSV70uqJi42jokDJnJIo0O4ZMIlbN2z1euSyqRwJiK1UlhUyIvfvci5R5zLUc2O8rocDk06lFE9RzF9+XTeW/Zepcuv3r4agG5tujHo+EHBLa4GBp8wmIbxDcnOy2bRhkUBbz8nP4dXvn+FPm/1Ib8wHzPjyfOf5IX+z5E4dED5Qcbfncb0IVMCXlMwTR8yJSrfl1Rdi8QWTBk0hbtOvYumCeH5BKfCmYjUysbdGzmi6REhGxGgKm7rcRsntjqRkR+NrHBg9G9/+ZZjnzuWF+a9EMLqamboe0M5b9x5/Lzj54C0l7k5kz989AfaPNGG66dez+rtq/lpx08ApB6dyo1db2Ta0IllB5kI7uesV/teTBs6kZirLoqq9yXVc9IhJ3HnqXdiZqzftb5aZ9lDQZ3QikhAOOfCpvNUgC9/+pKBbw9k2uBpdG7d+YD563etJ+WFFOrF1GPusLm0SGzhQZVV98OmHzjlpVPo0LwDn1/7OQ3iGtS4rXm/ziPlxRTiYuK4rNNl3JxyM2cedmaZP7/0rHRSx/Xf/+nFwjjG9HuGm1JuqnENXiooKuDyiZczfdmn5FvOvukJlsT0IVMUzOqQZZuXcerLp/Jwr4cZ0X1EyLdfXie09UJeiYhEjc05myksKqRVo1ZhFcwATj/sdFbdvoqvfv6KBg813i9cJFgSR7Y4jG252/jquq/CPpiB70mzNy59g/7j+zNs2jCuPela+rxx8QHvq6xwsXbnWl6c9yL169XnvjPvo0vrLjxz4TMM6DSAVo1aVbjdXu17seeBnfte78jdQYdnO5AYlxjYNxhC9WLq8d6g97wuQ8JAh+YdOOOwM7jj4zvYW7iX+z59sEqfqaBzzkXNV9euXZ2IhM79M+53CQ8nuC05W7wupUwzV810iQ+2cCSnO3C/fSWnO+5LdA/MfMDrEqvt7//7u+NvuLi/NinzfSU+2MLNXDXTFRYVuk9WfOIuGX+Jix0V6+xv5q5858qA1LC3YG9A2vHC7r273azVs1xRUZHXpUiY2L5nu2v3RDvHXxpW+JkKBmCuKyPP6J4zEamR/MJ8Xvr+Jc494lyaNWjmdTkHqGyYHt76gP/8b3TEDT90WtvTqFd4EPnjJlc4rNKV71xJ7zd68/lPn3P3aXez4vYVvHnpmwGpIS42DuccW3K2BKS9UJqweAJnvXYWs9fO9roUCRPfrfuOTTty4M1pYTNUme45E5EaeeeHd7h80uW8P/h9+nbo63U5B2jwUGNyX51acW/wyRkkXNtvv8t24a6q76v+7y/i5UvGcHmny4PS99yQyUNYsH4BC4YvCLtL2hXp8VIPsvOyWXLLkoiqW4LHy98VGvhcRAJqzLwxtGvcjguPutDrUsoUrcP0VPV95bGLq068KmidAp912Fks2riIr9d+HZT2g+H7dd8z55c5DE8ZrmAm+4Tj7wqFMxGptnXZ65iZNZNhXYcRGxPrdTnigcEnDCYpPomx88Z6XUqVjZ03lgb1GjDkxCFelyJSIYUzEam21kmtWXn7Sm5OudnrUsQjjeIbcfWJVzNh8YSw7WW9pMKiQj5Z+QmDjh9E0wbh2fGoSDGFMxGpkeQmyTRPbO51GeWK1mF6wul93dT1JvIK8xi3YFzQt1VbsTGxLB2xlMfOe8zrUiTMhNNnqpjCmYhUy4TFE+jzVp+wf1IvWofpCaf3ddIhJzF10NSwGFO1Is45ilwR9evVj4g+7SS0wukzVUzhTESqZfTc0SzbvCzsLw0VD9MTrcMPhcv7uqjjRbUarSAU5vwyhyOfPpLv133vdSkShsLtMwUaIUBEquGHTT8wa80sHj33UWIs/P9vV/xLNzWm3wG9fk+L4GF6wu19vTDvBVZuXcmj5z0a0u1W1Zh5Y9ics5mjmh3ldSkSpsLtM6VwJiJVNnbuWOJi4rj25Gu9LqXKSg8/FC3C6X0t3bSU5759jrtOu4uDGx7sdTn72bZnGxMWT+Cak64hqX5k3V8ooRVOn6nw/6+viISFnPwcXl/4Opd3upyWDVt6XY6EkZtSbiK/KJ9Xv3/V61IOMG7hOPYU7OGmrpE5SLvUTQpnIlIlhUWF/PG0PzKyx0ivS5Ewc0yLYzj78LMZO28sRa7I63L2cc4xZu4YurfpTufWnb0uR6TKdFlTRKokqX4S9515n9dlSJganjKcwe8M5tOVn3L+Ued7XQ4ADsej5z5KQr0Er0sRqRaFMxGp1NJNS1mwYQGXHnsp8bHxXpcjYeiSYy7hsmMvC6v7umIshos6XuR1GSLVpsuaIlKpp755imunXMvuvbu9LkXCVP169Xl74Nuc1u40r0sBYNPuTfx15l9Zv2u916WIVJvCmYhUKDsvmzcXvalhb6RKftn5C7PWzPK6DF6b/xoPf/5wRAwtJVKawpmIVOitRW+xa+8uPe0mVXLD+zdw1btXUVBU4FkNRa6IsfPGcuZhZ9KpZSfP6hCpKYUzESmXc44x88ZwUquT6NGmh9flSAQY1mUYa3euZfry6Z7VMDNrJiu3rdR/KCRiKZyJSLm27tlKXkEew1OGY2ZelyMRoG+HvhyadChj5431rIYxc8fQvEFzLut0mWc1iNSGntYUkXI1T2zOkluWUOgKvS5FIkRcbBzXd76eh2c9zOrtq0lukhzS7TvnqF+vPsO6DlMXGhKxFM5EZJ/0rHRSx/U/YGy56RE8DqWE3g1dbuDRLx/l8zWfhzycmRlvXvomzrmQblckkBTORATwBbO+aQPJTZsKq3vum56bnEHfogFMGzpRAU2q5LCDDmPdXeto1qBZSLdbWFTIiq0r6Niioy7DS0TTPWcisi+Y5aRN2i+YAbC6Jzlpk+ibNpD0rHRP6pPIUxzM9hbuDdk2P1rxEcc8dwwzs2aGbJsiwaBwJiKkjutfdjAr5g9oqeP6h7QuiWw3TL2BPm/1Cdn2xswbwyGNDuHMw84M2TZFgkHhTER895iVF8yKre65371oIpVp36Q9n636jOVblgd9Wz/t+Inpy6dz3cnXERcbF/TtiQSTwpmIiATFdZ2vo15MPV6Y90LQt/XSdy/hnOPGrjcGfVsiwaZwJiIiQdE6qTUXH3Mxr85/ldyC3KBtxznHW4ve4sKjLwz506EiwaBwJiIkWBIkZ1S8UHKGbzmRahjedThb9mzh3aXvBm0bZsa3N37L0xc8HbRtiISSwpmIMH3IFBoMHVB+QEvOIHHoAKYPmRLSuiTy9Wrfi9Gpo+l9ZO+gbqdpg6Yc2ezIoG5DJFQUzkSEXu170bvj6XBlnwMDmj+YqZ8zqYkYi+HmbjfTIrFFUNpfuXUlPV7qwffrvg9K+yJeUCe0IsLcX+cy9cepXHLixXxYv98BIwRM0wgBUksTFk9gU84mbu1+a0DbfWHeC8z7dR4HNzw4oO2KeEnhTKSOKywq5JYPbqFVo1a82v9VDko4yOuSJApN/XEqH/z4Add1vo7EuMSAtJlXkMer81+lX8d+tGncJiBtioQDXdYUqeMWbFjAoo2LeLz34wpmEjTDuw5nR94OJiyeELA2Jy+bzKacTQxPGR6wNkXCQVDDmZldYGaZZrbCzO4tY35TM5tsZgvNbI6ZHe+f3s7M0s1sqZktMbORwaxTpC7r0roLy29bzuDjB3tdikSxMw47g2NbHMuYeWMC1uaYuWM4oukRnHvEuQFrUyQcBC2cmVks8BxwIdAJGGxmnUotdh8w3zl3InAN8JR/egFwl3PuWOAUYEQZ64pILc37dR7OOdo2bquBoiWozIzhKcOZ88scvlv3Xa3bc84x5MQhjOo5ihjTRSCJLsE8orsDK5xzq5xze4HxQOmB+ToBMwCcc8uAZDNr5Zxb55z7zj89G1gK6IYCkQD64qcvSHkxhVfnv+p1KVJHDDlxCD3a9GBn3s5at2VmXN/leq4+8eoAVCYSXoIZztoAP5d4vZYDA9YC4FIAM+sOHA60LbmAmSUDnYFvytqImQ0zs7lmNnfTpk2BqVwkyuUX5nPzBzdz2EGHccVxV3hdjtQRTRs0ZfYNs+mZ3LNW7ezJ38Ozc55le+72gNQlEm6CGc7KukbiSr3+F9DUzOYDtwHf47uk6WvArBHwDnCHc67M/2o5515wzqU451JatmwZkMJFot0zc55h8cbFPHXBUzSMb+h1OVLHZOdl88OmH2q8/sQlE7ntw9tYsH5BAKsSCR/B7EpjLdCuxOu2wK8lF/AHrmsBzHfDS5b/CzOLwxfM3nTOBW/cD5E65pedv/BgxoP0OboP/TuWvtNAJPhS30olOy+b72/6vkb3Oo6dN5ZjWhzDWYefFYTqRLwXzDNn3wJHm1l7M4sHBgFTSy5gZk388wBuAGY553b6g9rLwFLn3BNBrFGkzlm5bSUtE1vy9IVP6yEA8cTVJ1zNgg0LmPPLnGqvu2D9Ar5e+zU3db1Jx69EraCFM+dcAXAr8DG+G/onOueWmNlwMyvulOZYYImZLcP3VGdxlxmnA0OA35nZfP9XarBqFalLzjr8LJbftpwjmh7hdSlSR115wpU0im9Uo241xs4bS0K9BK456ZogVCYSHoI6QoBzbjowvdS0MSW+/xo4uoz1vqDse9ZEpIbyCvJ4bf5rXNf5OuJi47wuR+qwpPpJXHXCVaQtSOOJ3k/QtEHTKq3nnGPtzrVccdwVNGvQLMhVinhHncOI1BH//urfDP9gOF/+/KXXpYhwU9ebyC3I5cMVH1Z5HTNj6uCpvHjRi0GsTMR7GltTpA7I2pbFI58/woBOA2rdjYFIIHRu3ZllI5bRsUXHKi3vnGPLni20SGyhM78S9XTmTCTKOee47cPbiLVYnjhfz9dI+CgOZs6V7mXpQHN/ncuhjx/Kxys+DnZZIp5TOBOJclMzp/LB8g8Y1XMUbRu3rXwFkRC6bfptXD/1+kqXGzN3DPGx8Zza7tQQVCXiLYUzkSjXqlErBh43kNt73O51KSIHiLEY3lj4Bpt2lz/Cy/bc7fx38X+58oQraVy/cQirE/GGwplIlDul7SlMuHyC7tORsHRTyk3kF+Xz2vzXyl3mjYVvsKdgDzd1vSl0hYl4SOFMJEot27yM2z+8nR25O7wuRaRcnVp24szDzmTsvLEUuaIyl3nxuxdJOTSFrod2DXF1It5QOBOJQs45RkwfwbiF48gtyPW6HJEKDU8ZzsptK5mxakaZ86cMmsLzfZ4PcVUi3lFXGiJRaPzi8czMmsno1NG0atTK63JEKnTZsZex5IwldGjeocz5yU2SSW6SHNqiRDykM2ciUWZH7g7u/OROUg5NYVjXYV6XI1Kp+vXq88g5j3B4k8P3m74lZwsXj7+YhRsWelSZiDcUzkSizKj/jWLDrg2MTh1NbEys1+WIVNknKz/h7R/e3vf6tfmvMSVzCqbR/KSO0WVNkSgzotsIOjTvQLc23bwuRaRa/jzjz3y3djHEDtg3zYrqszlns4dViYSewplIlCjuZf3IZkdyZLMjPa5GpHrSs9JZ/EsWjPsYVvfcN90lZ9DXBjBt6ER6te/lXYEiIaTLmiJR4pXvX+Gi/17EzrydXpciUi3pWen0TRvI3tff3S+YAbC6Jzlpk+ibNpD0rHRP6hMJNYUzkSiwJWcL93x2DzvydpAUn+R1OSLVkjquPzlpkw4MZsX8AS11XP+Q1iXiFYUzkSjw5xl/ZnvudkanjsZMN09LZMl12eUHs2Kre/qWE6kDFM5EItzstbN56buXGNljJCe0OsHrckREpJYUzkQi3N8y/kbrpNb8reffvC5FREQCQE9rikS48ZePZ8XWFSTV171mEpkSLInc5IyKL20mZ5BgOsalbtCZM5EItTNvJwVFBTRJaELKoSlelyNSY9OHTCFx6ABIzih7geQMEocOYPqQKSGtS8QrCmciEeqWD27htJdPo7Co0OtSRGqlV/teTBs6seyA5g9m6udM6hJd1hSJQBmrM3hz0Zvcf+b9GqJJokJxQEuN6bffU5kJlsS0IVMUzKROUTiro9Kz0kkd1/+AX4LT9Usw7JT1s7KieA5JOoT7zrzPw8pEAqtX+17seUCdKIsonNVBxb1x56ZN3e8G3NzkDPoW6fJBOCnvZ+WSM9h2zaXMXjtbPysRkSije87qmOI/9mX2xq1hUsJKZT+rvNff1c9KRCQKKZzVMRomJXLoZyUiUjcpnNUxGiYlcuhnJSJSNymciYiIiIQRhTMRERGRMKJwVsckWFL5vXAX0zApYUE/KxGRuknhrI7RMCmRQz8rEZG6SeGsjqlsmJT611ymfs7CRK/2vZh0VRoxV/fTkDYiInWIOqGtg4oD2gXWl73s/m1GYRwntj1Zf+zDSOrRqXx67Xv0idOQNiIidYXCWR3Vq30vMq77lPtm3sdLF73Ekc2OZPS3o9meu50iV0SM6aSq18YtGMeZh5/J7474nYa0ERGpQ8w5V/ECZn2B6c65otCUVHMpKSlu7ty5XpchUmtLNi7h5LEnc33n6xnTd4zX5YiISBCY2TznXErp6VU5PTIIWG5mj5nZsYEvTbxSVjAvLCpk/OLx/LDpBw8qEvD9XG6ZfguN6zfm4d897HU5IiISYpWGM+fc1UBnYCXwqpl9bWbDzPT8fqRLeTGFW6ffut+0nXk7Gfb+MB7630MeVSVvLHyDWWtm8ei5j9IisYXX5YiISIhV6cYi59xO4B1gPNAauAT4zsxuC2JtEkSFRYUs2biExLjE/aY3bdCUEd1GMHHJRJZtXuZRdXXX9tzt3P3p3ZzS9hSu63yd1+WIiIgHKg1nZnaRmU0GZgJxQHfn3IXAScDdQa5PgmTNjjXkFebRsXnHA+bdeeqdJNRL4B+f/8ODyuo2w7js2MsYnTpaD2WIiNRRVfntPwB40jl3onPu3865jQDOuRxA/7WPUJmbMwHo2OLAcNayYUuGpwznrUVvsXLrylCXVqcdlHAQo/uMpnPrzl6XIiIiHqlKOHsQmFP8wswamFkygHNuRpDqkiDL3OIPZ2WcOQO4+7S7Oe7g41i/a30oy6qzCosKuW7KdXyz9huvSxEREY9VJZxNAkp2o1HonyYRrGPzjtzY5cZybzg/NOlQ5t80n9MPOz3EldVNL333Eq/Of5UVW1d4XYqIiHisKuGsnnNub/EL//fxwStJQuHCoy/khYtewMzKXcbM2LV3FzNW6QRpMG3avYk/z/gzPZN7cuUJV3pdjoiIeKwq4WyTmfUrfmFm/YHNwStJQmHDrg1l9nNW2n0z7qPPW31Yl70uBFXVTfd8dg/Ze7N5LvW5CsOyiIjUDVUJZ8OB+8zsJzP7GbgHuCm4ZUkwZedlc8jjh/DYl49VuuzIHiPJL8rnP1/9JwSV1V56VjoNHmqMjbJ9Xw0eakx6VrrXpZXpm7Xf8Or8V7nr1Lvo1LKT1+WIiEgYqHRsTefcSuAUM2uEb7in7MrWkfC2fOtyAI5qdlSlyx7Z7EiuPOFKxswbw71n3EvLhi2DXV6NpWel0zdtILlpU2F1z33Tc5Mz6Fs0gGlDJ4bdQOGdW3fmqQue4vrO13tdioiIhIkqdaRkZn2AW4A/mNkDZvZAcMuSYKqoG42y3HfGfezJ38OTs58MZlm1UhzMctIm7RfMAFjdk5y0SfRNGxhWZ9CKXBHxsfHc3uN2GsY39LocEREJE1XphHYMcAVwG2D4+j07PMh1SRBlbsnEsCqdOQM4tuWxXN7pchZvXFyl+9S8kDquf9nBrJg/oKWO6x/Susrzy85f6PRcJzJWZ3hdioiIhJlKL2sCpznnTjSzhc65UWb2OPBusAuT4Mnckklyk2QS6iVUeZ20i9NoENcgiFXVTq7LLj+YFVvd07dcGLjrk7tYvX017Rq387oUEREJM1UJZ7n+f3PM7FBgC9A+eCVJsF19wtX0PqJ3tdYpDmbrsteRVD+JRvGNglFanfDZqs+YsGQCo3qO4shmR3pdjoiIhJmq3HP2vpk1Af4NfAesBv4bxJokyPp06MO1na+t9nprd66l/VPteW7Oc0Goqm7IK8hjxPQRHNn0SP50+p+8LkdERMJQheHMzGKAGc657c65d/Dda3aMc04PBESoXXt3MffXuezJ31Ptdds2bkvP5J48/vXj7N67OwjV1VyCJUFyRsULJWf4lvPQpB8m8eOWH3k29dlqXVYWEZG6o8Jw5pwrAh4v8TrPObcj6FVJ0Hyz9hu6vdiNr37+qkbr//Wsv7IpZxMvfvdigCurnUmD3sKuuqj8gJacQcxVF/H6ZS+HtK7SrjrhKr649gsuOOoCT+sQEZHwVZXLmp+Y2WWmrsujwr4Bz6vYjUZppx92Oj2Te/LYl4+RW5Bb+Qoh4Jzjpe9ewuJyqH/NZQcGtOQM6l9zKc0aJ3D8wcd7UiP4RmUwM41XKiIiFarKAwF3Ag2BAjPLxdedhnPONQ5qZRIUmZszaRjXkDZJbWrcxl/P+ivnjTuPWWtm0fvI6j1YEAxmxsDjBnLBURfQsXlHUmP77fdUZoIlMX3IFE5tdyoJ9RJwzvH12q85rd1pIatxauZUBr09iPSh6fRo2yNk2xURkchTlRECvL1JRwLqx60/0qF5h1qN4dgruRcrb19JcpPkwBVWQ9l52STVT9pvwPA9D+yscJ0JSyYw+J3B3HP6PTzyu0eIjYkNao05+Tnc/uHtHNH0CLq07hLUbYmISOSrSie0Z5X1FYriJPAyN2fW+JJmMTPbF8x27d0VgKpqZtGGRSQ/lcy0H6dVa71Lj72Um7rexKNfPsrFEy5mZ17FYa62Hpn1CGt2rGF0n9HExcYFdVsiIhL5qnJZ848lvk8AugPzgN9VtqKZXQA8BcQCLznn/lVqflPgFeBIfP2pXeecW+yf9wrQF9jonPPuRqEo80r/V2gYF5ihgv782Z95Z+k7/DDiB+rFVOVQCpwtOVvoP74/9WPrV/tsVHxsPGP6juHEVidy+4e3c+rLpzJ10NSg9Dm2bPMy/v3Vv7nmpGs463D9n0ZERCpX6Zkz59xFJb7OA44HNlS2npnFAs8BFwKdgMFm1qnUYvcB851zJwLX4AtyxV4D9EhbgPVM7km3Nt0C0laPtj1YvnU5ExZPCEh7VVVQVMAVb1/BL9m/MPmKyRyadGiN2rml2y18MuQTNu3etO9BiUD7bNVnNIpvxGPnPhaU9kVEJPpUaeDzUtbiC2iV6Q6scM6tcs7tBcYDpQc27ATMAHDOLQOSzayV//UsYGsN6pNyZG7OZPLSyQF7yrJfx34cf/DxPPL5IxS5ooC0WRV//OSPzMiawdi+Y2t9c/3v2v+OlbevJPXoVAAWblgY0PFDb+1+KytuX0GrRq0C1qaIiES3qtxz9oyZPe3/ehb4HFhQhbbbAD+XeL3WP62kBcCl/u10x9fJbduqFF6ivmFmNtfM5m7atKk6q9Y57yx9h0snXkpBUUFA2ouxGP5y5l9Yunkp7y4NzXCrzjkS6iUwssdIfn/y7wPSZlJ93zMvc3+dS+exnbn5g5vZW7i3Vm3uzNvJN2u/AaBZg2a1rlFEROqOqpw5m4vvHrN5wNfAPc65q6uwXlmPA5Y+JfEvoKmZzQduA74HqpUcnHMvOOdSnHMpLVu2rM6qdU7mlkzaJLUJ6LiYAzoNoEPzDjw5+8mAtVke5xxmxj/P/SdPnh/47XU+pDN/Ou1PjJ03lt7jerM5Z3ON23ow/UFOf+V01mxfE8AKRUSkLqjKXdxvA7nOuULw3UtmZonOuZxK1lsLtCvxui3wa8kFnHM7gWv97RqQ5f+SIMjcnEmH5h0C2mZsTCz/vey/HHbQYQFtt7R12eu46L8X8Vzqc/Ro26NWXYGUJzYmln+e+0+OP/h4rp96Pd1e7MbUQVM5odUJ1WpnwfoFPDPnGW7sciOHNzk84HWKiEh0q8qZsxlAgxKvGwCfVWG9b4Gjzay9mcUDg4CpJRcwsyb+eQA3ALP8gU0CzDlH5pZMOjavXTcaZenSugstElvgnAvo/VrF8gryuHTipSzdvJQGcQ0qX6GWrjrxKj6/9nP2Fu7l45UfV2vdIlfELdNvoWmDpjxyziNBqlBERKJZVcJZgnNuX2dW/u8TK1vJOVcA3Ap8DCwFJjrnlpjZcDMb7l/sWGCJmS3D91TnyOL1zey/+C6jdjSztWZ2fVXflBxoU84mtudur3UfZ+VZs30N3V/qzqerPg1ou845Rkwfwey1s0m7OI0TW50Y0PbL061NNxbdvIi7Tr0LgJVbV1YpeKbNT+Orn7/i3+f9W/eaiYhIjVTlsuZuM+vinPsOwMy6Anuq0rhzbjowvdS0MSW+/xo4upx1B1dlG1I1zRs0J/PWTJokNAlK+4c0OoT1u9bz91l/57wjzgvYZcfnvn2Ol79/mfvPvJ/LO10ekDarqjhc/Zr9KykvpnD+kefzSv9XSIwr//8mewr20PvI3lxz0jWhKlNERKJMVcLZHcAkMyu+X6w1cEXQKpKgiI2JDfj9ZiXVr1efe06/h9s+vI1Za2ZxdvLZtW7TOceMrBlc1OEiRvUaFYAqa6Z1o9bce/q9/HnGn1mxdQXvDXqP5VuWkzquf5ljeN6ccnNQ7okTEZG6wapyqcbM4oCO+J7AXOacyw92YTWRkpLi5s6d63UZYWnKsims37Wem1JuCto29uTvof1T7Tn+4OP57Jqq3JZYucKiQvIK8yo8WxUq036cxpXvXEm9mHrsyTFyX38HVvf8bYHkDBKHDmDa0In0at/LszpFRCQymNk851xK6elV6edsBNDQObfYObcIaGRmtwSjSAmeV+a/wjNzngnqNhrENeDu0+5mRtYMZq+dXeN2du/dzbVTrmXtzrXExsSGRTAD6NuhL09d+BTbs/ceGMwAVvckJ20SfdMGkp6V7kmNIiIS+aryQMCNzrntxS+cc9uAG4NWkQRFIAY8r4rhKcMZ23csJx9yco3Wd85x7ZRreX3B6yzZuCSwxQXALVNH4t6cdmAwK+YPaKnjSg+GISIiUjVVCWcxVuIGGv+YmfEVLC9hJr8wn5XbVgalG43SGsU3YljXYSTUS6jR+v/84p9M+mES/zznn5x/1PkBrq72cl12+cGs2Oqe+92LJiIiUh1VCWcfAxPN7Bwz+x3wX+DD4JYlgZS1PYuCooKQhLNiry94nZEfjqx8wRI++PED7p95P4OPH8wfT/tjkCoTEREJb1UJZ/fg64j2ZmAEsJD9O6WVMJe1zTfoQiguaxZbsXUFT895msUbF1dpeeccD3/+MCcfcjIv9XtJTzuKiEidVWk4c84VAbOBVUAKcA6+TmUlQpx/1Pns+vMuurbuGrJtjuwxkkbxjfjH5/+o0vJmxsdXf8z7g98PmwcAypJgSZCcUfFCyRm+5URERGqg3HBmZh3M7AEzWwo8C/wM4Jzr5Zx7NlQFSmA0jG9IXGxcyLbXPLE5t6TcwoQlE/hxy4/lLldYVMgTXz/Bnvw9NK7fmDaN24SsxpqYPmQKiUMHlB/Q/N1pTB8yJaR1iYhI9KjozNkyfGfJLnLOneGcewYoDE1ZEkh/mfEXxswdU/mCAXbXaXdRP7Z+hWfP/pr+V+765C6mZk4td5lw0qt9L6YNnVh2QFM/ZyIiEgAVjRBwGb7BytPN7CNgPL5OaCXCvPz9y/Q5ug/DU4ZXvnAAHdzwYB4991HaN21f5vwJiyfwzy/+ybAuw7ji+MgZdKI4oKXG9DtghIBpQ6YomImISK2UG86cc5OByWbWELgY+APQysyeByY75z4JTYlSGztyd7Bh94aQPgxQ0m09biM9K50GDzXeL8jE0whi9nJ6u9N5JjW4neMGQ6/2vdjzwE6vyxARkShU6diazrndwJvAm2bWDBgA3AsonEWAzC2ZACHtRqOk9Kx0+qQNJDdt6n79g+1NzsCu6ssfTvkD8bHqNk9ERKRYVbrS2Mc5t9U5N9Y597tgFSSBlbnZH848OHOWnpVO37SB7EmbVOZQR+7NaVwzcbiGOhIRESmhWuFMIs/u/N20TGzJEU2PCPm2U8f1J6esYFZMQx2JiIgcQOEsyg1PGc7GP2705NKhhjoSERGpPoUzERERkTCicBbFilwRp718Gm8sfMPrUkRERKSKFM6i2E87fuLrtV+Tk5/jyfY11JGIiEj1KZxFseJhk7zqRkNDHYmIiFSfwlkU87IbDdBQRyIiIjVRaSe0Erkyt2TSuH5jWjVs5VkNGupIRESkehTOotghjQ7hog4XYebtkKga6khERKTqFM6i2P1n3e91CSIiIlJNuudMREREJIwonEWpBesX0Oo/rZiZNdPrUkRERKQaFM6iVOaWTDbu3kiLxBZelyIiIiLVoHAWpYq70Tiq2VEeVyIiIiLVoXAWpTK3ZHLYQYeRGJfodSkiIiJSDQpnUSpzS6ZnIwOIiIhIzakrjSh14VEX0iapjddliIiISDUpnEWph3o95HUJIiIiUgO6rBmFcvJzyC3I9boMERERqQGFsyg0bsE4Eh9J5Jedv3hdioiIiFSTwlkUytySSUK9BFontfa6FBEREakmhbMolLklkw7NOxBj+vGKiIhEGv31jkKZmzPp2ELdaIiIiEQihbMok1eQR9b2LDo06+B1KSIiIlID6kojyhQUFfDv8/7N6e1O97oUERERqQGFsyjTML4hd556p9dliIiISA3psmaUydqWxaptq7wuQ0RERGpI4SzK/H3W3zn9FV3SFBERiVQKZ1FGA56LiIhENoWzKJO5WeFMREQkkimcRZEtOVvYsmeL+jgTERGJYApnUeTHLT8C6MyZiIhIBFM4iyIdmndg4uUT6dG2h9eliIiISA2pn7Mo0jyxOQOOG+B1GSIiIlILOnMWRT5d+Slzf53rdRkiIiJSCwpnUWTkRyN5eNbDXpchIiIitaBwFiUKiwpZsXWFHgYQERGJcApnUWL19tXkF+WrGw0REZEIp3AWJTK3ZALqRkNERCTSKZxFiczNvnDWoXkHjysRERGR2lBXGlHius7XcWq7U2mR2MLrUkRERKQWFM6ixEEJB3FK21O8LkNERERqSZc1o8SjXzzK1z9/7XUZIiIiUktBDWdmdoGZZZrZCjO7t4z5Tc1sspktNLM5ZnZ8VdeV32TnZXPvjHv535r/eV2KiIiI1FLQwpmZxQLPARcCnYDBZtap1GL3AfOdcycC1wBPVWNd8Sse8FwPA4iIiES+YJ456w6scM6tcs7tBcYD/Ust0wmYAeCcWwYkm1mrKq4rfupGQ0REJHoEM5y1AX4u8Xqtf1pJC4BLAcysO3A40LaK6+Jfb5iZzTWzuZs2bQpQ6ZElc3MmMRbDUc2O8roUERERqaVghjMrY5or9fpfQFMzmw/cBnwPFFRxXd9E515wzqU451JatmxZi3Ij16rtq0hukkz9evW9LkVERERqKZhdaawF2pV43Rb4teQCzrmdwLUAZmZAlv8rsbJ15TdpF6exbc82r8sQERGRAAjmmbNvgaPNrL2ZxQODgKklFzCzJv55ADcAs/yBrdJ15TcxFkPzxOZelyEiIiIBELRw5pwrAG4FPgaWAhOdc0vMbLiZDfcvdiywxMyW4Xsyc2RF6war1ki2ftd6rp9yPd+v+97rUkRERCQAgjpCgHNuOjC91LQxJb7/Gji6quvKgRZvXMwr81/hqhOv8roUERERCQCNEBDhigc8VzcaIiIi0UHhLMJlbsmkUXwjDk061OtSREREJAAUziJc5pZMOjTvgO9hVxEREYl0CmcRrrCokOMPPr7yBUVERCQiBPWBAAm+z675DOfK7J9XREREIpDOnEUBXdIUERGJHgpnEeyjFR/Re1xvftn5i9eliIiISIAonEWweb/O49NVn9IkoYnXpYiIiEiAKJxFsMwtmbRt3JaG8Q29LkVEREQCROEsgmVuyVTnsyIiIlFG4SxCOef4ccuPCmciIiJRRuEsQu0p2EOX1l3o1qab16WIiIhIAKmfswiVGJfIjGtmeF2GiIiIBJjOnImIiIiEEYWzCPWXGX+hy9guGh1AREQkyiicRahFGxeRX5Sv0QFERESijMJZhFI3GiIiItFJ4SwC5Rfms2rbKoUzERGRKKRwFoFWbVtFQVEBHZp38LoUERERCTCFswgUGxPL0JOG0vXQrl6XIiIiIgGmfs4i0FHNjuK1i1/zugwREREJAp05i0DZednqQkNERCRK6cxZFaRnpZM6rj+5LnvftARLYvqQKfRq3yvk9fR5qw8J9RL4ZMgnId+2iIiIBJfCWSXSs9LpmzaQ3LSpsLrnvum5yRn0LRrAtKETQx7QMrdkclGHi0K6TREREQkNXdasQHEwy0mbtF8wA2B1T3LSJtE3bSDpWekhq2l77nY27t6objRERESilMJZBVLH9S87mBXzB7TUcf1DVlPm5kwAOrZQOBMREYlGCmcVyHXZ5QezYqt77ncvWrBlbvGHM505ExERiUoKZxHmhINP4L4z7uOIpkd4XYqIiIgEgR4IiDCdW3emc+vOXpchIiIiQaIzZxVIsCRIzqh4oeQM33IhsmjDInbm7QzZ9kRERCS0FM4qMH3IFBKHDig/oCVnkDh0ANOHTAlJPUWuiG4vduOh/z0Uku2JiIhI6CmcVaBX+15MGzqx7ICWnEHCNZeFtJ+zn3b8RF5hnh4GEBERiWK656wSxQEtNaaf5yMEqBsNERGR6KdwVgW92vdizwMH3udVUFTA2z+8zWXHXoaZBb0OdaMhIiIS/XRZsxYmLJ7AgEkDmLBkQki2l7k5k4PqH8TBDQ8OyfZEREQk9BTOamHQ8YPo2rord358Z0ieoLyx64283O/lkJylExEREW8onNVCbEwsz/d5nvW71vNg+oNB397Jh5zMZZ0uC/p2RERExDsKZ7XUrU03bup6E8/MeYYF6xcEbTt78vfw7tJ32bBrQ9C2ISIiIt5TOAuAf5zzD8447AzyCvOCto1lm5dx2cTLmLVmVtC2ISIiIt7T05oB0LRBUzJ+nxHUbex7UlPdaIiIiEQ1nTkLoJ15O7lvxn1sydkS8LYzN2diGEc3OzrgbYuIiEj4UDgLoDXb1/DYl49x34z7At72j1t/5LCDDqNBXIOAty0iIiLhQ+EsgE5odQIje4zkxe9e5Ju13wS07czNmbqkKSIiUgeYc87rGgImJSXFzZ0719MasvOyOea5Y2jVsBXf3vgtsTGxAWl31bZV5Bbk0qllp4C0JyIiIt4ys3nOuZTS03XmLMCS6ifx5PlP8v3673l+7vMBa/eIpkcomImIiNQBelozCAZ0GsDCMxfS+8jeAWlv2eZlfLziY64+8WqaJzYPSJsiIiISnnTmLAjMjId/9zAdmncISHsZqzO44+M7yMnPCUh7IiIiEr4UzoJo0+5NXDrhUtKz0mvVTubmTBLjEmnTuE2AKhMREZFwpXAWRI3iGzF//XxGTB/B3sK9NW4nc0smRzc7mhjTj0tERCTa6a99EDWIa8DTFz7N0s1L+b/Z/1fjdjK3qBsNERGRukLhLMj6duhL/479GfW/Ufy046dqr7+3cC9rtq+hY3OFMxERkbpA4SwEnrrgKZxzPJD+QLXXjY+NZ8e9O7jz1DuDUJmIiIiEG3WlEQKHNzmcd694l26HdqvR+g3jGwa4IhEREQlXOnMWIhccdQHNE5tTWFRYrYcD3vnhHf706Z8ockVBrE5ERETChcJZCO3eu5vuL3XnkVmPVHmdacun8cbCN/SkpoiISB2hv/gh1DC+IR2bd+TRLx9lxdYVVVpHA56LiIjULQpnIfZ478eJj43n1um3UpVB5zO3ZOpJTRERkTokqOHMzC4ws0wzW2Fm95Yx/yAze9/MFpjZEjO7tsS8kWa22D/9jmDWGUqtk1rz915/5+OVH/Pu0ncrXHZzzma27tmqcCYiIlKHBC2cmVks8BxwIdAJGGxmnUotNgL4wTl3EtATeNzM4s3seOBGoDtwEtDXzI4OVq2hNqL7CE5qdRJj5o2pcLlfs3+leYPmARujU0RERMJfMLvS6A6scM6tAjCz8UB/4IcSyzggycwMaARsBQqAY4HZzrkc/7r/Ay4BHgtivSFTL6YeUwZNoXVS6wqXO7HViWz+0+YqXf4UERGR6BDMy5ptgJ9LvF7rn1bSs/iC2K/AImCkc64IWAycZWbNzSwRSAXalbURMxtmZnPNbO6mTZsC/R6C5vAmhxMfG8+uvbv4ecfPFS7ry64iIiJSFwQznJWVKEqfAjofmA8cCpwMPGtmjZ1zS4FHgU+Bj4AF+M6oHdigcy8451KccyktW7YMUOmh4Zzj9FdO5+rJV5d5duzuT+7mT5/+yYPKRERExCvBDGdr2f9sV1t8Z8hKuhZ41/msALKAYwCccy8757o4587Cd7lzeRBr9YSZcWu3W5m1ZhZvLHzjgPnTl09n+daoe9siIiJSgWCGs2+Bo82svZnFA4OAqaWW+Qk4B8DMWgEdgeJ71A72/3sYcCnw3yDW6pnru1xPjzY9uPvTu9m2Z9u+6QVFBazYukJPaoqIiNQxQQtnzrkC4FbgY2ApMNE5t8TMhpvZcP9ifwdOM7NFwAzgHufcZv+8d8zsB+B9YIRzbhtRKMZiGN1nNJtzNnP/zPv3TV+9fTX5RfkKZyIiInVMUAc+d85NB6aXmjamxPe/Ar3LWffMYNYWTrq07sItKbeQuSWTgqIC6sXUI3NzJoBGBxAREaljLJq6aUhJSXFz5871uowa2Vu4l7iYODJWZ5A6rj+5LnvfvARLYvqQKfRq38vDCkVERCSQzGyecy6l9PSgnjmTqouPjSc9K50+aQPITZsKq3vum5ebnEHfogFMGzpRAU1ERCTKaWzNMJGelU7ftIHsSXt7v2AGwOqe5KRNom/aQNKz0j2pT0REREJD4SxMpI7rT07apAODWTF/QEsd1z+kdYmIiEhoKZyFiVyXXX4wK7a65373oomIiEj0UTgTERERCSMKZyIiIiJhROEsTCRYEiRnVLxQcoZvOREREYlaCmdhYvqQKSQOHVB+QEvOIHHoAKYPmRLSukRERCS0FM7CRK/2vZg2dGLZAc0fzNTPmYiISPRTJ7RhpDigpcb0O2CEgGkaIUBERKROUDgLM73a92LPAzu9LkNEREQ8osuaIiIiImFE4UxEREQkjCiciYiIiIQRhTMRERGRMKJwJiIiIhJGFM5EREREwojCmYiIiEgYUTgTERERCSPmnPO6hoAxs03AGq/r8FgLYLPXRYQJ7Qsf7YffaF/8RvviN9oXPtoPvwnVvjjcOdey9MSoCmcCZjbXOZfidR3hQPvCR/vhN9oXv9G++I32hY/2w2+83he6rCkiIiISRhTORERERMKIwln0ecHrAsKI9oWP9sNvtC9+o33xG+0LH+2H33i6L3TPmYiIiEgY0ZkzERERkTCicBaBzKydmaWb2VIzW2JmI8tYpqeZ7TCz+f6vB7yoNRTMbLWZLfK/z7llzDcze9rMVpjZQjPr4kWdwWRmHUv8rOeb2U4zu6PUMlF7TJjZK2a20cwWl5jWzMw+NbPl/n+blrPuBWaW6T8+7g1d1cFRzr74t5kt8x//k82sSTnrVvhZijTl7Iu/mdkvJT4HqeWsGzXHRTn7YUKJfbDazOaXs260HRNl/v0Mu98Xzjl9RdgX0Bro4v8+CfgR6FRqmZ7ANK9rDdH+WA20qGB+KvAhYMApwDde1xzk/RELrMfXf06dOCaAs4AuwOIS0x4D7vV/fy/waDn7aiVwBBAPLCj9WYq0r3L2RW+gnv/7R8vaF/55FX6WIu2rnH3xN+DuStaLquOirP1Qav7jwAN15Jgo8+9nuP2+0JmzCOScW+ec+87/fTawFGjjbVVhrT/wuvOZDTQxs9ZeFxVE5wArnXN1pkNm59wsYGupyf2BNP/3acDFZazaHVjhnFvlnNsLjPevF7HK2hfOuU+ccwX+l7OBtiEvzAPlHBdVEVXHRUX7wcwMGAj8N6RFeaSCv59h9ftC4SzCmVky0Bn4pozZp5rZAjP70MyOC21lIeWAT8xsnpkNK2N+G+DnEq/XEt1hdhDl/6KtK8cEQCvn3Drw/UIGDi5jmbp2bABch+9Mclkq+yxFi1v9l3hfKefyVV06Ls4ENjjnlpczP2qPiVJ/P8Pq94XCWQQzs0bAO8AdzrmdpWZ/h++y1knAM8B7IS4vlE53znUBLgRGmNlZpeZbGetE5WPKZhYP9AMmlTG7Lh0TVVVnjg0AM/sLUAC8Wc4ilX2WosHzwJHAycA6fJf0SqtLx8VgKj5rFpXHRCV/P8tdrYxpQTkuFM4ilJnF4Tuw3nTOvVt6vnNup3Nul//76UCcmbUIcZkh4Zz71f/vRmAyvlPPJa0F2pV43Rb4NTTVhdyFwHfOuQ2lZ9SlY8JvQ/Hla/+/G8tYps4cG2Y2FOgLXOX8N9CUVoXPUsRzzm1wzhU654qAFyn7PdaJ48LM6gGXAhPKWyYaj4ly/n6G1e8LhbMI5L9H4GVgqXPuiXKWOcS/HGbWHd/PekvoqgwNM2toZknF3+O78XlxqcWmAteYzynAjuLT11Go3P8F15VjooSpwFD/90OBKWUs8y1wtJm19591HORfL6qY2QXAPUA/51xOOctU5bMU8Urdb3oJZb/HOnFcAOcCy5xza8uaGY3HRAV/P8Pr94XXT07oq/pfwBn4TqUuBOb7v1KB4cBw/zK3AkvwPU0yGzjN67qDtC+O8L/HBf73+xf/9JL7woDn8D1lswhI8bruIO2LRHxh66AS0+rEMYEvkK4D8vH97/Z6oDkwA1ju/7eZf9lDgekl1k3F98TWyuLjJ5K/ytkXK/DdK1P8+2JM6X1R3mcpkr/K2Rfj/L8HFuL7w9o62o+LsvaDf/prxb8fSiwb7cdEeX8/w+r3hUYIEBEREQkjuqwpIiIiEkYUzkRERETCiMKZiIiISBhROBMREREJIwpnIiIiImFE4UxEpAxmtqvE96lmttzMDvOyJhGpG+p5XYCISDgzs3PwDXfV2zn3k9f1iEj0UzgTESmHmZ2Jb4ifVOfcSq/rEZG6QZ3QioiUwczygWygp3Nuodf1iEjdoXvORETKlg98hW/IHxGRkFE4ExEpWxEwEOhmZvd5XYyI1B2650xEpBzOuRwz6wt8bmYbnHMve12TiEQ/hTMRkQo457aa2QXALDPb7Jyb4nVNIhLd9ECAiIiISBjRPWciIiIiYUThTERERCSMKJyJiIiIhBGFMxEREZEwonAmIiIiEkYUzkRERETCiMKZiIiISBhROBMREREJI/8PgRRGSNCxQkgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = []\n",
    "from sklearn import metrics \n",
    "for i in range(1,21):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i).fit(X_train,Y_train)\n",
    "    knn_predict = knn.predict(X_validation)\n",
    "    accuracy.append(metrics.accuracy_score(Y_validation,knn_predict))\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(range(1,21),accuracy,color = 'green',linestyle = 'dashed', marker = '8', markerfacecolor='blue', markersize = 10)\n",
    "plt.title('Accuracy vs K value') \n",
    "plt.xlabel('K') \n",
    "plt.ylabel('Accuracy')\n",
    "print(\"Maximum Accuracy:\", max(accuracy),\"at K = \" , accuracy.index(max(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5b0b500-b033-4341-9706-a565a4a1df7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Specifications          Score\n",
      "13            ram  931267.519053\n",
      "11      px_height   17363.569536\n",
      "0   battery_power   14129.866576\n",
      "12       px_width    9810.586750\n",
      "8       mobile_wt      95.972863\n",
      "6      int_memory      89.839124\n",
      "15           sc_w      16.480319\n",
      "16      talk_time      13.236400\n",
      "4              fc      10.135166\n",
      "14           sc_h       9.614878\n",
      "10             pc       9.186054\n",
      "9         n_cores       9.097556\n",
      "18   touch_screen       1.928429\n",
      "5          four_g       1.521572\n",
      "7           m_dep       0.745820\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2 \n",
    "#get the k optimal value that we just looped just now \n",
    "bestfeatures = SelectKBest(score_func = chi2,k=17) \n",
    "fit = bestfeatures.fit(X,Y) \n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns) \n",
    "# making sort of a datatable to visualize the values \n",
    "score_table = pd.concat([dfcolumns,dfscores],axis = 1)\n",
    "# name the table column\n",
    "score_table.columns = ['Specifications','Score'] \n",
    "# print 15 best features \n",
    "print(score_table.nlargest(15,'Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2d4ea96-be85-44ae-822b-2b66cf1492bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_table['three_g']\n",
    "del data_table['wifi']\n",
    "del data_table['dual_sim'] \n",
    "del data_table['clock_speed'] \n",
    "del data_table['blue'] \n",
    "\n",
    "##since im deleting three_g, its best to delete four_g too since from the heatmap theres some correlation\n",
    "del data_table['four_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bd5964e-ba67-44b0-b47b-cba15f34a9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>fc</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>794</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>106</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>1222</td>\n",
       "      <td>1890</td>\n",
       "      <td>668</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>915</td>\n",
       "      <td>1965</td>\n",
       "      <td>2032</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1911</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.7</td>\n",
       "      <td>108</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>868</td>\n",
       "      <td>1632</td>\n",
       "      <td>3057</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1512</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>0.1</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>336</td>\n",
       "      <td>670</td>\n",
       "      <td>869</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>510</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0.9</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>483</td>\n",
       "      <td>754</td>\n",
       "      <td>3919</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  fc  int_memory  m_dep  mobile_wt  n_cores  pc  px_height  \\\n",
       "0               842   1           7    0.6        188        2   2         20   \n",
       "1              1021   0          53    0.7        136        3   6        905   \n",
       "2               563   2          41    0.9        145        5   6       1263   \n",
       "3               615   0          10    0.8        131        6   9       1216   \n",
       "4              1821  13          44    0.6        141        2  14       1208   \n",
       "...             ...  ..         ...    ...        ...      ...  ..        ...   \n",
       "1995            794   0           2    0.8        106        6  14       1222   \n",
       "1996           1965   0          39    0.2        187        4   3        915   \n",
       "1997           1911   1          36    0.7        108        8   3        868   \n",
       "1998           1512   4          46    0.1        145        5   5        336   \n",
       "1999            510   5          45    0.9        168        6  16        483   \n",
       "\n",
       "      px_width   ram  sc_h  sc_w  talk_time  touch_screen  price_range  \n",
       "0          756  2549     9     7         19             0            1  \n",
       "1         1988  2631    17     3          7             1            2  \n",
       "2         1716  2603    11     2          9             1            2  \n",
       "3         1786  2769    16     8         11             0            2  \n",
       "4         1212  1411     8     2         15             1            1  \n",
       "...        ...   ...   ...   ...        ...           ...          ...  \n",
       "1995      1890   668    13     4         19             1            0  \n",
       "1996      1965  2032    11    10         16             1            2  \n",
       "1997      1632  3057     9     1          5             1            3  \n",
       "1998       670   869    18    10         19             1            0  \n",
       "1999       754  3919    19     4          2             1            3  \n",
       "\n",
       "[2000 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538667ad-4865-465e-9a2c-080e8266ebff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:darkblue\"> Training the models </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59b7c418-993d-48d4-8f04-1b9742e2a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X and Y again but convert to numpy and split!\n",
    "Y = data_table['price_range'].to_numpy()\n",
    "del data_table['price_range']\n",
    "X = data_table.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92e87285-fc32-4a2a-9963-5a2cf7dea7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "validation_size = 0.30\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d0225-e421-42f6-b3a6-ab694b42f6e7",
   "metadata": {},
   "source": [
    "### Normal Default configurations of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4885e43c-4a1f-49a1-ad10-12db839678aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normal Default configurations of models\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier , RandomForestClassifier  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa0840d8-e13c-467d-b656-0ecf71495b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hafsah/opt/anaconda3/envs/ai/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of  LogisticR : 65.16666666666666\n",
      "Accuracy of  KNN : 91.16666666666666\n",
      "Accuracy of  SVM : 95.0\n",
      "Accuracy of  GTB : 88.66666666666667\n",
      "Accuracy of  RF : 87.33333333333333\n",
      "Accuracy of  DTC : 82.33333333333334\n",
      "There are 6 models we are going to try out!! \n"
     ]
    }
   ],
   "source": [
    "# prepare models\n",
    "from sklearn import ensemble\n",
    "models = []\n",
    "models.append(('LogisticR', LogisticRegression()))\n",
    "#models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "#models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('GTB', ensemble.GradientBoostingClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "#models.append((\"SGDClassifier\",SGDClassifier()))\n",
    "models.append(('DTC', DecisionTreeClassifier()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    model.fit(X_train,Y_train)\n",
    "    pred = model.predict(X_validation)\n",
    "    accuracy_model = accuracy_score(Y_validation,pred)\n",
    "    print(\"Accuracy of \" ,name, \":\", accuracy_model * 100)\n",
    "print(\"There are\" , len(models) , \"models we are going to try out!! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff4dae-dcc6-4159-b640-693dbb5a9866",
   "metadata": {},
   "source": [
    "### Hyper Tuning Parameters for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7699fe5-8e95-4e3f-a4d0-f37d48f8c399",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705935f8-9d4e-4aac-b5bf-02c09d570c79",
   "metadata": {},
   "source": [
    "#### Building  Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df69acab-2b93-4eb3-98e8-a765b63c1271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuravy of the logistic Regression model:  65.16666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hafsah/opt/anaconda3/envs/ai/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logR = LogisticRegression()\n",
    "logR.fit(X_train,Y_train)\n",
    "pred = logR.predict(X_validation)\n",
    "accuracy_LR = accuracy_score(Y_validation,pred)\n",
    "print(\"Accuravy of the logistic Regression model: \", accuracy_LR * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964373c6-7e36-40e1-8453-597e8037c06f",
   "metadata": {},
   "source": [
    "#### HyperTuning Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4324d21-7730-4bca-ac64-78491f9fcf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuravy of the logistic Regression model:  96.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hafsah/opt/anaconda3/envs/ai/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logR2 = LogisticRegression(solver='newton-cg')\n",
    "logR2.fit(X_train,Y_train)\n",
    "pred2 = logR2.predict(X_validation)\n",
    "accuracy_LR2 = accuracy_score(Y_validation,pred2)\n",
    "print(\"Accuravy of the logistic Regression model: \", accuracy_LR2 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ae4ebf9-2042-4f07-978d-f0efec80ca8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/trained_mobile_logisticRegressor.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(logR2,'models/trained_mobile_logisticRegressor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53990251-7bbd-4ce0-8cc2-e161fe124c9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. KNeighborsClassifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae893c38-f304-4c13-a2a2-2285ac5e7f35",
   "metadata": {},
   "source": [
    "#### Building KNeighborsClassifier model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "befe2895-4933-4cca-bdb6-05401a9c085e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6405be0-e015-4277-a551-fb2be50fe574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the KNeighbors Classifier model:  91.16666666666666\n"
     ]
    }
   ],
   "source": [
    "knn_predict = knn.predict(X_validation)\n",
    "accuracy_KN = accuracy_score(Y_validation,knn_predict)\n",
    "print(\"Accuracy of the KNeighbors Classifier model: \", accuracy_KN * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca8d125f-d953-461f-998b-92180ceebd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/trained_mobile_KNNClassifier.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(knn,'models/trained_mobile_KNNClassifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22527db-5faf-4237-aa06-933c28db3ad5",
   "metadata": {},
   "source": [
    "#### HyperTuning KNeighborsClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29665872-68f3-47ef-a1f2-fab895d0d1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9349999999999999\n",
      "{'leaf_size': 1, 'n_neighbors': 16, 'weights': 'distance'}\n",
      "KNeighborsClassifier(leaf_size=1, n_neighbors=16, weights='distance')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "k_range = list(range(1,21))\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "\n",
    "param_grid = dict(n_neighbors = k_range, weights = weight_options,leaf_size=[1,2,4,5,10,15])\n",
    "#create a new knn object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "# use GridSearch\n",
    "grid = GridSearchCV(knn, param_grid, cv = 10, scoring = 'accuracy')\n",
    "#fit the model\n",
    "best_model = grid.fit(X_train,Y_train)\n",
    "\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print (grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adccbbdd-5db0-4559-a4e9-3fedc867137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predict1 = best_model.predict(X_validation)\n",
    "accuracy_KN2 = accuracy_score(Y_validation,knn_predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f3ba014-87e5-4af8-ab98-a8c8ab28b16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, 1, 0, 2, 3, 1, 1, 2, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 2, 1,\n",
       "       1, 1, 2, 1, 3, 1, 2, 2, 3, 0, 0, 3, 3, 2, 3, 2, 1, 3, 2, 3, 2, 0,\n",
       "       2, 1, 0, 3, 1, 0, 1, 3, 2, 0, 2, 2, 1, 2, 3, 3, 1, 1, 1, 3, 0, 3,\n",
       "       3, 1, 0, 3, 0, 0, 0, 1, 1, 0, 2, 0, 0, 3, 1, 3, 3, 1, 1, 0, 1, 2,\n",
       "       1, 3, 2, 3, 3, 0, 0, 3, 1, 0, 3, 1, 2, 0, 3, 2, 0, 1, 0, 2, 2, 2,\n",
       "       3, 3, 2, 0, 0, 2, 0, 2, 0, 3, 2, 1, 0, 2, 3, 0, 1, 0, 0, 2, 3, 2,\n",
       "       1, 0, 3, 1, 2, 1, 1, 0, 1, 0, 2, 3, 3, 1, 0, 1, 2, 0, 2, 0, 3, 1,\n",
       "       1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 0, 0, 2, 3, 2, 3, 0, 1, 1, 0, 1, 0,\n",
       "       3, 2, 1, 0, 2, 2, 2, 0, 0, 3, 1, 2, 0, 0, 0, 2, 2, 2, 1, 0, 0, 2,\n",
       "       3, 0, 0, 2, 2, 0, 0, 1, 3, 0, 1, 0, 0, 2, 2, 1, 0, 2, 3, 3, 1, 3,\n",
       "       2, 0, 3, 1, 3, 2, 2, 2, 0, 3, 2, 0, 2, 0, 1, 0, 3, 1, 0, 3, 3, 3,\n",
       "       1, 1, 1, 2, 2, 3, 0, 1, 3, 0, 0, 1, 2, 2, 3, 3, 3, 1, 1, 1, 1, 3,\n",
       "       0, 3, 1, 2, 2, 3, 2, 3, 3, 0, 0, 3, 3, 2, 1, 1, 3, 3, 1, 1, 3, 2,\n",
       "       0, 3, 2, 3, 2, 2, 1, 1, 0, 0, 2, 1, 2, 0, 0, 3, 3, 3, 1, 1, 1, 1,\n",
       "       1, 1, 0, 3, 3, 0, 1, 0, 2, 0, 3, 3, 0, 2, 2, 3, 0, 0, 1, 3, 2, 2,\n",
       "       0, 1, 2, 1, 1, 0, 0, 1, 3, 2, 0, 1, 3, 2, 1, 2, 0, 3, 2, 3, 1, 2,\n",
       "       3, 1, 2, 3, 1, 2, 0, 0, 0, 2, 1, 1, 2, 3, 0, 3, 0, 3, 0, 2, 2, 1,\n",
       "       2, 0, 2, 3, 2, 3, 3, 0, 2, 2, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 3, 2,\n",
       "       2, 0, 3, 3, 0, 2, 3, 0, 3, 0, 2, 3, 3, 3, 2, 0, 3, 1, 3, 3, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 1, 1, 3, 1, 3, 3, 0, 0, 0, 0, 1, 1, 1, 3, 0, 1,\n",
       "       3, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 3, 2, 0, 3, 2, 2, 3, 1, 3, 2, 1,\n",
       "       3, 2, 0, 0, 2, 0, 1, 1, 2, 0, 3, 0, 2, 1, 3, 0, 0, 3, 3, 3, 0, 1,\n",
       "       3, 3, 1, 1, 3, 3, 2, 0, 3, 0, 2, 0, 2, 2, 1, 1, 2, 2, 2, 0, 2, 2,\n",
       "       3, 0, 2, 0, 0, 0, 1, 3, 1, 0, 1, 2, 2, 0, 1, 3, 0, 2, 1, 1, 0, 1,\n",
       "       0, 2, 0, 1, 0, 2, 1, 2, 0, 0, 1, 1, 0, 2, 1, 2, 1, 1, 1, 3, 0, 2,\n",
       "       3, 1, 3, 3, 1, 1, 1, 2, 2, 0, 3, 0, 0, 3, 2, 0, 1, 3, 2, 2, 1, 2,\n",
       "       2, 0, 0, 0, 1, 3, 2, 1, 3, 1, 0, 1, 0, 2, 2, 3, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_predict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c188d3b-e696-4a22-920e-fd0ac26ccdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the KNeighbors Classifier model:  92.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the KNeighbors Classifier model: \", accuracy_KN2 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50168aca-dd7c-4bef-9ba5-aa227903fdf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Gradient Tree Boosting Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99062d6f-e7ec-4d62-89cc-39fd0a31f65b",
   "metadata": {},
   "source": [
    "#### Building Gradient Tree Boosting Classifier Model (Ensemble Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03c0d46b-af4b-443c-8feb-d2b0e776f6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the gradient tree boosting classifier model:  88.66666666666667\n"
     ]
    }
   ],
   "source": [
    "model1 = ensemble.GradientBoostingClassifier()\n",
    "model1.fit(X_train,Y_train)\n",
    "pred = model1.predict(X_validation)\n",
    "accuracy_GT1 = accuracy_score(Y_validation,pred)\n",
    "print(\"Accuracy of the gradient tree boosting classifier model: \", accuracy_GT1 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd4399d-84e7-4a8f-be79-698033285a67",
   "metadata": {},
   "source": [
    "#### HyperTuning Gradient Tree Boosting Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17d4131b-07b5-400a-ae45-8758078623e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## willl be using grid search here to tune hyper parameters\n",
    "from sklearn import ensemble\n",
    "model2 = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "# Parameters we want to try\n",
    "param_grid = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",\"squared_error\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[1000,3000],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bfcbd6d-3534-4d8b-a1a4-123e1432ff9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 392 candidates, totalling 1960 fits\n",
      "[CV 3/5; 1/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 1/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.861 total time=   3.2s\n",
      "[CV 1/5; 2/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 2/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.850 total time=   3.2s\n",
      "[CV 5/5; 2/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 2/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.857 total time=   3.2s\n",
      "[CV 4/5; 3/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 3/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.857 total time=   3.5s\n",
      "[CV 3/5; 4/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 4/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.857 total time=   3.6s\n",
      "[CV 2/5; 5/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 5/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.896 total time=   3.6s\n",
      "[CV 1/5; 6/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 6/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.861 total time=   3.7s\n",
      "[CV 5/5; 6/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 6/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.882 total time=   3.8s\n",
      "[CV 4/5; 7/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 7/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.854 total time=   3.8s\n",
      "[CV 3/5; 8/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 8/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.896 total time=  10.2s\n",
      "[CV 2/5; 9/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 9/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.925 total time=  11.1s\n",
      "[CV 2/5; 10/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 10/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.914 total time=  12.3s\n",
      "[CV 1/5; 11/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 11/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.875 total time=  12.7s\n",
      "[CV 5/5; 11/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 11/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.914 total time=  13.0s\n",
      "[CV 4/5; 12/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 12/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.854 total time=  13.3s\n",
      "[CV 3/5; 13/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 13/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.886 total time=  13.6s\n",
      "[CV 2/5; 14/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 14/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.914 total time=  13.3s\n",
      "[CV 1/5; 15/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 15/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.839 total time=   3.8s\n",
      "[CV 2/5; 15/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 15/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.896 total time=   3.8s\n",
      "[CV 3/5; 15/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 15/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.854 total time=   3.8s\n",
      "[CV 5/5; 15/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 15/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.875 total time=   3.9s\n",
      "[CV 4/5; 16/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 16/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.857 total time=   4.1s\n",
      "[CV 3/5; 17/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 17/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.857 total time=   4.4s\n",
      "[CV 2/5; 18/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 18/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.896 total time=   4.9s\n",
      "[CV 1/5; 19/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 19/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.857 total time=   5.6s\n",
      "[CV 5/5; 19/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 19/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.871 total time=   6.3s\n",
      "[CV 4/5; 20/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 20/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.857 total time=   5.6s\n",
      "[CV 3/5; 21/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 21/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.861 total time=   4.9s\n",
      "[CV 2/5; 22/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 22/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.914 total time=  14.2s\n",
      "[CV 1/5; 23/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 23/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.871 total time=  14.3s\n",
      "[CV 5/5; 23/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 23/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.904 total time=  13.5s\n",
      "[CV 4/5; 24/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 24/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.850 total time=  13.6s\n",
      "[CV 3/5; 25/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 25/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.882 total time=  13.9s\n",
      "[CV 2/5; 26/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 26/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.918 total time=  14.5s\n",
      "[CV 1/5; 27/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 27/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.879 total time=  14.6s\n",
      "[CV 5/5; 27/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 27/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.914 total time=  14.8s\n",
      "[CV 4/5; 28/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 28/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.854 total time=  14.1s\n",
      "[CV 1/5; 30/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 30/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.875 total time=   4.3s\n",
      "[CV 5/5; 30/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 30/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.907 total time=   4.3s\n",
      "[CV 4/5; 31/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 31/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.846 total time=   4.7s\n",
      "[CV 3/5; 32/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 32/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.882 total time=   5.1s\n",
      "[CV 2/5; 33/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 33/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.911 total time=   5.1s\n",
      "[CV 1/5; 34/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 34/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.875 total time=   5.2s\n",
      "[CV 5/5; 34/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 34/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.904 total time=   4.9s\n",
      "[CV 4/5; 35/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 35/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.864 total time=   4.7s\n",
      "[CV 3/5; 36/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 36/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.882 total time=  12.3s\n",
      "[CV 2/5; 37/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 37/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.918 total time=  13.1s\n",
      "[CV 1/5; 38/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 38/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.875 total time=  15.6s\n",
      "[CV 1/5; 39/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 39/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.868 total time=  15.9s\n",
      "[CV 5/5; 39/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 39/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.914 total time=  14.7s\n",
      "[CV 4/5; 40/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 40/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.850 total time=  15.1s\n",
      "[CV 3/5; 41/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 41/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.868 total time=  16.1s\n",
      "[CV 2/5; 42/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 42/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.914 total time=  15.2s\n",
      "[CV 1/5; 43/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 43/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.882 total time=   4.4s\n",
      "[CV 4/5; 1/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 1/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.854 total time=   3.2s\n",
      "[CV 2/5; 2/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 2/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.889 total time=   3.2s\n",
      "[CV 1/5; 3/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 3/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.854 total time=   3.5s\n",
      "[CV 5/5; 3/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 3/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.871 total time=   3.5s\n",
      "[CV 4/5; 4/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 4/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.857 total time=   3.6s\n",
      "[CV 3/5; 5/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 5/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.846 total time=   3.6s\n",
      "[CV 2/5; 6/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 6/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.886 total time=   3.7s\n",
      "[CV 1/5; 7/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 7/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.861 total time=   3.7s\n",
      "[CV 5/5; 7/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 7/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.893 total time=   3.8s\n",
      "[CV 4/5; 8/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 8/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.864 total time=  10.1s\n",
      "[CV 3/5; 9/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 9/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.886 total time=  11.1s\n",
      "[CV 1/5; 10/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 10/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.868 total time=  12.3s\n",
      "[CV 5/5; 10/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 10/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.921 total time=  12.5s\n",
      "[CV 4/5; 11/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 11/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.850 total time=  12.9s\n",
      "[CV 3/5; 12/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 12/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.886 total time=  13.2s\n",
      "[CV 2/5; 13/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 13/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.914 total time=  13.6s\n",
      "[CV 1/5; 14/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 14/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.871 total time=  13.3s\n",
      "[CV 5/5; 14/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 14/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.904 total time=  13.2s\n",
      "[CV 3/5; 16/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 16/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.868 total time=   4.1s\n",
      "[CV 2/5; 17/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 17/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.896 total time=   4.4s\n",
      "[CV 1/5; 18/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 18/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.861 total time=   4.6s\n",
      "[CV 5/5; 18/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 18/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.879 total time=   4.8s\n",
      "[CV 4/5; 19/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 19/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.843 total time=   6.3s\n",
      "[CV 3/5; 20/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 20/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.854 total time=   6.3s\n",
      "[CV 2/5; 21/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 21/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.886 total time=   4.8s\n",
      "[CV 1/5; 22/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 22/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.875 total time=  13.7s\n",
      "[CV 5/5; 22/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 22/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.911 total time=  14.1s\n",
      "[CV 4/5; 23/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 23/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.861 total time=  13.4s\n",
      "[CV 3/5; 24/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 24/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.886 total time=  13.8s\n",
      "[CV 2/5; 25/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 25/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.914 total time=  13.8s\n",
      "[CV 1/5; 26/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 26/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.871 total time=  14.5s\n",
      "[CV 5/5; 26/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 26/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.914 total time=  14.3s\n",
      "[CV 4/5; 27/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 27/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.857 total time=  14.5s\n",
      "[CV 3/5; 28/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 28/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.875 total time=  14.4s\n",
      "[CV 4/5; 29/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 29/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.868 total time=   4.0s\n",
      "[CV 2/5; 30/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 30/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.914 total time=   4.3s\n",
      "[CV 1/5; 31/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 31/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.871 total time=   4.6s\n",
      "[CV 5/5; 31/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 31/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.904 total time=   4.7s\n",
      "[CV 4/5; 32/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 32/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.857 total time=   5.1s\n",
      "[CV 3/5; 33/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 33/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.882 total time=   5.0s\n",
      "[CV 2/5; 34/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 34/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.914 total time=   5.2s\n",
      "[CV 1/5; 35/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 35/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.868 total time=   4.7s\n",
      "[CV 5/5; 35/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 35/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.900 total time=   4.7s\n",
      "[CV 4/5; 36/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 36/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.857 total time=  12.3s\n",
      "[CV 3/5; 37/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 37/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.889 total time=  13.1s\n",
      "[CV 2/5; 38/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 38/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.914 total time=  15.5s\n",
      "[CV 5/5; 38/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 38/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.911 total time=  15.7s\n",
      "[CV 4/5; 39/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 39/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.846 total time=  14.7s\n",
      "[CV 3/5; 40/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 40/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.882 total time=  14.9s\n",
      "[CV 2/5; 41/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 41/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  16.1s\n",
      "[CV 1/5; 42/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 42/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.868 total time=  15.1s\n",
      "[CV 5/5; 42/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 42/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.907 total time=  16.3s\n",
      "[CV 2/5; 44/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 44/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.911 total time=   5.1s\n",
      "[CV 1/5; 45/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 45/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.879 total time=   5.4s\n",
      "[CV 1/5; 1/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 1/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.857 total time=   3.2s\n",
      "[CV 5/5; 1/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 1/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.871 total time=   3.1s\n",
      "[CV 4/5; 2/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 2/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.846 total time=   3.2s\n",
      "[CV 3/5; 3/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 3/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.864 total time=   3.5s\n",
      "[CV 2/5; 4/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 4/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.889 total time=   3.6s\n",
      "[CV 1/5; 5/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 5/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.861 total time=   3.6s\n",
      "[CV 5/5; 5/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 5/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.871 total time=   3.6s\n",
      "[CV 4/5; 6/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 6/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.854 total time=   3.8s\n",
      "[CV 3/5; 7/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 7/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.854 total time=   3.8s\n",
      "[CV 2/5; 8/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 8/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.914 total time=  10.1s\n",
      "[CV 1/5; 9/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 9/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.886 total time=  11.1s\n",
      "[CV 5/5; 9/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 9/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.900 total time=  11.4s\n",
      "[CV 4/5; 10/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 10/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.857 total time=  12.5s\n",
      "[CV 3/5; 11/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 11/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.882 total time=  12.9s\n",
      "[CV 2/5; 12/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 12/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.914 total time=  13.2s\n",
      "[CV 1/5; 13/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 13/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.875 total time=  13.6s\n",
      "[CV 5/5; 13/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 13/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.911 total time=  13.8s\n",
      "[CV 4/5; 14/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 14/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.850 total time=  13.3s\n",
      "[CV 2/5; 16/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 16/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.893 total time=   4.1s\n",
      "[CV 1/5; 17/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 17/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.854 total time=   4.4s\n",
      "[CV 5/5; 17/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 17/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.868 total time=   4.4s\n",
      "[CV 4/5; 18/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 18/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.850 total time=   4.9s\n",
      "[CV 3/5; 19/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 19/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.857 total time=   6.0s\n",
      "[CV 2/5; 20/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 20/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.882 total time=   6.4s\n",
      "[CV 1/5; 21/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 21/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.846 total time=   5.1s\n",
      "[CV 5/5; 21/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 21/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.871 total time=   4.9s\n",
      "[CV 4/5; 22/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 22/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.861 total time=  14.6s\n",
      "[CV 3/5; 23/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 23/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.886 total time=  13.9s\n",
      "[CV 2/5; 24/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 24/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  14.5s\n",
      "[CV 1/5; 25/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 25/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.871 total time=  13.9s\n",
      "[CV 5/5; 25/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 25/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.914 total time=  13.8s\n",
      "[CV 4/5; 26/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 26/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.857 total time=  14.5s\n",
      "[CV 3/5; 27/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 27/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.882 total time=  14.5s\n",
      "[CV 2/5; 28/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 28/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.925 total time=  14.4s\n",
      "[CV 1/5; 29/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 29/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.879 total time=   4.0s\n",
      "[CV 2/5; 29/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 29/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.911 total time=   4.1s\n",
      "[CV 3/5; 29/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 29/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.879 total time=   4.0s\n",
      "[CV 5/5; 29/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 29/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.896 total time=   4.1s\n",
      "[CV 4/5; 30/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 30/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.861 total time=   4.3s\n",
      "[CV 3/5; 31/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 31/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.882 total time=   4.6s\n",
      "[CV 2/5; 32/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 32/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.907 total time=   5.1s\n",
      "[CV 1/5; 33/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 33/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.875 total time=   5.0s\n",
      "[CV 5/5; 33/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 33/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.896 total time=   5.2s\n",
      "[CV 4/5; 34/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 34/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.854 total time=   4.9s\n",
      "[CV 3/5; 35/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 35/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.868 total time=   4.7s\n",
      "[CV 2/5; 36/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 36/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.914 total time=  12.3s\n",
      "[CV 1/5; 37/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 37/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.875 total time=  13.1s\n",
      "[CV 5/5; 37/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 37/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.904 total time=  14.3s\n",
      "[CV 4/5; 38/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 38/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.846 total time=  15.9s\n",
      "[CV 3/5; 39/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 39/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.889 total time=  14.8s\n",
      "[CV 2/5; 40/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 40/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.914 total time=  14.8s\n",
      "[CV 1/5; 41/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 41/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.861 total time=  16.0s\n",
      "[CV 5/5; 41/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 41/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  15.7s\n",
      "[CV 4/5; 42/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 42/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.854 total time=  16.3s\n",
      "[CV 2/5; 1/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 1/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.893 total time=   3.3s\n",
      "[CV 3/5; 2/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 2/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.854 total time=   3.2s\n",
      "[CV 2/5; 3/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 3/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.900 total time=   3.5s\n",
      "[CV 1/5; 4/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 4/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.854 total time=   3.6s\n",
      "[CV 5/5; 4/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 4/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.875 total time=   3.6s\n",
      "[CV 4/5; 5/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 5/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.857 total time=   3.6s\n",
      "[CV 3/5; 6/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 6/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.854 total time=   3.7s\n",
      "[CV 2/5; 7/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 7/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.886 total time=   3.7s\n",
      "[CV 1/5; 8/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 8/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.875 total time=  10.0s\n",
      "[CV 5/5; 8/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 8/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.900 total time=  10.4s\n",
      "[CV 4/5; 9/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 9/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.857 total time=  11.4s\n",
      "[CV 3/5; 10/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 10/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.889 total time=  12.5s\n",
      "[CV 2/5; 11/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 11/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.911 total time=  13.0s\n",
      "[CV 1/5; 12/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 12/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.882 total time=  13.3s\n",
      "[CV 5/5; 12/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 12/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.914 total time=  13.4s\n",
      "[CV 4/5; 13/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 13/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.857 total time=  13.8s\n",
      "[CV 3/5; 14/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 14/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.879 total time=  13.3s\n",
      "[CV 4/5; 15/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 15/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.843 total time=   3.8s\n",
      "[CV 1/5; 16/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 16/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.861 total time=   4.1s\n",
      "[CV 5/5; 16/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 16/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.861 total time=   4.1s\n",
      "[CV 4/5; 17/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 17/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.857 total time=   4.4s\n",
      "[CV 3/5; 18/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 18/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.861 total time=   4.9s\n",
      "[CV 2/5; 19/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 19/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.893 total time=   5.8s\n",
      "[CV 1/5; 20/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 20/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.850 total time=   6.4s\n",
      "[CV 5/5; 20/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 20/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.868 total time=   5.4s\n",
      "[CV 4/5; 21/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 21/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.854 total time=   4.9s\n",
      "[CV 3/5; 22/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 22/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.889 total time=  14.4s\n",
      "[CV 2/5; 23/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 23/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.914 total time=  14.1s\n",
      "[CV 1/5; 24/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 24/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.875 total time=  14.5s\n",
      "[CV 5/5; 24/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 24/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  13.7s\n",
      "[CV 4/5; 25/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 25/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.857 total time=  13.9s\n",
      "[CV 3/5; 26/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 26/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.886 total time=  14.6s\n",
      "[CV 2/5; 27/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 27/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.918 total time=  14.6s\n",
      "[CV 1/5; 28/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 28/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.871 total time=  14.4s\n",
      "[CV 5/5; 28/392] START criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 28/392] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.911 total time=  14.1s\n",
      "[CV 3/5; 30/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 30/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.879 total time=   4.3s\n",
      "[CV 2/5; 31/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 31/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.911 total time=   4.7s\n",
      "[CV 1/5; 32/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 32/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.871 total time=   5.0s\n",
      "[CV 5/5; 32/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 32/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.914 total time=   4.9s\n",
      "[CV 4/5; 33/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 33/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.850 total time=   5.3s\n",
      "[CV 3/5; 34/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 34/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.875 total time=   5.0s\n",
      "[CV 2/5; 35/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 35/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.918 total time=   4.7s\n",
      "[CV 1/5; 36/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 36/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.871 total time=  12.3s\n",
      "[CV 5/5; 36/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 36/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.911 total time=  12.4s\n",
      "[CV 4/5; 37/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 37/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.850 total time=  14.4s\n",
      "[CV 3/5; 38/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 38/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.886 total time=  15.9s\n",
      "[CV 2/5; 39/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 39/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.907 total time=  14.8s\n",
      "[CV 1/5; 40/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 40/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.868 total time=  14.9s\n",
      "[CV 5/5; 40/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 40/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.914 total time=  15.6s\n",
      "[CV 4/5; 41/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 41/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.850 total time=  15.9s\n",
      "[CV 3/5; 42/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 42/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.871 total time=  15.9s\n",
      "[CV 4/5; 43/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 43/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.854 total time=   5.1s\n",
      "[CV 3/5; 44/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 44/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.889 total time=   5.2s\n",
      "[CV 2/5; 45/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 45/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.914 total time=   5.3s\n",
      "[CV 5/5; 45/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 45/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.904 total time=   5.2s\n",
      "[CV 4/5; 46/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 46/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.864 total time=   5.3s\n",
      "[CV 3/5; 47/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 47/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.868 total time=   5.6s\n",
      "[CV 2/5; 48/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 48/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.914 total time=   5.7s\n",
      "[CV 1/5; 49/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 49/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.868 total time=   5.4s\n",
      "[CV 5/5; 49/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 49/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.914 total time=   5.9s\n",
      "[CV 4/5; 50/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 50/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.857 total time=  14.8s\n",
      "[CV 3/5; 51/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 51/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.893 total time=  15.2s\n",
      "[CV 2/5; 52/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 52/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.911 total time=  16.6s\n",
      "[CV 1/5; 53/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 53/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.868 total time=  17.3s\n",
      "[CV 5/5; 53/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 53/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.914 total time=  17.0s\n",
      "[CV 4/5; 54/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 54/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.850 total time=  17.9s\n",
      "[CV 3/5; 55/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 55/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.886 total time=  17.5s\n",
      "[CV 2/5; 56/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 56/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.918 total time=  17.8s\n",
      "[CV 1/5; 57/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 57/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.875 total time=   5.1s\n",
      "[CV 2/5; 57/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 57/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.918 total time=   5.3s\n",
      "[CV 3/5; 57/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 57/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.879 total time=   5.2s\n",
      "[CV 1/5; 58/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 58/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.871 total time=   5.6s\n",
      "[CV 5/5; 58/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 58/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.900 total time=   5.8s\n",
      "[CV 4/5; 59/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 59/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.857 total time=   5.9s\n",
      "[CV 3/5; 60/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 60/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.889 total time=   5.8s\n",
      "[CV 2/5; 61/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 61/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.918 total time=   5.9s\n",
      "[CV 1/5; 62/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 62/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.875 total time=   6.0s\n",
      "[CV 5/5; 62/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 62/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.911 total time=   6.1s\n",
      "[CV 4/5; 63/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 63/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.861 total time=   6.0s\n",
      "[CV 3/5; 64/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 64/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.882 total time=  16.2s\n",
      "[CV 2/5; 65/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 65/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.904 total time=  17.4s\n",
      "[CV 1/5; 66/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 66/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.864 total time=  18.4s\n",
      "[CV 5/5; 66/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 66/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.911 total time=  18.0s\n",
      "[CV 4/5; 67/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 67/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.850 total time=  17.9s\n",
      "[CV 3/5; 68/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 68/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.886 total time=  18.2s\n",
      "[CV 2/5; 69/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 69/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  19.2s\n",
      "[CV 1/5; 70/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 70/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.854 total time=  18.3s\n",
      "[CV 5/5; 70/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 70/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.911 total time=  18.2s\n",
      "[CV 2/5; 72/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 72/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.914 total time=   5.5s\n",
      "[CV 1/5; 73/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 73/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.871 total time=   5.9s\n",
      "[CV 5/5; 73/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 73/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.911 total time=   6.0s\n",
      "[CV 4/5; 74/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 74/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.843 total time=   6.0s\n",
      "[CV 3/5; 75/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 75/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.882 total time=   6.4s\n",
      "[CV 2/5; 76/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 76/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.914 total time=   6.3s\n",
      "[CV 1/5; 77/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 77/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.864 total time=   5.8s\n",
      "[CV 5/5; 77/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 77/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.900 total time=   5.9s\n",
      "[CV 4/5; 78/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 78/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.857 total time=  15.5s\n",
      "[CV 3/5; 79/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 79/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.889 total time=  16.0s\n",
      "[CV 2/5; 80/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 80/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  16.2s\n",
      "[CV 1/5; 81/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 81/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.861 total time=  16.6s\n",
      "[CV 5/5; 81/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 81/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.911 total time=  16.9s\n",
      "[CV 4/5; 82/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 82/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.843 total time=  16.9s\n",
      "[CV 3/5; 83/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 83/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.889 total time=  16.6s\n",
      "[CV 2/5; 84/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 84/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.918 total time=  16.0s\n",
      "[CV 1/5; 85/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 85/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.886 total time=   4.6s\n",
      "[CV 2/5; 85/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 85/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.918 total time=   4.6s\n",
      "[CV 3/5; 85/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 85/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.886 total time=   4.7s\n",
      "[CV 5/5; 85/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 85/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.896 total time=   4.5s\n",
      "[CV 1/5; 44/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 44/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.879 total time=   5.2s\n",
      "[CV 5/5; 44/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 44/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.907 total time=   5.1s\n",
      "[CV 4/5; 45/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 45/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.864 total time=   5.3s\n",
      "[CV 3/5; 46/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 46/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.879 total time=   5.3s\n",
      "[CV 2/5; 47/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 47/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   5.6s\n",
      "[CV 1/5; 48/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 48/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.886 total time=   5.8s\n",
      "[CV 5/5; 48/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 48/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.904 total time=   5.5s\n",
      "[CV 4/5; 49/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 49/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.846 total time=   5.9s\n",
      "[CV 3/5; 50/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 50/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.886 total time=  14.8s\n",
      "[CV 2/5; 51/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 51/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.918 total time=  15.3s\n",
      "[CV 1/5; 52/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 52/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.868 total time=  16.5s\n",
      "[CV 5/5; 52/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 52/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  16.8s\n",
      "[CV 4/5; 53/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 53/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.846 total time=  17.2s\n",
      "[CV 3/5; 54/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 54/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.879 total time=  17.9s\n",
      "[CV 2/5; 55/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 55/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.914 total time=  17.7s\n",
      "[CV 1/5; 56/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 56/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.861 total time=  17.6s\n",
      "[CV 5/5; 56/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 56/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.907 total time=  18.2s\n",
      "[CV 2/5; 58/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 58/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.918 total time=   5.6s\n",
      "[CV 1/5; 59/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 59/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.889 total time=   6.3s\n",
      "[CV 5/5; 59/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 59/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.8s\n",
      "[CV 4/5; 60/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 60/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.850 total time=   5.8s\n",
      "[CV 3/5; 61/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 61/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.871 total time=   5.9s\n",
      "[CV 2/5; 62/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 62/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.918 total time=   6.0s\n",
      "[CV 1/5; 63/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 63/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.9s\n",
      "[CV 5/5; 63/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 63/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.911 total time=   6.1s\n",
      "[CV 4/5; 64/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 64/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.868 total time=  16.3s\n",
      "[CV 3/5; 65/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 65/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.889 total time=  17.3s\n",
      "[CV 2/5; 66/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 66/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.918 total time=  18.4s\n",
      "[CV 1/5; 67/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 67/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.861 total time=  18.3s\n",
      "[CV 5/5; 67/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 67/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.911 total time=  18.1s\n",
      "[CV 4/5; 68/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 68/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.843 total time=  18.1s\n",
      "[CV 3/5; 69/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 69/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.886 total time=  19.2s\n",
      "[CV 2/5; 70/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 70/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.925 total time=  18.3s\n",
      "[CV 1/5; 71/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 71/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.879 total time=   5.3s\n",
      "[CV 2/5; 71/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 71/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.911 total time=   5.2s\n",
      "[CV 3/5; 71/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 71/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.879 total time=   5.4s\n",
      "[CV 1/5; 72/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 72/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.879 total time=   5.6s\n",
      "[CV 5/5; 72/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 72/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.911 total time=   5.4s\n",
      "[CV 4/5; 73/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 73/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.846 total time=   5.9s\n",
      "[CV 3/5; 74/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 74/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.882 total time=   6.1s\n",
      "[CV 2/5; 75/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 75/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   6.4s\n",
      "[CV 1/5; 76/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 76/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.871 total time=   6.3s\n",
      "[CV 5/5; 76/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 76/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.911 total time=   6.1s\n",
      "[CV 4/5; 77/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 77/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.854 total time=   5.9s\n",
      "[CV 3/5; 78/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 78/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.886 total time=  15.5s\n",
      "[CV 2/5; 79/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 79/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.914 total time=  16.1s\n",
      "[CV 1/5; 80/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 80/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.868 total time=  16.2s\n",
      "[CV 5/5; 80/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 80/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.911 total time=  16.3s\n",
      "[CV 4/5; 81/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 81/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.850 total time=  16.9s\n",
      "[CV 3/5; 82/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 82/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.889 total time=  17.0s\n",
      "[CV 2/5; 83/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 83/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.911 total time=  16.6s\n",
      "[CV 1/5; 84/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 84/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.850 total time=  15.9s\n",
      "[CV 5/5; 84/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 84/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.904 total time=  16.1s\n",
      "[CV 3/5; 86/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 86/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.886 total time=   4.8s\n",
      "[CV 2/5; 43/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 43/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.914 total time=   4.6s\n",
      "[CV 3/5; 43/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 43/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.886 total time=   5.1s\n",
      "[CV 5/5; 43/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 43/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.904 total time=   5.0s\n",
      "[CV 4/5; 44/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 44/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.864 total time=   5.1s\n",
      "[CV 3/5; 45/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 45/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.893 total time=   5.3s\n",
      "[CV 2/5; 46/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 46/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.911 total time=   5.3s\n",
      "[CV 1/5; 47/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 47/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.868 total time=   5.5s\n",
      "[CV 5/5; 47/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 47/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.907 total time=   5.7s\n",
      "[CV 4/5; 48/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 48/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.864 total time=   5.5s\n",
      "[CV 3/5; 49/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 49/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.882 total time=   5.8s\n",
      "[CV 2/5; 50/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 50/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.918 total time=  14.9s\n",
      "[CV 1/5; 51/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 51/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.864 total time=  15.3s\n",
      "[CV 5/5; 51/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 51/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.911 total time=  15.3s\n",
      "[CV 4/5; 52/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 52/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.854 total time=  16.8s\n",
      "[CV 3/5; 53/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 53/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.893 total time=  17.3s\n",
      "[CV 2/5; 54/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 54/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.914 total time=  18.0s\n",
      "[CV 1/5; 55/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 55/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.871 total time=  17.7s\n",
      "[CV 5/5; 55/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 55/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.918 total time=  18.1s\n",
      "[CV 4/5; 56/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 56/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.854 total time=  18.2s\n",
      "[CV 5/5; 57/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 57/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.911 total time=   5.3s\n",
      "[CV 4/5; 58/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 58/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.854 total time=   5.8s\n",
      "[CV 3/5; 59/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 59/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.882 total time=   5.9s\n",
      "[CV 2/5; 60/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 60/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   5.9s\n",
      "[CV 1/5; 61/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 61/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.875 total time=   5.9s\n",
      "[CV 5/5; 61/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 61/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.904 total time=   5.9s\n",
      "[CV 4/5; 62/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 62/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.846 total time=   6.1s\n",
      "[CV 3/5; 63/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 63/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.875 total time=   5.9s\n",
      "[CV 2/5; 64/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 64/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.907 total time=  16.2s\n",
      "[CV 1/5; 65/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 65/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.868 total time=  17.4s\n",
      "[CV 5/5; 65/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 65/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.907 total time=  17.1s\n",
      "[CV 4/5; 66/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 66/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.846 total time=  18.1s\n",
      "[CV 3/5; 67/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 67/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.886 total time=  17.9s\n",
      "[CV 2/5; 68/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 68/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.921 total time=  18.4s\n",
      "[CV 1/5; 69/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 69/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.868 total time=  18.9s\n",
      "[CV 5/5; 69/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 69/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  19.1s\n",
      "[CV 4/5; 70/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 70/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.854 total time=  18.2s\n",
      "[CV 5/5; 71/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 71/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.900 total time=   5.3s\n",
      "[CV 4/5; 72/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 72/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.864 total time=   5.4s\n",
      "[CV 3/5; 73/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 73/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.886 total time=   5.9s\n",
      "[CV 2/5; 74/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 74/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.914 total time=   6.1s\n",
      "[CV 1/5; 75/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 75/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.868 total time=   6.5s\n",
      "[CV 5/5; 75/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 75/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   6.2s\n",
      "[CV 4/5; 76/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 76/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.857 total time=   6.1s\n",
      "[CV 3/5; 77/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 77/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.875 total time=   5.9s\n",
      "[CV 2/5; 78/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 78/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.911 total time=  15.5s\n",
      "[CV 1/5; 79/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 79/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.879 total time=  16.2s\n",
      "[CV 5/5; 79/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 79/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.904 total time=  15.1s\n",
      "[CV 4/5; 80/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 80/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.854 total time=  16.3s\n",
      "[CV 3/5; 81/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 81/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.882 total time=  16.8s\n",
      "[CV 2/5; 82/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 82/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.921 total time=  17.2s\n",
      "[CV 1/5; 83/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 83/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.846 total time=  16.6s\n",
      "[CV 5/5; 83/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 83/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.907 total time=  16.4s\n",
      "[CV 4/5; 84/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 84/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.846 total time=  16.2s\n",
      "[CV 1/5; 86/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 86/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.868 total time=   4.8s\n",
      "[CV 1/5; 46/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 46/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.875 total time=   5.4s\n",
      "[CV 5/5; 46/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 46/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.900 total time=   5.4s\n",
      "[CV 4/5; 47/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 47/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.854 total time=   5.7s\n",
      "[CV 3/5; 48/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 48/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.879 total time=   5.6s\n",
      "[CV 2/5; 49/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 49/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.925 total time=   5.6s\n",
      "[CV 1/5; 50/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 50/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.871 total time=  15.2s\n",
      "[CV 5/5; 50/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 50/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.914 total time=  14.5s\n",
      "[CV 4/5; 51/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 51/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.843 total time=  15.3s\n",
      "[CV 3/5; 52/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 52/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.886 total time=  16.8s\n",
      "[CV 2/5; 53/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 53/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.911 total time=  17.4s\n",
      "[CV 1/5; 54/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 54/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.875 total time=  18.0s\n",
      "[CV 5/5; 54/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 54/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.914 total time=  17.5s\n",
      "[CV 4/5; 55/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 55/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.846 total time=  17.9s\n",
      "[CV 3/5; 56/392] START criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 56/392] END criterion=friedman_mse, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.871 total time=  18.3s\n",
      "[CV 4/5; 57/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 57/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.854 total time=   5.3s\n",
      "[CV 3/5; 58/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 58/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.886 total time=   5.7s\n",
      "[CV 2/5; 59/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 59/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.918 total time=   6.2s\n",
      "[CV 1/5; 60/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 60/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.879 total time=   5.9s\n",
      "[CV 5/5; 60/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 60/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.918 total time=   5.8s\n",
      "[CV 4/5; 61/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 61/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.846 total time=   6.0s\n",
      "[CV 3/5; 62/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 62/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.889 total time=   6.1s\n",
      "[CV 2/5; 63/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 63/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.918 total time=   5.9s\n",
      "[CV 1/5; 64/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 64/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.868 total time=  16.2s\n",
      "[CV 5/5; 64/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 64/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.904 total time=  16.4s\n",
      "[CV 4/5; 65/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 65/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.857 total time=  17.3s\n",
      "[CV 3/5; 66/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 66/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.886 total time=  18.4s\n",
      "[CV 2/5; 67/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 67/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.911 total time=  18.0s\n",
      "[CV 1/5; 68/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 68/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.857 total time=  18.4s\n",
      "[CV 5/5; 68/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 68/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.911 total time=  18.6s\n",
      "[CV 4/5; 69/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 69/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.843 total time=  19.2s\n",
      "[CV 3/5; 70/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 70/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.864 total time=  18.2s\n",
      "[CV 4/5; 71/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 71/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.861 total time=   5.4s\n",
      "[CV 3/5; 72/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 72/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.886 total time=   5.5s\n",
      "[CV 2/5; 73/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 73/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.907 total time=   5.9s\n",
      "[CV 1/5; 74/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 74/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.875 total time=   6.1s\n",
      "[CV 5/5; 74/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 74/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.921 total time=   6.1s\n",
      "[CV 4/5; 75/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 75/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.850 total time=   6.5s\n",
      "[CV 3/5; 76/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 76/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.879 total time=   6.3s\n",
      "[CV 2/5; 77/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 77/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.918 total time=   6.0s\n",
      "[CV 1/5; 78/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 78/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.864 total time=  15.3s\n",
      "[CV 5/5; 78/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 78/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.907 total time=  15.6s\n",
      "[CV 4/5; 79/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 79/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.854 total time=  15.3s\n",
      "[CV 3/5; 80/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 80/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.882 total time=  16.3s\n",
      "[CV 2/5; 81/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 81/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.914 total time=  16.7s\n",
      "[CV 1/5; 82/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 82/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.871 total time=  17.5s\n",
      "[CV 5/5; 82/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 82/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.911 total time=  16.5s\n",
      "[CV 4/5; 83/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 83/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.843 total time=  16.4s\n",
      "[CV 3/5; 84/392] START criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 84/392] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.875 total time=  16.1s\n",
      "[CV 4/5; 85/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 85/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.861 total time=   4.8s\n",
      "[CV 2/5; 86/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 86/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.911 total time=   4.8s\n",
      "[CV 1/5; 87/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 87/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.861 total time=   5.2s\n",
      "[CV 5/5; 87/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 87/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.914 total time=   5.1s\n",
      "[CV 4/5; 88/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 88/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.850 total time=   5.1s\n",
      "[CV 3/5; 89/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 89/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.886 total time=   5.3s\n",
      "[CV 5/5; 86/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 86/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.893 total time=   4.8s\n",
      "[CV 4/5; 87/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 87/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.850 total time=   5.2s\n",
      "[CV 3/5; 88/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 88/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.886 total time=   5.1s\n",
      "[CV 2/5; 89/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 89/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.914 total time=   5.2s\n",
      "[CV 1/5; 90/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 90/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.857 total time=   5.4s\n",
      "[CV 5/5; 90/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 90/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.921 total time=   5.4s\n",
      "[CV 4/5; 91/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 91/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.850 total time=   5.3s\n",
      "[CV 3/5; 92/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 92/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.886 total time=  13.3s\n",
      "[CV 2/5; 93/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 93/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.904 total time=  13.5s\n",
      "[CV 1/5; 94/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 94/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.861 total time=  15.1s\n",
      "[CV 5/5; 94/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 94/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.911 total time=  13.6s\n",
      "[CV 4/5; 95/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 95/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.846 total time=  14.9s\n",
      "[CV 3/5; 96/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 96/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.886 total time=  15.9s\n",
      "[CV 2/5; 97/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 97/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.911 total time=  14.4s\n",
      "[CV 1/5; 98/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 98/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.854 total time=  13.9s\n",
      "[CV 5/5; 98/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 98/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.904 total time=  13.3s\n",
      "[CV 1/5; 100/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 100/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.871 total time=   5.1s\n",
      "[CV 5/5; 100/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 100/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.907 total time=   5.1s\n",
      "[CV 4/5; 101/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 101/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.857 total time=   5.4s\n",
      "[CV 3/5; 102/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 102/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.893 total time=   6.2s\n",
      "[CV 2/5; 103/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 103/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   5.9s\n",
      "[CV 1/5; 104/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 104/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.861 total time=   5.8s\n",
      "[CV 5/5; 104/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 104/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.914 total time=   5.8s\n",
      "[CV 4/5; 105/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 105/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.846 total time=   6.4s\n",
      "[CV 3/5; 106/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 106/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.889 total time=  13.1s\n",
      "[CV 3/5; 107/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 107/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.886 total time=  13.0s\n",
      "[CV 2/5; 108/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 108/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  13.5s\n",
      "[CV 1/5; 109/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 109/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.864 total time=  13.4s\n",
      "[CV 5/5; 109/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 109/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.907 total time=  14.4s\n",
      "[CV 4/5; 110/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 110/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.854 total time=  14.2s\n",
      "[CV 3/5; 111/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 111/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.879 total time=  13.4s\n",
      "[CV 2/5; 112/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 112/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.907 total time=  13.0s\n",
      "[CV 1/5; 113/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 113/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.871 total time=   4.5s\n",
      "[CV 2/5; 113/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 113/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.911 total time=   4.7s\n",
      "[CV 3/5; 113/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 113/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.886 total time=   4.7s\n",
      "[CV 2/5; 114/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 114/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.907 total time=   4.8s\n",
      "[CV 1/5; 115/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 115/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.875 total time=   5.5s\n",
      "[CV 5/5; 115/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 115/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.2s\n",
      "[CV 4/5; 116/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 116/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.854 total time=   5.4s\n",
      "[CV 3/5; 117/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 117/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.889 total time=   5.5s\n",
      "[CV 2/5; 118/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 118/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.907 total time=   5.7s\n",
      "[CV 1/5; 119/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 119/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.3s\n",
      "[CV 5/5; 119/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 119/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.904 total time=   5.1s\n",
      "[CV 4/5; 120/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 120/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.857 total time=  11.0s\n",
      "[CV 3/5; 121/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 121/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.886 total time=  11.9s\n",
      "[CV 2/5; 122/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 122/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.914 total time=  13.4s\n",
      "[CV 1/5; 123/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 123/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.864 total time=  13.8s\n",
      "[CV 5/5; 123/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 123/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.904 total time=  12.6s\n",
      "[CV 4/5; 124/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 124/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.854 total time=  12.4s\n",
      "[CV 3/5; 125/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 125/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.882 total time=  13.2s\n",
      "[CV 2/5; 126/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 126/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.907 total time=  11.9s\n",
      "[CV 1/5; 127/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 127/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.875 total time=   4.6s\n",
      "[CV 2/5; 127/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 127/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.914 total time=   5.3s\n",
      "[CV 5/5; 127/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 127/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.911 total time=   5.2s\n",
      "[CV 4/5; 86/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 86/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.854 total time=   4.7s\n",
      "[CV 3/5; 87/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 87/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.889 total time=   5.2s\n",
      "[CV 2/5; 88/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 88/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.921 total time=   5.1s\n",
      "[CV 1/5; 89/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 89/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.875 total time=   5.2s\n",
      "[CV 5/5; 89/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 89/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.911 total time=   5.3s\n",
      "[CV 4/5; 90/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 90/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.843 total time=   5.4s\n",
      "[CV 3/5; 91/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 91/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.875 total time=   5.3s\n",
      "[CV 2/5; 92/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 92/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.904 total time=  13.3s\n",
      "[CV 1/5; 93/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 93/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.868 total time=  13.2s\n",
      "[CV 5/5; 93/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 93/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.907 total time=  14.3s\n",
      "[CV 4/5; 94/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 94/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.854 total time=  13.8s\n",
      "[CV 3/5; 95/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 95/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.886 total time=  14.8s\n",
      "[CV 2/5; 96/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 96/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.911 total time=  16.1s\n",
      "[CV 1/5; 97/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 97/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.871 total time=  14.3s\n",
      "[CV 5/5; 97/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 97/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  14.6s\n",
      "[CV 4/5; 98/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 98/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.854 total time=  13.2s\n",
      "[CV 5/5; 99/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 99/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.914 total time=   4.9s\n",
      "[CV 4/5; 100/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 100/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.843 total time=   4.9s\n",
      "[CV 3/5; 101/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 101/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.886 total time=   5.5s\n",
      "[CV 2/5; 102/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 102/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.918 total time=   5.8s\n",
      "[CV 1/5; 103/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 103/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.875 total time=   6.2s\n",
      "[CV 5/5; 103/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 103/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   5.6s\n",
      "[CV 4/5; 104/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 104/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.846 total time=   5.9s\n",
      "[CV 3/5; 105/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 105/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.875 total time=   6.3s\n",
      "[CV 2/5; 106/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 106/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.914 total time=  13.3s\n",
      "[CV 1/5; 107/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 107/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.857 total time=  12.8s\n",
      "[CV 5/5; 107/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 107/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.896 total time=  12.7s\n",
      "[CV 4/5; 108/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 108/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.846 total time=  13.2s\n",
      "[CV 3/5; 109/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 109/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.893 total time=  13.9s\n",
      "[CV 2/5; 110/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 110/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.918 total time=  15.0s\n",
      "[CV 1/5; 111/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 111/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.864 total time=  13.1s\n",
      "[CV 5/5; 111/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 111/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.911 total time=  13.2s\n",
      "[CV 4/5; 112/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 112/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.843 total time=  13.0s\n",
      "[CV 5/5; 113/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 113/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.904 total time=   4.6s\n",
      "[CV 4/5; 114/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 114/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.864 total time=   4.8s\n",
      "[CV 3/5; 115/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 115/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.886 total time=   5.5s\n",
      "[CV 2/5; 116/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 116/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.921 total time=   5.2s\n",
      "[CV 1/5; 117/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 117/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.864 total time=   5.5s\n",
      "[CV 5/5; 117/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 117/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.911 total time=   5.6s\n",
      "[CV 4/5; 118/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 118/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.857 total time=   5.7s\n",
      "[CV 3/5; 119/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 119/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.875 total time=   5.2s\n",
      "[CV 2/5; 120/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 120/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.911 total time=  11.0s\n",
      "[CV 1/5; 121/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 121/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.868 total time=  11.8s\n",
      "[CV 5/5; 121/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 121/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.904 total time=  12.3s\n",
      "[CV 4/5; 122/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 122/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.854 total time=  13.4s\n",
      "[CV 2/5; 123/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 123/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.904 total time=  14.1s\n",
      "[CV 1/5; 124/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 124/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.857 total time=  11.6s\n",
      "[CV 5/5; 124/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 124/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.907 total time=  13.5s\n",
      "[CV 4/5; 125/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 125/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.850 total time=  12.1s\n",
      "[CV 3/5; 126/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 126/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.871 total time=  12.1s\n",
      "[CV 3/5; 127/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 127/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.882 total time=   5.3s\n",
      "[CV 2/5; 128/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 128/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.911 total time=   5.4s\n",
      "[CV 1/5; 129/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 129/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.864 total time=   5.5s\n",
      "[CV 5/5; 129/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 129/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.911 total time=   4.9s\n",
      "[CV 2/5; 87/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 87/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.2s\n",
      "[CV 1/5; 88/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 88/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.882 total time=   5.2s\n",
      "[CV 5/5; 88/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 88/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   5.1s\n",
      "[CV 4/5; 89/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 89/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.850 total time=   5.3s\n",
      "[CV 3/5; 90/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 90/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.882 total time=   5.3s\n",
      "[CV 2/5; 91/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 91/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.904 total time=   5.2s\n",
      "[CV 1/5; 92/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 92/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.871 total time=  12.8s\n",
      "[CV 5/5; 92/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 92/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.907 total time=  13.1s\n",
      "[CV 4/5; 93/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 93/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.846 total time=  13.7s\n",
      "[CV 3/5; 94/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 94/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.889 total time=  14.6s\n",
      "[CV 2/5; 95/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 95/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.907 total time=  13.9s\n",
      "[CV 1/5; 96/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 96/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.861 total time=  16.6s\n",
      "[CV 5/5; 96/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 96/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.904 total time=  14.5s\n",
      "[CV 4/5; 97/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 97/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.854 total time=  14.2s\n",
      "[CV 3/5; 98/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 98/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.871 total time=  13.8s\n",
      "[CV 3/5; 99/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 99/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.889 total time=   4.8s\n",
      "[CV 2/5; 100/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 100/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.911 total time=   5.0s\n",
      "[CV 1/5; 101/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 101/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.882 total time=   5.4s\n",
      "[CV 5/5; 101/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 101/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.4s\n",
      "[CV 4/5; 102/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 102/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.850 total time=   6.2s\n",
      "[CV 3/5; 103/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 103/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.879 total time=   5.9s\n",
      "[CV 2/5; 104/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 104/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.904 total time=   5.8s\n",
      "[CV 1/5; 105/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 105/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.864 total time=   5.7s\n",
      "[CV 5/5; 105/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 105/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.918 total time=   6.4s\n",
      "[CV 4/5; 106/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 106/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.854 total time=  12.8s\n",
      "[CV 2/5; 107/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 107/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.907 total time=  13.0s\n",
      "[CV 1/5; 108/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 108/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.871 total time=  13.1s\n",
      "[CV 5/5; 108/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 108/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.907 total time=  13.3s\n",
      "[CV 4/5; 109/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 109/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.850 total time=  14.2s\n",
      "[CV 3/5; 110/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 110/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.882 total time=  14.5s\n",
      "[CV 2/5; 111/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 111/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.907 total time=  13.4s\n",
      "[CV 1/5; 112/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 112/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.864 total time=  12.8s\n",
      "[CV 5/5; 112/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 112/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.904 total time=  13.0s\n",
      "[CV 1/5; 114/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 114/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.868 total time=   4.9s\n",
      "[CV 5/5; 114/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 114/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.914 total time=   4.9s\n",
      "[CV 4/5; 115/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 115/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.850 total time=   5.4s\n",
      "[CV 3/5; 116/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 116/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.886 total time=   5.2s\n",
      "[CV 2/5; 117/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 117/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.914 total time=   5.7s\n",
      "[CV 1/5; 118/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 118/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.861 total time=   5.7s\n",
      "[CV 5/5; 118/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 118/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.911 total time=   5.5s\n",
      "[CV 4/5; 119/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 119/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.850 total time=   5.2s\n",
      "[CV 3/5; 120/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 120/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.889 total time=  11.0s\n",
      "[CV 2/5; 121/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 121/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.911 total time=  11.9s\n",
      "[CV 1/5; 122/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 122/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.861 total time=  12.6s\n",
      "[CV 5/5; 122/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 122/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.904 total time=  13.9s\n",
      "[CV 4/5; 123/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 123/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.854 total time=  13.1s\n",
      "[CV 3/5; 124/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 124/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.893 total time=  12.2s\n",
      "[CV 2/5; 125/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 125/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.904 total time=  13.4s\n",
      "[CV 1/5; 126/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 126/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.857 total time=  11.7s\n",
      "[CV 5/5; 126/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 126/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.907 total time=  12.2s\n",
      "[CV 1/5; 128/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 128/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.875 total time=   5.5s\n",
      "[CV 5/5; 128/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 128/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.907 total time=   5.3s\n",
      "[CV 4/5; 129/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 129/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.846 total time=   5.1s\n",
      "[CV 3/5; 130/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 130/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.882 total time=   4.9s\n",
      "[CV 2/5; 90/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 90/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.918 total time=   5.4s\n",
      "[CV 1/5; 91/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 91/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.868 total time=   5.3s\n",
      "[CV 5/5; 91/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 91/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.907 total time=   5.4s\n",
      "[CV 4/5; 92/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 92/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.861 total time=  13.0s\n",
      "[CV 3/5; 93/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 93/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.889 total time=  13.5s\n",
      "[CV 2/5; 94/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 94/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.911 total time=  15.4s\n",
      "[CV 1/5; 95/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 95/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.857 total time=  13.6s\n",
      "[CV 5/5; 95/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 95/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.911 total time=  15.4s\n",
      "[CV 4/5; 96/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 96/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.854 total time=  15.8s\n",
      "[CV 3/5; 97/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 97/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.879 total time=  14.4s\n",
      "[CV 2/5; 98/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 98/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.914 total time=  14.3s\n",
      "[CV 1/5; 99/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 99/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.875 total time=   4.6s\n",
      "[CV 2/5; 99/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 99/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.914 total time=   4.7s\n",
      "[CV 4/5; 99/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 99/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.843 total time=   4.9s\n",
      "[CV 3/5; 100/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 100/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.879 total time=   4.9s\n",
      "[CV 2/5; 101/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 101/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.929 total time=   5.6s\n",
      "[CV 1/5; 102/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 102/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.868 total time=   5.6s\n",
      "[CV 5/5; 102/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 102/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.914 total time=   6.3s\n",
      "[CV 4/5; 103/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 103/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.854 total time=   5.6s\n",
      "[CV 3/5; 104/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 104/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.871 total time=   6.0s\n",
      "[CV 2/5; 105/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 105/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.918 total time=   6.3s\n",
      "[CV 1/5; 106/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 106/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.864 total time=  13.3s\n",
      "[CV 5/5; 106/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 106/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.900 total time=  12.8s\n",
      "[CV 4/5; 107/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 107/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.854 total time=  12.8s\n",
      "[CV 3/5; 108/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 108/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.889 total time=  13.3s\n",
      "[CV 2/5; 109/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 109/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.904 total time=  14.2s\n",
      "[CV 1/5; 110/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 110/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.854 total time=  14.8s\n",
      "[CV 5/5; 110/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 110/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.911 total time=  13.2s\n",
      "[CV 4/5; 111/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 111/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.850 total time=  13.3s\n",
      "[CV 3/5; 112/392] START criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 112/392] END criterion=friedman_mse, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.871 total time=  13.4s\n",
      "[CV 4/5; 113/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 113/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.868 total time=   4.7s\n",
      "[CV 3/5; 114/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 114/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.879 total time=   4.8s\n",
      "[CV 2/5; 115/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 115/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.918 total time=   5.6s\n",
      "[CV 1/5; 116/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 116/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.864 total time=   5.2s\n",
      "[CV 5/5; 116/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 116/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.907 total time=   5.4s\n",
      "[CV 4/5; 117/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 117/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.843 total time=   5.6s\n",
      "[CV 3/5; 118/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 118/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.886 total time=   5.7s\n",
      "[CV 2/5; 119/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 119/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.914 total time=   5.3s\n",
      "[CV 1/5; 120/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 120/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.861 total time=  10.9s\n",
      "[CV 5/5; 120/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 120/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.893 total time=  11.8s\n",
      "[CV 4/5; 121/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 121/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.854 total time=  12.3s\n",
      "[CV 3/5; 122/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 122/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.882 total time=  13.6s\n",
      "[CV 3/5; 123/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 123/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.882 total time=  14.1s\n",
      "[CV 2/5; 124/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 124/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.911 total time=  12.1s\n",
      "[CV 1/5; 125/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 125/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.850 total time=  13.6s\n",
      "[CV 5/5; 125/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 125/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  12.3s\n",
      "[CV 4/5; 126/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 126/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.839 total time=  12.2s\n",
      "[CV 4/5; 127/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 127/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.864 total time=   5.3s\n",
      "[CV 3/5; 128/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 128/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.889 total time=   5.4s\n",
      "[CV 2/5; 129/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 129/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.914 total time=   5.3s\n",
      "[CV 1/5; 130/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 130/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.864 total time=   4.9s\n",
      "[CV 5/5; 130/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 130/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.918 total time=   4.9s\n",
      "[CV 4/5; 131/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 131/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.861 total time=   5.4s\n",
      "[CV 3/5; 132/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 132/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.879 total time=   5.6s\n",
      "[CV 4/5; 128/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 128/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.850 total time=   5.4s\n",
      "[CV 3/5; 129/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 129/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.893 total time=   5.2s\n",
      "[CV 2/5; 130/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 130/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.911 total time=   4.9s\n",
      "[CV 1/5; 131/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 131/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.864 total time=   5.0s\n",
      "[CV 5/5; 131/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 131/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   5.4s\n",
      "[CV 4/5; 132/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 132/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.846 total time=   5.6s\n",
      "[CV 3/5; 133/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 133/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.871 total time=   5.2s\n",
      "[CV 2/5; 134/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 134/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.904 total time=  10.6s\n",
      "[CV 1/5; 135/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 135/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.864 total time=  10.5s\n",
      "[CV 5/5; 135/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 135/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.914 total time=  10.6s\n",
      "[CV 4/5; 136/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 136/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.854 total time=  10.6s\n",
      "[CV 3/5; 137/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 137/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.882 total time=  11.9s\n",
      "[CV 2/5; 138/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 138/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.911 total time=  11.0s\n",
      "[CV 1/5; 139/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 139/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.857 total time=  11.1s\n",
      "[CV 5/5; 139/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 139/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.914 total time=  11.3s\n",
      "[CV 4/5; 140/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 140/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.857 total time=  10.0s\n",
      "[CV 4/5; 141/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 141/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.857 total time=   4.1s\n",
      "[CV 3/5; 142/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 142/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.882 total time=   4.3s\n",
      "[CV 2/5; 143/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 143/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.918 total time=   5.1s\n",
      "[CV 1/5; 144/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 144/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.861 total time=   5.1s\n",
      "[CV 5/5; 144/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 144/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   5.0s\n",
      "[CV 4/5; 145/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 145/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.857 total time=   5.0s\n",
      "[CV 3/5; 146/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 146/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.886 total time=   5.0s\n",
      "[CV 2/5; 147/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 147/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.914 total time=   4.7s\n",
      "[CV 1/5; 148/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 148/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.864 total time=   9.7s\n",
      "[CV 5/5; 148/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 148/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.896 total time=   9.3s\n",
      "[CV 4/5; 149/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 149/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.854 total time=   9.3s\n",
      "[CV 3/5; 150/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 150/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.886 total time=  10.0s\n",
      "[CV 2/5; 151/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 151/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.914 total time=  10.3s\n",
      "[CV 1/5; 152/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 152/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.857 total time=   9.8s\n",
      "[CV 5/5; 152/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 152/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.914 total time=  10.0s\n",
      "[CV 4/5; 153/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 153/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.850 total time=  11.3s\n",
      "[CV 3/5; 154/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 154/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.879 total time=   9.1s\n",
      "[CV 4/5; 155/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 155/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.864 total time=   4.1s\n",
      "[CV 3/5; 156/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 156/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.889 total time=   4.3s\n",
      "[CV 1/5; 157/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 157/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.861 total time=   4.7s\n",
      "[CV 5/5; 157/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 157/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.911 total time=   4.7s\n",
      "[CV 4/5; 158/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 158/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.857 total time=   4.9s\n",
      "[CV 3/5; 159/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 159/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.871 total time=   5.3s\n",
      "[CV 2/5; 160/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 160/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.914 total time=   5.1s\n",
      "[CV 1/5; 161/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 161/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.854 total time=   4.9s\n",
      "[CV 5/5; 161/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 161/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.904 total time=   5.2s\n",
      "[CV 4/5; 162/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 162/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.868 total time=   9.7s\n",
      "[CV 3/5; 163/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 163/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.882 total time=   9.2s\n",
      "[CV 2/5; 164/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 164/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  10.2s\n",
      "[CV 1/5; 165/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 165/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.857 total time=  10.1s\n",
      "[CV 5/5; 165/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 165/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.896 total time=   9.7s\n",
      "[CV 4/5; 166/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 166/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.854 total time=   9.7s\n",
      "[CV 3/5; 167/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 167/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.886 total time=   9.8s\n",
      "[CV 2/5; 168/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 168/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.907 total time=   9.1s\n",
      "[CV 1/5; 169/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 169/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.879 total time=   4.3s\n",
      "[CV 2/5; 169/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 169/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.914 total time=   4.7s\n",
      "[CV 1/5; 170/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 170/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.864 total time=   4.5s\n",
      "[CV 5/5; 170/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 170/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.896 total time=   4.4s\n",
      "[CV 4/5; 130/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 130/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.843 total time=   4.9s\n",
      "[CV 3/5; 131/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 131/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.882 total time=   5.3s\n",
      "[CV 2/5; 132/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 132/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.904 total time=   5.5s\n",
      "[CV 1/5; 133/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 133/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.868 total time=   5.4s\n",
      "[CV 5/5; 133/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 133/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.911 total time=   4.9s\n",
      "[CV 4/5; 134/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 134/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.861 total time=  10.3s\n",
      "[CV 3/5; 135/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 135/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.886 total time=  10.5s\n",
      "[CV 2/5; 136/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 136/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  10.9s\n",
      "[CV 1/5; 137/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 137/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.864 total time=  10.9s\n",
      "[CV 5/5; 137/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 137/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.904 total time=  11.6s\n",
      "[CV 4/5; 138/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 138/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.857 total time=  10.8s\n",
      "[CV 3/5; 139/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 139/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.886 total time=  11.2s\n",
      "[CV 2/5; 140/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 140/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.904 total time=  10.7s\n",
      "[CV 1/5; 141/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 141/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.864 total time=   4.1s\n",
      "[CV 2/5; 141/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 141/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.911 total time=   4.1s\n",
      "[CV 1/5; 142/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 142/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.868 total time=   4.4s\n",
      "[CV 5/5; 142/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 142/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.900 total time=   4.3s\n",
      "[CV 4/5; 143/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 143/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.846 total time=   5.2s\n",
      "[CV 3/5; 144/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 144/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.886 total time=   5.1s\n",
      "[CV 2/5; 145/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 145/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.911 total time=   5.1s\n",
      "[CV 1/5; 146/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 146/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.857 total time=   5.1s\n",
      "[CV 5/5; 146/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 146/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.907 total time=   5.0s\n",
      "[CV 4/5; 147/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 147/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.850 total time=   4.7s\n",
      "[CV 3/5; 148/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 148/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.886 total time=   9.8s\n",
      "[CV 2/5; 149/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 149/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.907 total time=   9.4s\n",
      "[CV 1/5; 150/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 150/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.864 total time=   9.5s\n",
      "[CV 5/5; 150/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 150/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.900 total time=   9.9s\n",
      "[CV 4/5; 151/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 151/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.846 total time=  10.1s\n",
      "[CV 3/5; 152/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 152/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.875 total time=   9.8s\n",
      "[CV 2/5; 153/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 153/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.921 total time=  10.3s\n",
      "[CV 1/5; 154/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 154/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.861 total time=  10.2s\n",
      "[CV 4/5; 154/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 154/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.850 total time=   9.1s\n",
      "[CV 2/5; 155/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 155/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.911 total time=   4.1s\n",
      "[CV 1/5; 156/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 156/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.868 total time=   4.3s\n",
      "[CV 5/5; 156/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 156/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.907 total time=   4.3s\n",
      "[CV 4/5; 157/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 157/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.850 total time=   4.7s\n",
      "[CV 3/5; 158/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 158/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.886 total time=   4.8s\n",
      "[CV 2/5; 159/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 159/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   5.2s\n",
      "[CV 1/5; 160/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 160/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.861 total time=   5.2s\n",
      "[CV 5/5; 160/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 160/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.907 total time=   5.0s\n",
      "[CV 4/5; 161/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 161/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.850 total time=   5.1s\n",
      "[CV 3/5; 162/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 162/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.886 total time=   9.8s\n",
      "[CV 2/5; 163/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 163/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.925 total time=   9.1s\n",
      "[CV 1/5; 164/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 164/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.857 total time=   9.9s\n",
      "[CV 5/5; 164/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 164/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.904 total time=  10.2s\n",
      "[CV 4/5; 165/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 165/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.846 total time=   9.7s\n",
      "[CV 3/5; 166/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 166/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.882 total time=   9.8s\n",
      "[CV 2/5; 167/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 167/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.914 total time=   9.8s\n",
      "[CV 1/5; 168/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 168/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.857 total time=   8.9s\n",
      "[CV 5/5; 168/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 168/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.896 total time=   9.6s\n",
      "[CV 5/5; 169/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 169/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.904 total time=   4.4s\n",
      "[CV 4/5; 170/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 170/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.857 total time=   4.3s\n",
      "[CV 3/5; 171/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 171/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.882 total time=   4.6s\n",
      "[CV 2/5; 172/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 172/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.914 total time=   4.5s\n",
      "[CV 2/5; 131/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 131/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.918 total time=   5.1s\n",
      "[CV 1/5; 132/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 132/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.861 total time=   5.5s\n",
      "[CV 5/5; 132/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 132/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.914 total time=   5.6s\n",
      "[CV 4/5; 133/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 133/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.857 total time=   5.2s\n",
      "[CV 3/5; 134/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 134/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.886 total time=  10.5s\n",
      "[CV 2/5; 135/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 135/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.918 total time=  10.6s\n",
      "[CV 1/5; 136/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 136/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.861 total time=  10.7s\n",
      "[CV 5/5; 136/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 136/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.911 total time=  10.7s\n",
      "[CV 4/5; 137/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 137/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.846 total time=  11.8s\n",
      "[CV 3/5; 138/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 138/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.882 total time=  10.9s\n",
      "[CV 2/5; 139/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 139/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.900 total time=  11.3s\n",
      "[CV 1/5; 140/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 140/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.846 total time=  10.6s\n",
      "[CV 5/5; 140/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 140/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.911 total time=  10.1s\n",
      "[CV 5/5; 141/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 141/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.896 total time=   4.1s\n",
      "[CV 4/5; 142/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 142/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.864 total time=   4.3s\n",
      "[CV 3/5; 143/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 143/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.889 total time=   5.1s\n",
      "[CV 2/5; 144/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 144/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.907 total time=   5.1s\n",
      "[CV 1/5; 145/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 145/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.854 total time=   5.1s\n",
      "[CV 5/5; 145/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 145/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.907 total time=   5.0s\n",
      "[CV 4/5; 146/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 146/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.850 total time=   5.0s\n",
      "[CV 3/5; 147/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 147/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.889 total time=   4.7s\n",
      "[CV 2/5; 148/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 148/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.907 total time=   9.8s\n",
      "[CV 1/5; 149/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 149/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.864 total time=   9.3s\n",
      "[CV 5/5; 149/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 149/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.900 total time=   9.3s\n",
      "[CV 4/5; 150/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 150/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.850 total time=   9.9s\n",
      "[CV 3/5; 151/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 151/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.882 total time=  10.3s\n",
      "[CV 2/5; 152/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 152/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.911 total time=   9.9s\n",
      "[CV 1/5; 153/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 153/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.864 total time=  10.0s\n",
      "[CV 5/5; 153/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 153/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.904 total time=  11.3s\n",
      "[CV 5/5; 154/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 154/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.896 total time=   9.1s\n",
      "[CV 5/5; 155/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 155/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.896 total time=   4.1s\n",
      "[CV 4/5; 156/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 156/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.854 total time=   4.4s\n",
      "[CV 3/5; 157/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 157/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.893 total time=   4.6s\n",
      "[CV 2/5; 158/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 158/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.911 total time=   4.8s\n",
      "[CV 1/5; 159/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 159/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.857 total time=   5.0s\n",
      "[CV 5/5; 159/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 159/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.907 total time=   5.3s\n",
      "[CV 4/5; 160/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 160/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.857 total time=   5.0s\n",
      "[CV 3/5; 161/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 161/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.886 total time=   4.9s\n",
      "[CV 2/5; 162/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 162/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.907 total time=  10.2s\n",
      "[CV 1/5; 163/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 163/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.875 total time=   9.2s\n",
      "[CV 4/5; 163/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 163/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.854 total time=   9.3s\n",
      "[CV 3/5; 164/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 164/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.879 total time=  10.3s\n",
      "[CV 2/5; 165/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 165/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.904 total time=  10.0s\n",
      "[CV 1/5; 166/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 166/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.868 total time=   9.6s\n",
      "[CV 5/5; 166/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 166/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.911 total time=   9.8s\n",
      "[CV 4/5; 167/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 167/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.861 total time=   9.8s\n",
      "[CV 3/5; 168/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 168/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.879 total time=   9.3s\n",
      "[CV 3/5; 169/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 169/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.875 total time=   4.6s\n",
      "[CV 2/5; 170/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 170/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.911 total time=   4.5s\n",
      "[CV 1/5; 171/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 171/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.875 total time=   4.5s\n",
      "[CV 5/5; 171/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 171/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.914 total time=   4.6s\n",
      "[CV 4/5; 172/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 172/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.850 total time=   4.5s\n",
      "[CV 3/5; 173/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 173/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.889 total time=   4.6s\n",
      "[CV 2/5; 174/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 174/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.904 total time=   4.8s\n",
      "[CV 2/5; 133/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 133/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.907 total time=   5.3s\n",
      "[CV 1/5; 134/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 134/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.871 total time=  10.4s\n",
      "[CV 5/5; 134/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 134/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.914 total time=  10.4s\n",
      "[CV 4/5; 135/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 135/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.861 total time=  10.6s\n",
      "[CV 3/5; 136/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 136/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.879 total time=  10.8s\n",
      "[CV 2/5; 137/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 137/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.907 total time=  12.0s\n",
      "[CV 1/5; 138/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 138/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.864 total time=  10.9s\n",
      "[CV 5/5; 138/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 138/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.907 total time=  11.2s\n",
      "[CV 4/5; 139/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 139/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.843 total time=  11.3s\n",
      "[CV 3/5; 140/392] START criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 140/392] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.882 total time=  10.2s\n",
      "[CV 3/5; 141/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 141/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.875 total time=   4.1s\n",
      "[CV 2/5; 142/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 142/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.911 total time=   4.4s\n",
      "[CV 1/5; 143/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 143/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.868 total time=   5.1s\n",
      "[CV 5/5; 143/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 143/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.1s\n",
      "[CV 4/5; 144/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 144/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.861 total time=   5.0s\n",
      "[CV 3/5; 145/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 145/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.882 total time=   5.1s\n",
      "[CV 2/5; 146/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 146/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.918 total time=   5.1s\n",
      "[CV 1/5; 147/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 147/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.857 total time=   4.7s\n",
      "[CV 5/5; 147/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 147/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.911 total time=   4.9s\n",
      "[CV 4/5; 148/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 148/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.846 total time=   9.9s\n",
      "[CV 3/5; 149/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 149/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.889 total time=   9.3s\n",
      "[CV 2/5; 150/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 150/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.904 total time=  10.0s\n",
      "[CV 1/5; 151/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 151/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.864 total time=  10.0s\n",
      "[CV 5/5; 151/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 151/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.904 total time=  10.1s\n",
      "[CV 4/5; 152/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 152/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.843 total time=   9.5s\n",
      "[CV 3/5; 153/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 153/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.893 total time=  11.6s\n",
      "[CV 2/5; 154/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 154/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.907 total time=   9.8s\n",
      "[CV 1/5; 155/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 155/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.879 total time=   4.2s\n",
      "[CV 3/5; 155/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 155/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.886 total time=   4.1s\n",
      "[CV 2/5; 156/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 156/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.911 total time=   4.4s\n",
      "[CV 2/5; 157/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 157/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.914 total time=   4.7s\n",
      "[CV 1/5; 158/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 158/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.864 total time=   4.8s\n",
      "[CV 5/5; 158/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 158/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.911 total time=   4.9s\n",
      "[CV 4/5; 159/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 159/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.846 total time=   5.3s\n",
      "[CV 3/5; 160/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 160/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.882 total time=   5.1s\n",
      "[CV 2/5; 161/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 161/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.914 total time=   4.9s\n",
      "[CV 1/5; 162/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 162/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.875 total time=  10.2s\n",
      "[CV 5/5; 162/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 162/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.904 total time=   9.2s\n",
      "[CV 5/5; 163/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 163/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.904 total time=   9.5s\n",
      "[CV 4/5; 164/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 164/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.861 total time=  10.4s\n",
      "[CV 3/5; 165/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 165/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.889 total time=  10.1s\n",
      "[CV 2/5; 166/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 166/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.907 total time=   9.8s\n",
      "[CV 1/5; 167/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 167/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.864 total time=   9.8s\n",
      "[CV 5/5; 167/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 167/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.904 total time=   9.9s\n",
      "[CV 4/5; 168/392] START criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 168/392] END criterion=friedman_mse, learning_rate=0.15, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.832 total time=   9.2s\n",
      "[CV 4/5; 169/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 169/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.861 total time=   4.7s\n",
      "[CV 3/5; 170/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 170/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.879 total time=   4.5s\n",
      "[CV 2/5; 171/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 171/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.904 total time=   4.6s\n",
      "[CV 1/5; 172/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 172/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.868 total time=   4.6s\n",
      "[CV 5/5; 172/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 172/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   4.6s\n",
      "[CV 4/5; 173/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 173/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.857 total time=   4.6s\n",
      "[CV 3/5; 174/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 174/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.893 total time=   4.9s\n",
      "[CV 2/5; 175/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 175/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.900 total time=   4.7s\n",
      "[CV 1/5; 176/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 176/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.875 total time=   9.0s\n",
      "[CV 4/5; 171/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 171/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.854 total time=   4.6s\n",
      "[CV 3/5; 172/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 172/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.893 total time=   4.6s\n",
      "[CV 2/5; 173/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 173/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.907 total time=   4.6s\n",
      "[CV 1/5; 174/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 174/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.857 total time=   4.7s\n",
      "[CV 5/5; 174/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 174/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.907 total time=   4.8s\n",
      "[CV 4/5; 175/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 175/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.857 total time=   4.4s\n",
      "[CV 3/5; 176/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 176/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.904 total time=   9.1s\n",
      "[CV 2/5; 177/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 177/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.918 total time=   9.1s\n",
      "[CV 1/5; 178/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 178/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.875 total time=   9.2s\n",
      "[CV 5/5; 178/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 178/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.900 total time=   9.1s\n",
      "[CV 4/5; 179/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 179/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.843 total time=   9.1s\n",
      "[CV 3/5; 180/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 180/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.886 total time=   9.1s\n",
      "[CV 2/5; 181/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 181/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.921 total time=   9.0s\n",
      "[CV 1/5; 182/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 182/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.864 total time=   8.1s\n",
      "[CV 5/5; 182/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 182/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.900 total time=   8.2s\n",
      "[CV 5/5; 183/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 183/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.907 total time=   4.2s\n",
      "[CV 4/5; 184/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 184/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.854 total time=   4.2s\n",
      "[CV 3/5; 185/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 185/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.886 total time=   4.4s\n",
      "[CV 2/5; 186/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 186/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.904 total time=   4.5s\n",
      "[CV 1/5; 187/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 187/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.850 total time=   4.4s\n",
      "[CV 5/5; 187/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 187/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.896 total time=   4.7s\n",
      "[CV 4/5; 188/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 188/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.850 total time=   4.9s\n",
      "[CV 3/5; 189/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 189/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.879 total time=   4.6s\n",
      "[CV 2/5; 190/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 190/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.900 total time=   9.8s\n",
      "[CV 1/5; 191/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 191/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.871 total time=   9.3s\n",
      "[CV 5/5; 191/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 191/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.893 total time=  10.1s\n",
      "[CV 4/5; 192/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 192/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.864 total time=   9.5s\n",
      "[CV 3/5; 193/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 193/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.886 total time=   9.3s\n",
      "[CV 2/5; 194/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 194/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.918 total time=   9.8s\n",
      "[CV 1/5; 195/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 195/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.875 total time=   9.3s\n",
      "[CV 5/5; 195/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 195/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.904 total time=   9.1s\n",
      "[CV 5/5; 196/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 196/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.911 total time=   8.3s\n",
      "[CV 5/5; 197/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 197/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.868 total time=   4.3s\n",
      "[CV 4/5; 198/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 198/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.846 total time=   4.6s\n",
      "[CV 3/5; 199/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 199/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.861 total time=   4.9s\n",
      "[CV 2/5; 200/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 200/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.893 total time=   5.2s\n",
      "[CV 1/5; 201/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 201/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.861 total time=   5.5s\n",
      "[CV 5/5; 201/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 201/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.868 total time=   5.7s\n",
      "[CV 4/5; 202/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 202/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.861 total time=   5.5s\n",
      "[CV 3/5; 203/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 203/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.2s\n",
      "[CV 2/5; 204/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 204/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.918 total time=  14.2s\n",
      "[CV 1/5; 205/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 205/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.882 total time=  14.7s\n",
      "[CV 5/5; 205/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 205/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.907 total time=  14.9s\n",
      "[CV 4/5; 206/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 206/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.850 total time=  16.1s\n",
      "[CV 3/5; 207/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 207/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.882 total time=  16.2s\n",
      "[CV 2/5; 208/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 208/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.914 total time=  16.6s\n",
      "[CV 1/5; 209/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 209/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.875 total time=  17.0s\n",
      "[CV 5/5; 209/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 209/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  18.0s\n",
      "[CV 4/5; 210/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 210/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.857 total time=  17.1s\n",
      "[CV 5/5; 211/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 211/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.864 total time=   4.8s\n",
      "[CV 3/5; 212/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 212/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.854 total time=   5.1s\n",
      "[CV 2/5; 213/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 213/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.900 total time=   6.2s\n",
      "[CV 1/5; 214/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 214/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.864 total time=   5.9s\n",
      "[CV 5/5; 214/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 176/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 176/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.889 total time=   9.1s\n",
      "[CV 4/5; 177/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 177/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.857 total time=   9.4s\n",
      "[CV 3/5; 178/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 178/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.882 total time=   9.1s\n",
      "[CV 2/5; 179/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 179/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.904 total time=   9.2s\n",
      "[CV 1/5; 180/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 180/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.871 total time=   9.2s\n",
      "[CV 5/5; 180/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 180/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.914 total time=   9.0s\n",
      "[CV 4/5; 181/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 181/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.854 total time=   9.0s\n",
      "[CV 3/5; 182/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 182/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.879 total time=   8.3s\n",
      "[CV 3/5; 183/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 183/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.879 total time=   4.3s\n",
      "[CV 2/5; 184/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 184/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.904 total time=   4.3s\n",
      "[CV 1/5; 185/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 185/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.864 total time=   4.3s\n",
      "[CV 5/5; 185/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 185/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.914 total time=   4.4s\n",
      "[CV 4/5; 186/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 186/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.850 total time=   4.4s\n",
      "[CV 3/5; 187/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 187/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.882 total time=   4.7s\n",
      "[CV 2/5; 188/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 188/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.904 total time=   4.9s\n",
      "[CV 1/5; 189/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 189/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.846 total time=   4.6s\n",
      "[CV 5/5; 189/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 189/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.896 total time=   4.7s\n",
      "[CV 4/5; 190/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 190/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.868 total time=   9.6s\n",
      "[CV 3/5; 191/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 191/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.889 total time=   9.5s\n",
      "[CV 2/5; 192/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 192/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.896 total time=  10.3s\n",
      "[CV 1/5; 193/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 193/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.868 total time=   9.3s\n",
      "[CV 5/5; 193/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 193/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.911 total time=   9.6s\n",
      "[CV 4/5; 194/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 194/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.864 total time=   9.6s\n",
      "[CV 3/5; 195/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 195/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.886 total time=   9.4s\n",
      "[CV 2/5; 196/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 196/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.914 total time=   8.5s\n",
      "[CV 1/5; 197/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 197/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.850 total time=   4.4s\n",
      "[CV 2/5; 197/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 197/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.882 total time=   4.3s\n",
      "[CV 1/5; 198/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 198/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.839 total time=   4.6s\n",
      "[CV 5/5; 198/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 198/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.868 total time=   4.5s\n",
      "[CV 4/5; 199/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 199/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.857 total time=   5.0s\n",
      "[CV 3/5; 200/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 200/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.850 total time=   5.3s\n",
      "[CV 2/5; 201/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 201/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.893 total time=   5.6s\n",
      "[CV 1/5; 202/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 202/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.857 total time=   5.8s\n",
      "[CV 5/5; 202/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 202/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.886 total time=   5.5s\n",
      "[CV 4/5; 203/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 203/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.850 total time=   5.7s\n",
      "[CV 3/5; 204/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 204/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.882 total time=  14.0s\n",
      "[CV 2/5; 205/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 205/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.918 total time=  14.8s\n",
      "[CV 1/5; 206/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 206/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.868 total time=  16.0s\n",
      "[CV 5/5; 206/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 206/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.914 total time=  16.2s\n",
      "[CV 4/5; 207/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 207/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.846 total time=  16.4s\n",
      "[CV 3/5; 208/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 208/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.875 total time=  16.5s\n",
      "[CV 2/5; 209/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 209/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.921 total time=  17.4s\n",
      "[CV 2/5; 210/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 210/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.918 total time=  17.9s\n",
      "[CV 1/5; 211/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 211/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.843 total time=   4.9s\n",
      "[CV 2/5; 211/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 211/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.886 total time=   4.8s\n",
      "[CV 4/5; 211/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 211/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.850 total time=   4.8s\n",
      "[CV 2/5; 212/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 212/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.893 total time=   5.1s\n",
      "[CV 1/5; 213/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 213/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.868 total time=   6.1s\n",
      "[CV 5/5; 213/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 213/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.875 total time=   5.9s\n",
      "[CV 4/5; 214/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 214/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.850 total time=   6.0s\n",
      "[CV 3/5; 215/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 215/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.854 total time=   6.9s\n",
      "[CV 2/5; 216/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 216/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.893 total time=   6.1s\n",
      "[CV 1/5; 217/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 217/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.7s\n",
      "[CV 5/5; 217/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 173/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 173/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.861 total time=   4.6s\n",
      "[CV 5/5; 173/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 173/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.904 total time=   4.6s\n",
      "[CV 4/5; 174/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 174/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.846 total time=   4.8s\n",
      "[CV 3/5; 175/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 175/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.875 total time=   4.6s\n",
      "[CV 2/5; 176/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 176/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.914 total time=   9.1s\n",
      "[CV 1/5; 177/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 177/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.868 total time=   9.0s\n",
      "[CV 5/5; 177/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 177/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.904 total time=   9.2s\n",
      "[CV 4/5; 178/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 178/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.857 total time=   9.0s\n",
      "[CV 3/5; 179/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 179/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.896 total time=   9.1s\n",
      "[CV 2/5; 180/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 180/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.911 total time=   9.1s\n",
      "[CV 1/5; 181/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 181/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.857 total time=   8.8s\n",
      "[CV 5/5; 181/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 181/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=   8.9s\n",
      "[CV 4/5; 182/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 182/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.850 total time=   8.1s\n",
      "[CV 4/5; 183/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 183/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.850 total time=   4.1s\n",
      "[CV 3/5; 184/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 184/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.882 total time=   4.3s\n",
      "[CV 2/5; 185/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 185/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.911 total time=   4.4s\n",
      "[CV 1/5; 186/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 186/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.864 total time=   4.4s\n",
      "[CV 5/5; 186/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 186/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.907 total time=   4.4s\n",
      "[CV 4/5; 187/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 187/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.850 total time=   4.6s\n",
      "[CV 3/5; 188/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 188/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.868 total time=   4.9s\n",
      "[CV 2/5; 189/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 189/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.925 total time=   4.6s\n",
      "[CV 1/5; 190/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 190/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.868 total time=   9.7s\n",
      "[CV 5/5; 190/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 190/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.889 total time=   9.2s\n",
      "[CV 4/5; 191/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 191/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.864 total time=  10.1s\n",
      "[CV 3/5; 192/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 192/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.882 total time=   9.5s\n",
      "[CV 2/5; 193/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 193/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.911 total time=   9.4s\n",
      "[CV 1/5; 194/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 194/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.864 total time=   9.6s\n",
      "[CV 5/5; 194/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 194/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.904 total time=   9.3s\n",
      "[CV 4/5; 195/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 195/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.843 total time=   9.1s\n",
      "[CV 3/5; 196/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 196/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.882 total time=   8.3s\n",
      "[CV 3/5; 197/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 197/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.864 total time=   4.2s\n",
      "[CV 2/5; 198/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 198/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.886 total time=   4.6s\n",
      "[CV 1/5; 199/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 199/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.854 total time=   4.9s\n",
      "[CV 5/5; 199/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 199/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.871 total time=   5.0s\n",
      "[CV 4/5; 200/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 200/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.850 total time=   5.4s\n",
      "[CV 3/5; 201/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 201/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.868 total time=   5.6s\n",
      "[CV 2/5; 202/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 202/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.886 total time=   5.6s\n",
      "[CV 1/5; 203/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 203/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.2s\n",
      "[CV 5/5; 203/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 203/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.875 total time=   5.8s\n",
      "[CV 4/5; 204/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 204/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.861 total time=  13.8s\n",
      "[CV 3/5; 205/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 205/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.889 total time=  14.7s\n",
      "[CV 2/5; 206/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 206/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.921 total time=  15.9s\n",
      "[CV 1/5; 207/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 207/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.875 total time=  16.4s\n",
      "[CV 5/5; 207/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 207/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.914 total time=  16.3s\n",
      "[CV 4/5; 208/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 208/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.861 total time=  16.4s\n",
      "[CV 3/5; 209/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 209/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.886 total time=  17.3s\n",
      "[CV 1/5; 210/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 210/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.871 total time=  17.8s\n",
      "[CV 5/5; 210/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 210/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.907 total time=  16.6s\n",
      "[CV 4/5; 212/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 212/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.850 total time=   5.1s\n",
      "[CV 3/5; 213/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 213/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.868 total time=   6.2s\n",
      "[CV 2/5; 214/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 214/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.889 total time=   5.9s\n",
      "[CV 1/5; 215/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 215/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.850 total time=   6.8s\n",
      "[CV 5/5; 215/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 215/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.879 total time=   6.3s\n",
      "[CV 4/5; 216/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 175/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 175/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.861 total time=   4.6s\n",
      "[CV 5/5; 175/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 175/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.904 total time=   4.4s\n",
      "[CV 4/5; 176/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 176/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.864 total time=   9.0s\n",
      "[CV 3/5; 177/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 177/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.879 total time=   9.1s\n",
      "[CV 2/5; 178/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 178/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.911 total time=   9.2s\n",
      "[CV 1/5; 179/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 179/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.861 total time=   9.0s\n",
      "[CV 5/5; 179/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 179/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.900 total time=   9.1s\n",
      "[CV 4/5; 180/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 180/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.857 total time=   9.0s\n",
      "[CV 3/5; 181/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 181/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.882 total time=   8.9s\n",
      "[CV 2/5; 182/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 182/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.907 total time=   8.2s\n",
      "[CV 1/5; 183/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 183/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.871 total time=   4.2s\n",
      "[CV 2/5; 183/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 183/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.921 total time=   4.3s\n",
      "[CV 1/5; 184/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 184/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.871 total time=   4.3s\n",
      "[CV 5/5; 184/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 184/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.896 total time=   4.2s\n",
      "[CV 4/5; 185/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 185/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.861 total time=   4.3s\n",
      "[CV 3/5; 186/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 186/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.879 total time=   4.5s\n",
      "[CV 2/5; 187/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 187/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   4.5s\n",
      "[CV 1/5; 188/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 188/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.868 total time=   4.7s\n",
      "[CV 5/5; 188/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 188/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.911 total time=   4.9s\n",
      "[CV 4/5; 189/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 189/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.843 total time=   4.5s\n",
      "[CV 3/5; 190/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 190/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.889 total time=   9.8s\n",
      "[CV 2/5; 191/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 191/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.907 total time=   9.3s\n",
      "[CV 1/5; 192/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 192/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.868 total time=  10.1s\n",
      "[CV 5/5; 192/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 192/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.907 total time=   9.5s\n",
      "[CV 4/5; 193/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 193/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.843 total time=   9.2s\n",
      "[CV 3/5; 194/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 194/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.893 total time=   9.8s\n",
      "[CV 2/5; 195/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 195/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.896 total time=   9.4s\n",
      "[CV 1/5; 196/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 196/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.854 total time=   8.3s\n",
      "[CV 4/5; 196/392] START criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 196/392] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.850 total time=   8.2s\n",
      "[CV 4/5; 197/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 197/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.836 total time=   4.3s\n",
      "[CV 3/5; 198/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 198/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.850 total time=   4.6s\n",
      "[CV 2/5; 199/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 199/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.886 total time=   4.9s\n",
      "[CV 1/5; 200/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 200/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.846 total time=   5.2s\n",
      "[CV 5/5; 200/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 200/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.879 total time=   5.3s\n",
      "[CV 4/5; 201/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 201/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.854 total time=   5.7s\n",
      "[CV 3/5; 202/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 202/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.846 total time=   5.6s\n",
      "[CV 2/5; 203/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 203/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.889 total time=   5.3s\n",
      "[CV 1/5; 204/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 204/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.875 total time=  14.2s\n",
      "[CV 5/5; 204/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 204/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.911 total time=  14.0s\n",
      "[CV 4/5; 205/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 205/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.857 total time=  14.8s\n",
      "[CV 3/5; 206/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 206/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.886 total time=  16.0s\n",
      "[CV 2/5; 207/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 207/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.907 total time=  16.3s\n",
      "[CV 1/5; 208/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 208/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.879 total time=  16.6s\n",
      "[CV 5/5; 208/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 208/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.907 total time=  16.7s\n",
      "[CV 4/5; 209/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 209/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.846 total time=  18.0s\n",
      "[CV 3/5; 210/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 210/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.871 total time=  17.2s\n",
      "[CV 3/5; 211/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 211/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.861 total time=   4.7s\n",
      "[CV 1/5; 212/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 212/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.854 total time=   5.0s\n",
      "[CV 5/5; 212/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 212/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.875 total time=   5.6s\n",
      "[CV 4/5; 213/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 213/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.850 total time=   6.0s\n",
      "[CV 3/5; 214/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 214/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.864 total time=   5.9s\n",
      "[CV 2/5; 215/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 215/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.889 total time=   6.8s\n",
      "[CV 1/5; 216/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 216/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.861 total time=   6.2s\n",
      "[CV 5/5; 216/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 214/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.875 total time=   6.5s\n",
      "[CV 4/5; 215/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 215/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.854 total time=   6.4s\n",
      "[CV 3/5; 216/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 216/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.864 total time=   5.9s\n",
      "[CV 2/5; 217/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 217/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.882 total time=   5.7s\n",
      "[CV 1/5; 218/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 218/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.871 total time=  14.4s\n",
      "[CV 5/5; 218/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 218/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.907 total time=  15.0s\n",
      "[CV 4/5; 219/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 219/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.861 total time=  16.4s\n",
      "[CV 3/5; 220/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 220/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.889 total time=  17.9s\n",
      "[CV 2/5; 221/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 221/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.918 total time=  18.4s\n",
      "[CV 1/5; 222/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 222/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.871 total time=  18.4s\n",
      "[CV 5/5; 222/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 222/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.918 total time=  18.3s\n",
      "[CV 4/5; 223/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 223/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.843 total time=  19.6s\n",
      "[CV 3/5; 224/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 224/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.875 total time=  18.7s\n",
      "[CV 4/5; 225/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 225/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.861 total time=   5.3s\n",
      "[CV 2/5; 226/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 226/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.918 total time=   5.5s\n",
      "[CV 1/5; 227/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 227/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.868 total time=   6.0s\n",
      "[CV 5/5; 227/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 227/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.9s\n",
      "[CV 4/5; 228/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 228/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.854 total time=   6.0s\n",
      "[CV 3/5; 229/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 229/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.879 total time=   6.6s\n",
      "[CV 2/5; 230/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 230/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.911 total time=   6.6s\n",
      "[CV 1/5; 231/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 231/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.864 total time=   6.2s\n",
      "[CV 5/5; 231/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 231/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.911 total time=   6.4s\n",
      "[CV 4/5; 232/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 232/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.854 total time=  16.5s\n",
      "[CV 3/5; 233/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 233/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.889 total time=  16.6s\n",
      "[CV 2/5; 234/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 234/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.911 total time=  18.0s\n",
      "[CV 1/5; 235/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 235/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.871 total time=  18.6s\n",
      "[CV 5/5; 235/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 235/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.914 total time=  18.4s\n",
      "[CV 4/5; 236/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 236/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.843 total time=  18.5s\n",
      "[CV 3/5; 237/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 237/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.868 total time=  18.7s\n",
      "[CV 2/5; 238/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 238/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.911 total time=  17.9s\n",
      "[CV 1/5; 239/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 239/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.879 total time=   5.1s\n",
      "[CV 2/5; 239/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 239/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.907 total time=   5.1s\n",
      "[CV 4/5; 239/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 239/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.857 total time=   5.1s\n",
      "[CV 2/5; 240/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 240/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.900 total time=   5.5s\n",
      "[CV 1/5; 241/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 241/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.875 total time=   5.8s\n",
      "[CV 5/5; 241/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 241/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.907 total time=   5.7s\n",
      "[CV 4/5; 242/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 242/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.850 total time=   5.8s\n",
      "[CV 3/5; 243/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 243/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.879 total time=   5.9s\n",
      "[CV 2/5; 244/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 244/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.914 total time=   6.0s\n",
      "[CV 1/5; 245/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 245/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.868 total time=   5.9s\n",
      "[CV 5/5; 245/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 245/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.904 total time=   5.9s\n",
      "[CV 4/5; 246/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 246/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.861 total time=  17.0s\n",
      "[CV 3/5; 247/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 247/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.889 total time=  17.2s\n",
      "[CV 2/5; 248/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 248/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  19.1s\n",
      "[CV 1/5; 249/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 249/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.875 total time=  19.1s\n",
      "[CV 5/5; 249/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 249/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.918 total time=  18.9s\n",
      "[CV 4/5; 250/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 250/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.850 total time=  18.5s\n",
      "[CV 3/5; 251/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 251/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.882 total time=  23.5s\n",
      "[CV 2/5; 252/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 252/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.911 total time=  25.0s\n",
      "[CV 1/5; 253/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 253/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.879 total time=   7.1s\n",
      "[CV 2/5; 253/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 253/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.904 total time=   6.3s\n",
      "[CV 3/5; 253/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 253/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.886 total time=   6.7s\n",
      "[CV 1/5; 254/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 254/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.882 total time=   6.9s\n",
      "[CV 5/5; 254/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 216/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.871 total time=   5.8s\n",
      "[CV 4/5; 217/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 217/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.6s\n",
      "[CV 3/5; 218/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 218/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.889 total time=  14.4s\n",
      "[CV 2/5; 219/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 219/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.918 total time=  16.0s\n",
      "[CV 1/5; 220/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 220/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.875 total time=  17.7s\n",
      "[CV 5/5; 220/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 220/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  17.9s\n",
      "[CV 4/5; 221/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 221/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.846 total time=  18.4s\n",
      "[CV 3/5; 222/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 222/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.886 total time=  18.5s\n",
      "[CV 2/5; 223/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 223/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.918 total time=  18.7s\n",
      "[CV 1/5; 224/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 224/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.868 total time=  19.2s\n",
      "[CV 5/5; 224/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 224/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.907 total time=  18.4s\n",
      "[CV 3/5; 226/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 226/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.879 total time=   5.5s\n",
      "[CV 2/5; 227/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 227/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.918 total time=   6.0s\n",
      "[CV 1/5; 228/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 228/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.871 total time=   6.0s\n",
      "[CV 5/5; 228/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 228/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   6.0s\n",
      "[CV 4/5; 229/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 229/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.864 total time=   6.6s\n",
      "[CV 3/5; 230/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 230/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.879 total time=   6.6s\n",
      "[CV 2/5; 231/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 231/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.911 total time=   6.2s\n",
      "[CV 1/5; 232/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 232/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.875 total time=  16.4s\n",
      "[CV 5/5; 232/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 232/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.907 total time=  16.0s\n",
      "[CV 4/5; 233/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 233/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.850 total time=  16.7s\n",
      "[CV 3/5; 234/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 234/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.889 total time=  17.7s\n",
      "[CV 2/5; 235/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 235/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.914 total time=  19.1s\n",
      "[CV 1/5; 236/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 236/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.875 total time=  18.3s\n",
      "[CV 5/5; 236/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 236/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.918 total time=  18.7s\n",
      "[CV 4/5; 237/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 237/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.850 total time=  18.5s\n",
      "[CV 3/5; 238/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 238/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.871 total time=  17.6s\n",
      "[CV 3/5; 239/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 239/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.882 total time=   5.1s\n",
      "[CV 1/5; 240/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 240/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.864 total time=   5.5s\n",
      "[CV 5/5; 240/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 240/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.904 total time=   5.3s\n",
      "[CV 4/5; 241/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 241/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.864 total time=   5.7s\n",
      "[CV 3/5; 242/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 242/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.882 total time=   5.8s\n",
      "[CV 2/5; 243/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 243/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.904 total time=   5.9s\n",
      "[CV 1/5; 244/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 244/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.871 total time=   6.0s\n",
      "[CV 5/5; 244/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 244/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.911 total time=   6.1s\n",
      "[CV 4/5; 245/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 245/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.9s\n",
      "[CV 3/5; 246/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 246/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.882 total time=  16.8s\n",
      "[CV 2/5; 247/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 247/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.914 total time=  17.1s\n",
      "[CV 1/5; 248/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 248/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.871 total time=  19.1s\n",
      "[CV 5/5; 248/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 248/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  18.8s\n",
      "[CV 4/5; 249/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 249/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.850 total time=  19.0s\n",
      "[CV 3/5; 250/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 250/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.886 total time=  18.5s\n",
      "[CV 2/5; 251/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 251/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.911 total time=  23.3s\n",
      "[CV 1/5; 252/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 252/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.861 total time=  24.7s\n",
      "[CV 5/5; 252/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 252/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.911 total time=  22.9s\n",
      "[CV 2/5; 254/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 254/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.914 total time=   6.9s\n",
      "[CV 1/5; 255/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 255/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.875 total time=   7.9s\n",
      "[CV 5/5; 255/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 255/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.918 total time=   8.2s\n",
      "[CV 4/5; 256/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 256/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.854 total time=   8.4s\n",
      "[CV 3/5; 257/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 257/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.871 total time=   7.5s\n",
      "[CV 2/5; 258/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 258/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.911 total time=   7.7s\n",
      "[CV 1/5; 259/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 259/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.861 total time=   7.7s\n",
      "[CV 5/5; 259/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 259/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.907 total time=   7.9s\n",
      "[CV 4/5; 260/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 216/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.857 total time=   5.9s\n",
      "[CV 3/5; 217/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 217/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.868 total time=   5.7s\n",
      "[CV 2/5; 218/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 218/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.918 total time=  14.3s\n",
      "[CV 1/5; 219/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 219/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.875 total time=  16.0s\n",
      "[CV 5/5; 219/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 219/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.904 total time=  16.3s\n",
      "[CV 4/5; 220/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 220/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.857 total time=  18.0s\n",
      "[CV 3/5; 221/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 221/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.889 total time=  18.3s\n",
      "[CV 2/5; 222/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 222/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.911 total time=  18.5s\n",
      "[CV 1/5; 223/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 223/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.871 total time=  18.7s\n",
      "[CV 5/5; 223/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 223/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.911 total time=  19.7s\n",
      "[CV 4/5; 224/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 224/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.861 total time=  18.6s\n",
      "[CV 5/5; 225/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 225/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.904 total time=   5.2s\n",
      "[CV 4/5; 226/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 226/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.879 total time=   5.5s\n",
      "[CV 3/5; 227/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 227/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.882 total time=   5.9s\n",
      "[CV 2/5; 228/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 228/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   6.0s\n",
      "[CV 1/5; 229/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 229/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.875 total time=   6.2s\n",
      "[CV 5/5; 229/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 229/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.914 total time=   6.5s\n",
      "[CV 4/5; 230/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 230/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.864 total time=   6.6s\n",
      "[CV 3/5; 231/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 231/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.871 total time=   6.4s\n",
      "[CV 2/5; 232/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 232/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.914 total time=  16.5s\n",
      "[CV 1/5; 233/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 233/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.879 total time=  16.8s\n",
      "[CV 5/5; 233/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 233/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.911 total time=  16.7s\n",
      "[CV 4/5; 234/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 234/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.843 total time=  17.7s\n",
      "[CV 3/5; 235/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 235/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.889 total time=  19.1s\n",
      "[CV 2/5; 236/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 236/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.918 total time=  18.3s\n",
      "[CV 1/5; 237/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 237/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.864 total time=  18.9s\n",
      "[CV 5/5; 237/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 237/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.911 total time=  18.5s\n",
      "[CV 4/5; 238/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 238/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.854 total time=  17.6s\n",
      "[CV 5/5; 239/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 239/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.896 total time=   5.1s\n",
      "[CV 4/5; 240/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 240/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.857 total time=   5.4s\n",
      "[CV 3/5; 241/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 241/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.893 total time=   5.7s\n",
      "[CV 2/5; 242/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 242/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.911 total time=   5.8s\n",
      "[CV 1/5; 243/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 243/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.871 total time=   5.9s\n",
      "[CV 5/5; 243/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 243/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.907 total time=   5.9s\n",
      "[CV 4/5; 244/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 244/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.857 total time=   6.0s\n",
      "[CV 3/5; 245/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 245/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.875 total time=   5.9s\n",
      "[CV 2/5; 246/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 246/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.918 total time=  16.3s\n",
      "[CV 5/5; 246/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 246/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.911 total time=  16.3s\n",
      "[CV 4/5; 247/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 247/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.846 total time=  17.6s\n",
      "[CV 3/5; 248/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 248/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.889 total time=  19.0s\n",
      "[CV 2/5; 249/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 249/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.925 total time=  18.9s\n",
      "[CV 1/5; 250/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 250/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.861 total time=  18.8s\n",
      "[CV 5/5; 250/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 250/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.914 total time=  20.1s\n",
      "[CV 4/5; 251/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 251/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.850 total time=  25.6s\n",
      "[CV 3/5; 252/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 252/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.868 total time=  24.3s\n",
      "[CV 4/5; 253/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 253/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.879 total time=   6.5s\n",
      "[CV 3/5; 254/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 254/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.889 total time=   7.0s\n",
      "[CV 2/5; 255/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 255/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.911 total time=   8.0s\n",
      "[CV 1/5; 256/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 256/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.868 total time=   8.3s\n",
      "[CV 5/5; 256/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 256/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   8.5s\n",
      "[CV 4/5; 257/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 257/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.850 total time=   7.2s\n",
      "[CV 3/5; 258/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 258/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.882 total time=   7.7s\n",
      "[CV 2/5; 259/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 259/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.914 total time=   7.6s\n",
      "[CV 1/5; 260/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 217/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.882 total time=   5.6s\n",
      "[CV 4/5; 218/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 218/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.868 total time=  14.5s\n",
      "[CV 3/5; 219/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 219/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.889 total time=  16.2s\n",
      "[CV 2/5; 220/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 220/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.918 total time=  17.7s\n",
      "[CV 1/5; 221/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 221/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.879 total time=  18.3s\n",
      "[CV 5/5; 221/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 221/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.911 total time=  18.4s\n",
      "[CV 4/5; 222/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 222/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.854 total time=  18.6s\n",
      "[CV 3/5; 223/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 223/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.886 total time=  19.1s\n",
      "[CV 2/5; 224/392] START criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 224/392] END criterion=squared_error, learning_rate=0.01, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.921 total time=  19.1s\n",
      "[CV 1/5; 225/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 225/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.879 total time=   5.2s\n",
      "[CV 2/5; 225/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 225/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.918 total time=   5.4s\n",
      "[CV 3/5; 225/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 225/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.882 total time=   5.3s\n",
      "[CV 1/5; 226/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 226/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.879 total time=   5.5s\n",
      "[CV 5/5; 226/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 226/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.907 total time=   5.6s\n",
      "[CV 4/5; 227/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 227/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.864 total time=   5.9s\n",
      "[CV 3/5; 228/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 228/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.882 total time=   6.0s\n",
      "[CV 2/5; 229/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 229/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.914 total time=   6.6s\n",
      "[CV 1/5; 230/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 230/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.868 total time=   6.7s\n",
      "[CV 5/5; 230/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 230/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.904 total time=   6.4s\n",
      "[CV 4/5; 231/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 231/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.857 total time=   6.4s\n",
      "[CV 3/5; 232/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 232/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.882 total time=  16.5s\n",
      "[CV 2/5; 233/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 233/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.918 total time=  16.7s\n",
      "[CV 1/5; 234/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 234/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.868 total time=  18.1s\n",
      "[CV 5/5; 234/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 234/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.914 total time=  18.3s\n",
      "[CV 4/5; 235/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 235/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.846 total time=  18.6s\n",
      "[CV 3/5; 236/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 236/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.886 total time=  18.5s\n",
      "[CV 2/5; 237/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 237/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  18.8s\n",
      "[CV 1/5; 238/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 238/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.861 total time=  18.0s\n",
      "[CV 5/5; 238/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 238/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.907 total time=  17.6s\n",
      "[CV 3/5; 240/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 240/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.875 total time=   5.5s\n",
      "[CV 2/5; 241/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 241/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.904 total time=   5.8s\n",
      "[CV 1/5; 242/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 242/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.871 total time=   5.9s\n",
      "[CV 5/5; 242/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 242/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.907 total time=   5.9s\n",
      "[CV 4/5; 243/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 243/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.864 total time=   6.0s\n",
      "[CV 3/5; 244/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 244/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.882 total time=   6.0s\n",
      "[CV 2/5; 245/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 245/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.896 total time=   6.0s\n",
      "[CV 1/5; 246/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 246/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.868 total time=  16.4s\n",
      "[CV 1/5; 247/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 247/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.879 total time=  17.4s\n",
      "[CV 5/5; 247/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 247/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.911 total time=  17.7s\n",
      "[CV 4/5; 248/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 248/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.846 total time=  19.1s\n",
      "[CV 3/5; 249/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 249/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.886 total time=  19.1s\n",
      "[CV 2/5; 250/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 250/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.911 total time=  18.8s\n",
      "[CV 1/5; 251/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 251/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.868 total time=  21.8s\n",
      "[CV 5/5; 251/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 251/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.914 total time=  25.8s\n",
      "[CV 4/5; 252/392] START criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 252/392] END criterion=squared_error, learning_rate=0.025, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.846 total time=  23.8s\n",
      "[CV 5/5; 253/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 253/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.925 total time=   6.3s\n",
      "[CV 4/5; 254/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 254/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.861 total time=   7.2s\n",
      "[CV 3/5; 255/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 255/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.875 total time=   8.3s\n",
      "[CV 2/5; 256/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 256/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.900 total time=   8.2s\n",
      "[CV 1/5; 257/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 257/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.861 total time=   8.2s\n",
      "[CV 5/5; 257/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 257/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.914 total time=   7.4s\n",
      "[CV 4/5; 258/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 258/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.857 total time=   7.9s\n",
      "[CV 3/5; 259/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 259/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.875 total time=   7.8s\n",
      "[CV 2/5; 260/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 254/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.900 total time=   7.3s\n",
      "[CV 4/5; 255/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 255/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.846 total time=   8.2s\n",
      "[CV 3/5; 256/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 256/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.886 total time=   8.3s\n",
      "[CV 2/5; 257/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 257/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.907 total time=   7.9s\n",
      "[CV 1/5; 258/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 258/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.864 total time=   7.6s\n",
      "[CV 5/5; 258/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 258/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.914 total time=   8.0s\n",
      "[CV 4/5; 259/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 259/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.861 total time=   7.9s\n",
      "[CV 3/5; 260/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 260/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.889 total time=  17.4s\n",
      "[CV 2/5; 261/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 261/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.907 total time=  17.7s\n",
      "[CV 1/5; 262/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 262/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.871 total time=  18.6s\n",
      "[CV 5/5; 262/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 262/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.914 total time=  18.2s\n",
      "[CV 4/5; 263/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 263/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.839 total time=  18.1s\n",
      "[CV 3/5; 264/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 264/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.875 total time=  18.2s\n",
      "[CV 2/5; 265/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 265/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.918 total time=  18.4s\n",
      "[CV 1/5; 266/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 266/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.861 total time=  17.7s\n",
      "[CV 5/5; 266/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 266/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.896 total time=  17.5s\n",
      "[CV 3/5; 268/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 268/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.896 total time=   5.4s\n",
      "[CV 2/5; 269/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 269/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.914 total time=   5.8s\n",
      "[CV 1/5; 270/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 270/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.882 total time=   5.9s\n",
      "[CV 5/5; 270/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 270/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.914 total time=   5.9s\n",
      "[CV 4/5; 271/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 271/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.846 total time=   6.0s\n",
      "[CV 3/5; 272/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 272/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.879 total time=   6.1s\n",
      "[CV 2/5; 273/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 273/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.918 total time=   5.9s\n",
      "[CV 1/5; 274/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 274/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.864 total time=  15.3s\n",
      "[CV 5/5; 274/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 274/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.911 total time=  15.3s\n",
      "[CV 4/5; 275/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 275/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.864 total time=  16.2s\n",
      "[CV 3/5; 276/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 276/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.889 total time=  17.4s\n",
      "[CV 2/5; 277/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 277/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.914 total time=  17.8s\n",
      "[CV 1/5; 278/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 278/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.861 total time=  18.0s\n",
      "[CV 5/5; 278/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 278/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.914 total time=  18.0s\n",
      "[CV 4/5; 279/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 279/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.843 total time=  18.2s\n",
      "[CV 3/5; 280/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 280/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.871 total time=  17.8s\n",
      "[CV 4/5; 281/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 281/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.850 total time=   5.1s\n",
      "[CV 2/5; 282/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 282/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.904 total time=   5.4s\n",
      "[CV 1/5; 283/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 283/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.879 total time=   6.0s\n",
      "[CV 5/5; 283/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 283/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.914 total time=   5.8s\n",
      "[CV 4/5; 284/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 284/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.843 total time=   6.0s\n",
      "[CV 3/5; 285/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 285/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.882 total time=   6.0s\n",
      "[CV 2/5; 286/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 286/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.904 total time=   6.1s\n",
      "[CV 1/5; 287/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 287/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.854 total time=   5.9s\n",
      "[CV 5/5; 287/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 287/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.904 total time=   5.9s\n",
      "[CV 4/5; 288/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 288/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.864 total time=  14.0s\n",
      "[CV 2/5; 289/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 289/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.918 total time=  14.7s\n",
      "[CV 1/5; 290/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 290/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.857 total time=  14.8s\n",
      "[CV 5/5; 290/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 290/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.911 total time=  14.9s\n",
      "[CV 4/5; 291/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 291/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.854 total time=  15.0s\n",
      "[CV 3/5; 292/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 292/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.886 total time=  15.4s\n",
      "[CV 2/5; 293/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 293/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.921 total time=  15.5s\n",
      "[CV 1/5; 294/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 294/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.854 total time=  14.4s\n",
      "[CV 5/5; 294/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 294/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.904 total time=  14.5s\n",
      "[CV 2/5; 296/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 296/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.904 total time=   5.4s\n",
      "[CV 1/5; 297/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 297/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.879 total time=   5.8s\n",
      "[CV 5/5; 297/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 297/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.907 total time=   5.8s\n",
      "[CV 4/5; 298/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 260/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.857 total time=  18.7s\n",
      "[CV 5/5; 260/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 260/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.907 total time=  16.7s\n",
      "[CV 4/5; 261/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 261/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.857 total time=  17.4s\n",
      "[CV 3/5; 262/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 262/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.886 total time=  18.3s\n",
      "[CV 2/5; 263/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 263/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.911 total time=  18.3s\n",
      "[CV 1/5; 264/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 264/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.864 total time=  18.3s\n",
      "[CV 5/5; 264/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 264/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.904 total time=  18.1s\n",
      "[CV 4/5; 265/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 265/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.843 total time=  18.4s\n",
      "[CV 3/5; 266/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 266/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.864 total time=  17.6s\n",
      "[CV 4/5; 267/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 267/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.850 total time=   5.1s\n",
      "[CV 2/5; 268/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 268/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.918 total time=   5.4s\n",
      "[CV 1/5; 269/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 269/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.882 total time=   5.8s\n",
      "[CV 5/5; 269/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 269/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.8s\n",
      "[CV 4/5; 270/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 270/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.857 total time=   5.9s\n",
      "[CV 3/5; 271/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 271/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.879 total time=   6.0s\n",
      "[CV 2/5; 272/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 272/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.907 total time=   6.1s\n",
      "[CV 1/5; 273/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 273/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.871 total time=   5.9s\n",
      "[CV 5/5; 273/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 273/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.911 total time=   5.9s\n",
      "[CV 4/5; 274/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 274/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.857 total time=  15.2s\n",
      "[CV 3/5; 275/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 275/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.886 total time=  16.1s\n",
      "[CV 2/5; 276/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 276/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.911 total time=  17.4s\n",
      "[CV 1/5; 277/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 277/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.868 total time=  17.8s\n",
      "[CV 5/5; 277/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 277/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.911 total time=  17.6s\n",
      "[CV 4/5; 278/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 278/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.850 total time=  18.0s\n",
      "[CV 3/5; 279/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 279/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.886 total time=  18.3s\n",
      "[CV 2/5; 280/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 280/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.904 total time=  17.8s\n",
      "[CV 1/5; 281/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 281/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.868 total time=   5.1s\n",
      "[CV 2/5; 281/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 281/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.914 total time=   5.2s\n",
      "[CV 3/5; 281/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 281/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.889 total time=   5.1s\n",
      "[CV 5/5; 281/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 281/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.907 total time=   5.1s\n",
      "[CV 4/5; 282/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 282/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.857 total time=   5.6s\n",
      "[CV 3/5; 283/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 283/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.886 total time=   5.8s\n",
      "[CV 2/5; 284/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 284/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   6.0s\n",
      "[CV 1/5; 285/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 285/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.868 total time=   6.0s\n",
      "[CV 5/5; 285/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 285/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.907 total time=   6.0s\n",
      "[CV 4/5; 286/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 286/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.850 total time=   6.1s\n",
      "[CV 3/5; 287/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 287/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.882 total time=   5.9s\n",
      "[CV 2/5; 288/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 288/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.907 total time=  14.3s\n",
      "[CV 1/5; 289/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 289/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.868 total time=  14.3s\n",
      "[CV 5/5; 289/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 289/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.904 total time=  14.4s\n",
      "[CV 4/5; 290/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 290/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.850 total time=  14.8s\n",
      "[CV 3/5; 291/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 291/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.882 total time=  15.1s\n",
      "[CV 2/5; 292/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 292/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.914 total time=  15.4s\n",
      "[CV 1/5; 293/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 293/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.861 total time=  15.2s\n",
      "[CV 5/5; 293/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 293/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  15.3s\n",
      "[CV 4/5; 294/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 294/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.857 total time=  14.5s\n",
      "[CV 5/5; 295/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 295/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.911 total time=   5.1s\n",
      "[CV 4/5; 296/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 296/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.854 total time=   5.4s\n",
      "[CV 3/5; 297/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 297/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.886 total time=   5.8s\n",
      "[CV 2/5; 298/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 298/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.918 total time=   5.9s\n",
      "[CV 1/5; 299/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 299/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.868 total time=   6.0s\n",
      "[CV 5/5; 299/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 299/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.911 total time=   6.0s\n",
      "[CV 4/5; 300/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 300/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.846 total time=   6.1s\n",
      "[CV 3/5; 301/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 260/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.850 total time=  17.2s\n",
      "[CV 3/5; 261/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 261/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.879 total time=  17.5s\n",
      "[CV 2/5; 262/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 262/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.914 total time=  18.6s\n",
      "[CV 1/5; 263/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 263/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.861 total time=  18.5s\n",
      "[CV 5/5; 263/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 263/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.911 total time=  18.1s\n",
      "[CV 4/5; 264/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 264/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.850 total time=  18.1s\n",
      "[CV 3/5; 265/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 265/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.889 total time=  18.4s\n",
      "[CV 2/5; 266/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 266/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.914 total time=  17.8s\n",
      "[CV 1/5; 267/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 267/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.875 total time=   5.1s\n",
      "[CV 2/5; 267/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 267/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.914 total time=   5.1s\n",
      "[CV 3/5; 267/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 267/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.893 total time=   5.1s\n",
      "[CV 1/5; 268/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 268/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.871 total time=   5.4s\n",
      "[CV 5/5; 268/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 268/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.907 total time=   5.4s\n",
      "[CV 4/5; 269/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 269/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.861 total time=   5.8s\n",
      "[CV 3/5; 270/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 270/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.882 total time=   5.9s\n",
      "[CV 2/5; 271/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 271/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.904 total time=   6.0s\n",
      "[CV 1/5; 272/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 272/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.875 total time=   6.1s\n",
      "[CV 5/5; 272/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 272/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.918 total time=   6.1s\n",
      "[CV 4/5; 273/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 273/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.9s\n",
      "[CV 3/5; 274/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 274/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.889 total time=  15.2s\n",
      "[CV 2/5; 275/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 275/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.918 total time=  16.1s\n",
      "[CV 1/5; 276/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 276/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.871 total time=  17.4s\n",
      "[CV 5/5; 276/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 276/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.911 total time=  17.5s\n",
      "[CV 4/5; 277/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 277/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.846 total time=  17.7s\n",
      "[CV 3/5; 278/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 278/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.886 total time=  18.0s\n",
      "[CV 2/5; 279/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 279/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.914 total time=  18.3s\n",
      "[CV 1/5; 280/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 280/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.861 total time=  17.6s\n",
      "[CV 5/5; 280/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 280/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.904 total time=  17.8s\n",
      "[CV 3/5; 282/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 282/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.882 total time=   5.5s\n",
      "[CV 2/5; 283/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 283/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.921 total time=   5.9s\n",
      "[CV 1/5; 284/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 284/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.871 total time=   5.9s\n",
      "[CV 5/5; 284/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 284/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.907 total time=   6.0s\n",
      "[CV 4/5; 285/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 285/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.857 total time=   6.0s\n",
      "[CV 3/5; 286/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 286/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.886 total time=   6.1s\n",
      "[CV 2/5; 287/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 287/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.918 total time=   5.9s\n",
      "[CV 1/5; 288/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 288/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.861 total time=  14.0s\n",
      "[CV 5/5; 288/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 288/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.893 total time=  14.0s\n",
      "[CV 4/5; 289/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 289/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.850 total time=  14.3s\n",
      "[CV 3/5; 290/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 290/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.882 total time=  15.0s\n",
      "[CV 2/5; 291/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 291/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.921 total time=  15.2s\n",
      "[CV 1/5; 292/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 292/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.857 total time=  15.1s\n",
      "[CV 5/5; 292/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 292/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.911 total time=  15.2s\n",
      "[CV 4/5; 293/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 293/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.854 total time=  15.2s\n",
      "[CV 3/5; 294/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 294/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.875 total time=  14.7s\n",
      "[CV 3/5; 295/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 295/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.875 total time=   5.1s\n",
      "[CV 1/5; 296/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 296/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.871 total time=   5.4s\n",
      "[CV 5/5; 296/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 296/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.911 total time=   5.4s\n",
      "[CV 4/5; 297/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 297/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.854 total time=   5.8s\n",
      "[CV 3/5; 298/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 298/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.889 total time=   5.9s\n",
      "[CV 2/5; 299/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 299/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   6.0s\n",
      "[CV 1/5; 300/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 300/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.861 total time=   6.1s\n",
      "[CV 5/5; 300/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 300/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.911 total time=   6.1s\n",
      "[CV 4/5; 301/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 301/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.850 total time=   5.9s\n",
      "[CV 3/5; 302/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 260/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.914 total time=  18.0s\n",
      "[CV 1/5; 261/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 261/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.875 total time=  17.7s\n",
      "[CV 5/5; 261/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 261/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.914 total time=  17.4s\n",
      "[CV 4/5; 262/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 262/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.861 total time=  18.3s\n",
      "[CV 3/5; 263/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 263/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.886 total time=  18.3s\n",
      "[CV 2/5; 264/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 264/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.911 total time=  18.3s\n",
      "[CV 1/5; 265/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 265/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.854 total time=  18.5s\n",
      "[CV 5/5; 265/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 265/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.907 total time=  18.4s\n",
      "[CV 4/5; 266/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 266/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.857 total time=  17.6s\n",
      "[CV 5/5; 267/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 267/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.918 total time=   5.1s\n",
      "[CV 4/5; 268/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 268/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.861 total time=   5.4s\n",
      "[CV 3/5; 269/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 269/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.882 total time=   5.8s\n",
      "[CV 2/5; 270/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 270/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.904 total time=   5.9s\n",
      "[CV 1/5; 271/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 271/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.868 total time=   6.0s\n",
      "[CV 5/5; 271/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 271/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.914 total time=   6.0s\n",
      "[CV 4/5; 272/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 272/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.864 total time=   6.1s\n",
      "[CV 3/5; 273/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 273/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.882 total time=   5.9s\n",
      "[CV 2/5; 274/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 274/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.911 total time=  15.3s\n",
      "[CV 1/5; 275/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 275/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.864 total time=  16.1s\n",
      "[CV 5/5; 275/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 275/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.900 total time=  16.2s\n",
      "[CV 4/5; 276/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 276/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.846 total time=  17.5s\n",
      "[CV 3/5; 277/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 277/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.879 total time=  17.8s\n",
      "[CV 2/5; 278/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 278/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.911 total time=  18.1s\n",
      "[CV 1/5; 279/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 279/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.861 total time=  18.3s\n",
      "[CV 5/5; 279/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 279/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.904 total time=  18.3s\n",
      "[CV 4/5; 280/392] START criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 280/392] END criterion=squared_error, learning_rate=0.05, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.846 total time=  17.8s\n",
      "[CV 1/5; 282/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 282/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.871 total time=   5.4s\n",
      "[CV 5/5; 282/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 282/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.911 total time=   5.6s\n",
      "[CV 4/5; 283/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 283/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.854 total time=   5.8s\n",
      "[CV 3/5; 284/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 284/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.889 total time=   6.0s\n",
      "[CV 2/5; 285/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 285/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.904 total time=   6.0s\n",
      "[CV 1/5; 286/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 286/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.871 total time=   6.1s\n",
      "[CV 5/5; 286/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 286/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.914 total time=   6.1s\n",
      "[CV 4/5; 287/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 287/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.857 total time=   5.9s\n",
      "[CV 3/5; 288/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 288/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.882 total time=  14.3s\n",
      "[CV 3/5; 289/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 289/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.889 total time=  14.7s\n",
      "[CV 2/5; 290/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 290/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.911 total time=  15.2s\n",
      "[CV 1/5; 291/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 291/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.868 total time=  15.0s\n",
      "[CV 5/5; 291/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 291/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.911 total time=  15.2s\n",
      "[CV 4/5; 292/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 292/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.843 total time=  15.3s\n",
      "[CV 3/5; 293/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 293/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.886 total time=  15.5s\n",
      "[CV 2/5; 294/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 294/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.907 total time=  14.8s\n",
      "[CV 1/5; 295/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 295/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.889 total time=   5.1s\n",
      "[CV 2/5; 295/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 295/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.918 total time=   5.1s\n",
      "[CV 4/5; 295/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 295/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.854 total time=   5.2s\n",
      "[CV 3/5; 296/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 296/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.879 total time=   5.4s\n",
      "[CV 2/5; 297/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 297/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.8s\n",
      "[CV 1/5; 298/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 298/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.868 total time=   5.9s\n",
      "[CV 5/5; 298/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 298/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.921 total time=   5.9s\n",
      "[CV 4/5; 299/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 299/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.850 total time=   6.0s\n",
      "[CV 3/5; 300/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 300/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.875 total time=   6.1s\n",
      "[CV 2/5; 301/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 301/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.907 total time=   5.9s\n",
      "[CV 1/5; 302/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 302/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.868 total time=  14.0s\n",
      "[CV 5/5; 302/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 298/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.843 total time=   5.9s\n",
      "[CV 3/5; 299/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 299/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.886 total time=   6.0s\n",
      "[CV 2/5; 300/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 300/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.914 total time=   6.1s\n",
      "[CV 1/5; 301/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 301/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.9s\n",
      "[CV 5/5; 301/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 301/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.911 total time=   5.9s\n",
      "[CV 4/5; 302/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 302/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.850 total time=  14.0s\n",
      "[CV 3/5; 303/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 303/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.879 total time=  14.5s\n",
      "[CV 2/5; 304/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 304/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  15.1s\n",
      "[CV 1/5; 305/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 305/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.857 total time=  14.9s\n",
      "[CV 5/5; 305/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 305/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.911 total time=  15.0s\n",
      "[CV 4/5; 306/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 306/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.843 total time=  15.0s\n",
      "[CV 3/5; 307/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 307/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.882 total time=  15.4s\n",
      "[CV 2/5; 308/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 308/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.914 total time=  14.8s\n",
      "[CV 1/5; 309/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 309/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.871 total time=   5.1s\n",
      "[CV 2/5; 309/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 309/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.914 total time=   5.1s\n",
      "[CV 3/5; 309/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 309/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.875 total time=   5.1s\n",
      "[CV 2/5; 310/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 310/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.925 total time=   5.4s\n",
      "[CV 1/5; 311/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 311/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.864 total time=   6.0s\n",
      "[CV 5/5; 311/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 311/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.907 total time=   5.9s\n",
      "[CV 4/5; 312/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 312/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.854 total time=   5.9s\n",
      "[CV 3/5; 313/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 313/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.882 total time=   6.0s\n",
      "[CV 2/5; 314/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 314/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.911 total time=   6.1s\n",
      "[CV 1/5; 315/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 315/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.864 total time=   5.9s\n",
      "[CV 5/5; 315/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 315/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.907 total time=   5.9s\n",
      "[CV 4/5; 316/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 316/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.854 total time=  12.7s\n",
      "[CV 3/5; 317/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 317/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.896 total time=  13.0s\n",
      "[CV 2/5; 318/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 318/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.904 total time=  13.4s\n",
      "[CV 1/5; 319/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 319/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.861 total time=  13.2s\n",
      "[CV 5/5; 319/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 319/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.900 total time=  13.2s\n",
      "[CV 4/5; 320/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 320/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.850 total time=  13.2s\n",
      "[CV 3/5; 321/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 321/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.882 total time=  13.8s\n",
      "[CV 2/5; 322/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 322/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.907 total time=  12.7s\n",
      "[CV 1/5; 323/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 323/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.875 total time=   5.1s\n",
      "[CV 2/5; 323/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 323/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.921 total time=   5.2s\n",
      "[CV 1/5; 324/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 324/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.864 total time=   5.4s\n",
      "[CV 5/5; 324/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 324/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.907 total time=   5.4s\n",
      "[CV 4/5; 325/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 325/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.850 total time=   5.9s\n",
      "[CV 3/5; 326/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 326/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.896 total time=   5.9s\n",
      "[CV 2/5; 327/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 327/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.911 total time=   6.0s\n",
      "[CV 1/5; 328/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 328/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.861 total time=   6.1s\n",
      "[CV 5/5; 328/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 328/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.914 total time=   6.1s\n",
      "[CV 4/5; 329/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 329/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.864 total time=   5.9s\n",
      "[CV 3/5; 330/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 330/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.900 total time=  12.8s\n",
      "[CV 2/5; 331/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 331/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.911 total time=  13.0s\n",
      "[CV 1/5; 332/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 332/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.868 total time=  13.1s\n",
      "[CV 5/5; 332/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 332/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.911 total time=  13.3s\n",
      "[CV 4/5; 333/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 333/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.854 total time=  14.0s\n",
      "[CV 3/5; 334/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 334/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.886 total time=  13.6s\n",
      "[CV 2/5; 335/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 335/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.914 total time=  13.7s\n",
      "[CV 1/5; 336/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 336/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.850 total time=  12.7s\n",
      "[CV 5/5; 336/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 336/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.907 total time=  12.8s\n",
      "[CV 5/5; 337/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 337/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.886 total time=   5.2s\n",
      "[CV 4/5; 338/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 338/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.864 total time=   5.5s\n",
      "[CV 3/5; 339/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 339/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.889 total time=   5.9s\n",
      "[CV 3/5; 301/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.886 total time=   5.9s\n",
      "[CV 2/5; 302/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 302/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.907 total time=  14.2s\n",
      "[CV 1/5; 303/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 303/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.868 total time=  14.4s\n",
      "[CV 5/5; 303/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 303/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.900 total time=  14.4s\n",
      "[CV 4/5; 304/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 304/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.850 total time=  14.8s\n",
      "[CV 3/5; 305/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 305/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.882 total time=  15.1s\n",
      "[CV 2/5; 306/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 306/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.907 total time=  15.4s\n",
      "[CV 1/5; 307/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 307/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.854 total time=  15.1s\n",
      "[CV 5/5; 307/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 307/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.911 total time=  15.3s\n",
      "[CV 4/5; 308/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 308/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.843 total time=  14.5s\n",
      "[CV 5/5; 309/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 309/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.914 total time=   5.1s\n",
      "[CV 4/5; 310/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 310/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.854 total time=   5.4s\n",
      "[CV 3/5; 311/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 311/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.886 total time=   6.0s\n",
      "[CV 2/5; 312/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 312/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.904 total time=   6.0s\n",
      "[CV 1/5; 313/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 313/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.864 total time=   6.0s\n",
      "[CV 5/5; 313/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 313/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.907 total time=   6.0s\n",
      "[CV 4/5; 314/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 314/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.843 total time=   6.1s\n",
      "[CV 3/5; 315/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 315/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.871 total time=   5.9s\n",
      "[CV 2/5; 316/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 316/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.907 total time=  12.8s\n",
      "[CV 1/5; 317/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 317/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.857 total time=  12.7s\n",
      "[CV 5/5; 317/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 317/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.907 total time=  12.9s\n",
      "[CV 4/5; 318/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 318/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.846 total time=  13.2s\n",
      "[CV 3/5; 319/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 319/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.882 total time=  13.3s\n",
      "[CV 2/5; 320/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 320/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.914 total time=  13.4s\n",
      "[CV 1/5; 321/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 321/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.861 total time=  13.3s\n",
      "[CV 5/5; 321/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 321/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.900 total time=  13.8s\n",
      "[CV 4/5; 322/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 322/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.850 total time=  12.5s\n",
      "[CV 4/5; 323/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 323/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.857 total time=   5.2s\n",
      "[CV 3/5; 324/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 324/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.886 total time=   5.5s\n",
      "[CV 2/5; 325/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 325/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.9s\n",
      "[CV 1/5; 326/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 326/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.861 total time=   5.9s\n",
      "[CV 5/5; 326/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 326/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.907 total time=   5.9s\n",
      "[CV 4/5; 327/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 327/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.854 total time=   6.0s\n",
      "[CV 3/5; 328/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 328/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.879 total time=   6.1s\n",
      "[CV 2/5; 329/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 329/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.907 total time=   5.9s\n",
      "[CV 1/5; 330/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 330/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.861 total time=  12.6s\n",
      "[CV 5/5; 330/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 330/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.904 total time=  12.6s\n",
      "[CV 4/5; 331/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 331/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.854 total time=  12.8s\n",
      "[CV 3/5; 332/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 332/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.882 total time=  13.3s\n",
      "[CV 2/5; 333/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 333/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.914 total time=  14.2s\n",
      "[CV 1/5; 334/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 334/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.857 total time=  13.5s\n",
      "[CV 5/5; 334/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 334/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.907 total time=  13.5s\n",
      "[CV 4/5; 335/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 335/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.846 total time=  13.6s\n",
      "[CV 3/5; 336/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 336/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.879 total time=  12.9s\n",
      "[CV 3/5; 337/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 337/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.889 total time=   5.2s\n",
      "[CV 2/5; 338/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 338/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.907 total time=   5.4s\n",
      "[CV 1/5; 339/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 339/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.871 total time=   5.9s\n",
      "[CV 5/5; 339/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 339/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.911 total time=   5.9s\n",
      "[CV 4/5; 340/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 340/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.857 total time=   6.0s\n",
      "[CV 3/5; 341/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 341/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.879 total time=   6.0s\n",
      "[CV 2/5; 342/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 342/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.907 total time=   6.2s\n",
      "[CV 1/5; 343/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 343/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.864 total time=   5.9s\n",
      "[CV 5/5; 343/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 343/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.904 total time=   5.9s\n",
      "[CV 4/5; 344/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 344/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.854 total time=  11.3s\n",
      "[CV 3/5; 302/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.886 total time=  14.2s\n",
      "[CV 2/5; 303/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 303/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.911 total time=  14.6s\n",
      "[CV 1/5; 304/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 304/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.864 total time=  14.8s\n",
      "[CV 5/5; 304/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 304/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.904 total time=  14.9s\n",
      "[CV 4/5; 305/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 305/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.846 total time=  14.9s\n",
      "[CV 3/5; 306/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 306/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.879 total time=  15.3s\n",
      "[CV 2/5; 307/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 307/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.907 total time=  15.4s\n",
      "[CV 1/5; 308/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 308/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.850 total time=  14.4s\n",
      "[CV 5/5; 308/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 308/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.911 total time=  14.4s\n",
      "[CV 1/5; 310/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 310/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.875 total time=   5.4s\n",
      "[CV 5/5; 310/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 310/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.907 total time=   5.4s\n",
      "[CV 4/5; 311/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 311/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.857 total time=   6.0s\n",
      "[CV 3/5; 312/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 312/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.882 total time=   6.0s\n",
      "[CV 2/5; 313/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 313/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.914 total time=   6.0s\n",
      "[CV 1/5; 314/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 314/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.857 total time=   6.1s\n",
      "[CV 5/5; 314/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 314/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.907 total time=   6.1s\n",
      "[CV 4/5; 315/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 315/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.843 total time=   5.9s\n",
      "[CV 3/5; 316/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 316/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.886 total time=  12.7s\n",
      "[CV 2/5; 317/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 317/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.904 total time=  13.0s\n",
      "[CV 1/5; 318/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 1/5; 318/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.850 total time=  13.1s\n",
      "[CV 5/5; 318/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 318/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.896 total time=  13.3s\n",
      "[CV 4/5; 319/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 4/5; 319/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.846 total time=  13.1s\n",
      "[CV 3/5; 320/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 3/5; 320/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.886 total time=  13.3s\n",
      "[CV 2/5; 321/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 2/5; 321/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.914 total time=  13.8s\n",
      "[CV 1/5; 322/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 1/5; 322/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.875 total time=  12.6s\n",
      "[CV 5/5; 322/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 5/5; 322/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.904 total time=  12.6s\n",
      "[CV 5/5; 323/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 5/5; 323/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.914 total time=   5.1s\n",
      "[CV 4/5; 324/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 4/5; 324/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.850 total time=   5.5s\n",
      "[CV 3/5; 325/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 3/5; 325/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.882 total time=   5.8s\n",
      "[CV 2/5; 326/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 2/5; 326/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.907 total time=   5.9s\n",
      "[CV 1/5; 327/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 1/5; 327/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.879 total time=   6.0s\n",
      "[CV 5/5; 327/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 5/5; 327/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.904 total time=   6.0s\n",
      "[CV 4/5; 328/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 4/5; 328/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.850 total time=   6.1s\n",
      "[CV 3/5; 329/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 3/5; 329/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.886 total time=   5.8s\n",
      "[CV 2/5; 330/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 2/5; 330/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.914 total time=  12.7s\n",
      "[CV 1/5; 331/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 1/5; 331/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.868 total time=  12.8s\n",
      "[CV 5/5; 331/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 5/5; 331/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.900 total time=  12.8s\n",
      "[CV 4/5; 332/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 4/5; 332/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.857 total time=  13.2s\n",
      "[CV 3/5; 333/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 3/5; 333/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.889 total time=  14.0s\n",
      "[CV 2/5; 334/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 2/5; 334/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.918 total time=  13.7s\n",
      "[CV 1/5; 335/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 1/5; 335/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.868 total time=  13.6s\n",
      "[CV 5/5; 335/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 5/5; 335/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.904 total time=  13.6s\n",
      "[CV 4/5; 336/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 4/5; 336/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.857 total time=  12.8s\n",
      "[CV 4/5; 337/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 337/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.854 total time=   5.2s\n",
      "[CV 3/5; 338/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 338/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.879 total time=   5.4s\n",
      "[CV 2/5; 339/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 339/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.914 total time=   5.9s\n",
      "[CV 1/5; 340/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 340/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.868 total time=   6.0s\n",
      "[CV 5/5; 340/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 340/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   5.9s\n",
      "[CV 4/5; 341/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 341/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.861 total time=   6.0s\n",
      "[CV 3/5; 342/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 342/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.879 total time=   6.1s\n",
      "[CV 2/5; 343/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 343/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.907 total time=   5.9s\n",
      "[CV 1/5; 344/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 344/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.864 total time=  11.4s\n",
      "[CV 5/5; 344/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 344/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.900 total time=  12.2s\n",
      "[CV 4/5; 345/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 345/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.857 total time=  11.6s\n",
      "[CV 3/5; 346/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 5/5; 302/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.907 total time=  14.1s\n",
      "[CV 4/5; 303/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 303/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.854 total time=  14.4s\n",
      "[CV 3/5; 304/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 304/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.889 total time=  15.1s\n",
      "[CV 2/5; 305/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 305/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.914 total time=  15.3s\n",
      "[CV 1/5; 306/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 306/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.850 total time=  15.1s\n",
      "[CV 5/5; 306/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 306/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.907 total time=  15.2s\n",
      "[CV 4/5; 307/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 307/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.846 total time=  15.3s\n",
      "[CV 3/5; 308/392] START criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 308/392] END criterion=squared_error, learning_rate=0.075, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.871 total time=  14.8s\n",
      "[CV 4/5; 309/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 4/5; 309/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.846 total time=   5.1s\n",
      "[CV 3/5; 310/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 3/5; 310/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.889 total time=   5.4s\n",
      "[CV 2/5; 311/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 2/5; 311/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.914 total time=   6.0s\n",
      "[CV 1/5; 312/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 1/5; 312/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.864 total time=   6.0s\n",
      "[CV 5/5; 312/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 5/5; 312/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.911 total time=   5.9s\n",
      "[CV 4/5; 313/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 4/5; 313/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.846 total time=   6.0s\n",
      "[CV 3/5; 314/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 3/5; 314/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.882 total time=   6.1s\n",
      "[CV 2/5; 315/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 2/5; 315/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.921 total time=   5.9s\n",
      "[CV 1/5; 316/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 1/5; 316/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.861 total time=  12.6s\n",
      "[CV 5/5; 316/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 5/5; 316/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.893 total time=  12.7s\n",
      "[CV 4/5; 317/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 4/5; 317/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.871 total time=  12.9s\n",
      "[CV 3/5; 318/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n",
      "[CV 3/5; 318/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8;, score=0.882 total time=  13.4s\n",
      "[CV 2/5; 319/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85\n",
      "[CV 2/5; 319/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.85;, score=0.914 total time=  13.5s\n",
      "[CV 1/5; 320/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 1/5; 320/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.875 total time=  13.3s\n",
      "[CV 5/5; 320/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9\n",
      "[CV 5/5; 320/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.9;, score=0.900 total time=  13.4s\n",
      "[CV 4/5; 321/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95\n",
      "[CV 4/5; 321/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.95;, score=0.850 total time=  13.8s\n",
      "[CV 3/5; 322/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0\n",
      "[CV 3/5; 322/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=log2, n_estimators=3000, subsample=1.0;, score=0.879 total time=  12.8s\n",
      "[CV 3/5; 323/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5\n",
      "[CV 3/5; 323/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.5;, score=0.886 total time=   5.2s\n",
      "[CV 2/5; 324/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618\n",
      "[CV 2/5; 324/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.618;, score=0.907 total time=   5.5s\n",
      "[CV 1/5; 325/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 1/5; 325/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.864 total time=   5.8s\n",
      "[CV 5/5; 325/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8\n",
      "[CV 5/5; 325/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.8;, score=0.907 total time=   5.9s\n",
      "[CV 4/5; 326/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85\n",
      "[CV 4/5; 326/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.85;, score=0.861 total time=   5.9s\n",
      "[CV 3/5; 327/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9\n",
      "[CV 3/5; 327/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.9;, score=0.886 total time=   6.0s\n",
      "[CV 2/5; 328/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95\n",
      "[CV 2/5; 328/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=0.95;, score=0.911 total time=   6.1s\n",
      "[CV 1/5; 329/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 1/5; 329/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.861 total time=   5.9s\n",
      "[CV 5/5; 329/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0\n",
      "[CV 5/5; 329/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=1000, subsample=1.0;, score=0.911 total time=   5.9s\n",
      "[CV 4/5; 330/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5\n",
      "[CV 4/5; 330/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.5;, score=0.868 total time=  12.7s\n",
      "[CV 3/5; 331/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618\n",
      "[CV 3/5; 331/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.618;, score=0.886 total time=  13.0s\n",
      "[CV 2/5; 332/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8\n",
      "[CV 2/5; 332/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.8;, score=0.914 total time=  13.4s\n",
      "[CV 1/5; 333/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 1/5; 333/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.861 total time=  13.3s\n",
      "[CV 5/5; 333/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85\n",
      "[CV 5/5; 333/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.85;, score=0.911 total time=  14.2s\n",
      "[CV 4/5; 334/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9\n",
      "[CV 4/5; 334/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.9;, score=0.861 total time=  13.6s\n",
      "[CV 3/5; 335/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95\n",
      "[CV 3/5; 335/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=0.95;, score=0.886 total time=  13.7s\n",
      "[CV 2/5; 336/392] START criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0\n",
      "[CV 2/5; 336/392] END criterion=squared_error, learning_rate=0.1, loss=deviance, max_features=sqrt, n_estimators=3000, subsample=1.0;, score=0.911 total time=  13.0s\n",
      "[CV 1/5; 337/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 1/5; 337/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.864 total time=   5.2s\n",
      "[CV 2/5; 337/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5\n",
      "[CV 2/5; 337/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.5;, score=0.911 total time=   5.2s\n",
      "[CV 1/5; 338/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 1/5; 338/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.868 total time=   5.5s\n",
      "[CV 5/5; 338/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618\n",
      "[CV 5/5; 338/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.618;, score=0.911 total time=   5.5s\n",
      "[CV 4/5; 339/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8\n",
      "[CV 4/5; 339/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.8;, score=0.861 total time=   5.9s\n",
      "[CV 3/5; 340/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85\n",
      "[CV 3/5; 340/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.85;, score=0.882 total time=   6.0s\n",
      "[CV 2/5; 341/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9\n",
      "[CV 2/5; 341/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.9;, score=0.911 total time=   6.1s\n",
      "[CV 1/5; 342/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 1/5; 342/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.861 total time=   6.2s\n",
      "[CV 5/5; 342/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95\n",
      "[CV 5/5; 342/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=0.95;, score=0.904 total time=   6.2s\n",
      "[CV 4/5; 343/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0\n",
      "[CV 4/5; 343/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=1000, subsample=1.0;, score=0.854 total time=   6.0s\n",
      "[CV 3/5; 344/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5\n",
      "[CV 3/5; 344/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.5;, score=0.882 total time=  11.6s\n",
      "[CV 2/5; 345/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618\n",
      "[CV 2/5; 345/392] END criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.618;, score=0.907 total time=  12.6s\n",
      "[CV 1/5; 346/392] START criterion=squared_error, learning_rate=0.15, loss=deviance, max_features=log2, n_estimators=3000, subsample=0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=GradientBoostingClassifier(), n_jobs=4,\n",
       "             param_grid={'criterion': ['friedman_mse', 'squared_error'],\n",
       "                         'learning_rate': [0.01, 0.025, 0.05, 0.075, 0.1, 0.15,\n",
       "                                           0.2],\n",
       "                         'loss': ['deviance'], 'max_features': ['log2', 'sqrt'],\n",
       "                         'n_estimators': [1000, 3000],\n",
       "                         'subsample': [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0]},\n",
       "             verbose=100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the grid search we want to run. Run it with four cpus in parallel.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gs_cv = GridSearchCV(model2, param_grid, n_jobs=4, verbose=100)\n",
    "\n",
    "# Run the grid search - on only the training data!\n",
    "gs_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2985b96c-8d49-44aa-8a63-de7b730f924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'squared_error', 'learning_rate': 0.05, 'loss': 'deviance', 'max_features': 'log2', 'n_estimators': 1000, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Print the parameters that gave us the best result!\n",
    "print(gs_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dea63da-1576-4bd1-9569-aac8f0f817e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "# creating a gradient boosting classifier with best params\n",
    "GT_BR = ensemble.GradientBoostingClassifier(\n",
    "        criterion = 'squared_error',\n",
    "        learning_rate = 0.05,\n",
    "        loss = 'deviance',\n",
    "        max_features = 'log2',\n",
    "        n_estimators = 1000,\n",
    "        subsample = 0.5,\n",
    ")\n",
    "# fit\n",
    "gt_br_model = GT_BR.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39bc6e49-4b57-47e9-9ed5-1862cde0f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_br_predict = gt_br_model.predict(X_validation)\n",
    "#find accuracy score of the hypertuned model\n",
    "accuracy_GT2 = accuracy_score(Y_validation,gt_br_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbc25c2c-49e5-4451-aac7-b8b31b515404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy done by gradient boosting: 0.8983333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy done by gradient boosting:\",accuracy_GT2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2dbcf82-d7cf-409c-a53b-37bb9a95be39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/trained_mobile_gradientBoosting.pkl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(gt_br_model,'models/trained_mobile_gradientBoosting.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc824f-1eda-415f-8963-7f4076cd9f22",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d37d9-719c-46c7-9ae4-3ee9f3df1665",
   "metadata": {},
   "source": [
    "#### Building Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31fc5b7b-f8a5-4ed9-95e5-eb9a518bc2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the svm model:  95.0\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm_model = svm.fit(X_train,Y_train)\n",
    "svm_predict = svm_model.predict(X_validation)\n",
    "accuracy_svm = accuracy_score(Y_validation,svm_predict)\n",
    "print(\"Accuracy of the svm model: \", accuracy_svm * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af839c3-938d-470e-9f8a-675ba6018ea2",
   "metadata": {},
   "source": [
    "#### HyperTuning Support Vector Machine Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "070e0075-20f4-4b66-9b1f-6c69c41fe7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search method \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'gamma': [.1,.5,.25,1],\n",
    "             'kernel':['rbf','poly','sigmoid','linear']}\n",
    "GS = GridSearchCV(SVC(),param_grid,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da23cac8-4a1c-42dc-aeaf-5e27b58913c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the parameters {'gamma': 0.1, 'kernel': 'linear'} are the best.\n",
      "the best score is 0.97.\n"
     ]
    }
   ],
   "source": [
    "model_svm = GS.fit(X_train,Y_train)\n",
    "print(\"the parameters {} are the best.\".format(GS.best_params_))\n",
    "print(\"the best score is {:.2f}.\".format(GS.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddbada27-9106-46ca-8a24-65da98f7b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(\n",
    "    gamma = 0.1,\n",
    "    kernel = 'linear'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f817534-6074-43d4-9b2d-47995c506d14",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_svm2 = SVM.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79ae13cb-3e80-406c-84a7-19b292551d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pred2  = model_svm2.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7aad9b8a-e3ad-4676-9eec-e3413605665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       162\n",
      "           1       0.98      0.97      0.98       144\n",
      "           2       0.97      0.96      0.96       154\n",
      "           3       0.97      0.98      0.98       140\n",
      "\n",
      "    accuracy                           0.98       600\n",
      "   macro avg       0.98      0.98      0.98       600\n",
      "weighted avg       0.98      0.98      0.98       600\n",
      "\n",
      "0.9766666666666667\n"
     ]
    }
   ],
   "source": [
    "# Importing classification report and confusion matrix from sklearn metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Printing classification report\n",
    "print(classification_report(Y_validation, svm_pred2))\n",
    "\n",
    "# Printing accuracy\n",
    "print(accuracy_score(Y_validation,svm_pred2))\n",
    "accuracy_svm = accuracy_score(Y_validation,svm_pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3955dd6-6afc-4335-b7fc-46c5f783dc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuravy of the updated svm model:  97.66666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuravy of the updated svm model: \", accuracy_svm * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a88acb8-175f-4756-a3bb-46adaf76dfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix=confusion_matrix(Y_validation,svm_pred2)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(matrix,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6c447d1-fcc5-4b14-be1c-4eaeb5de91a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/trained_mobile_svm_updated_final.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_svm2,'models/trained_mobile_svm_updated_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb0857-cf3f-4fa2-a6d9-d3b7d80d7988",
   "metadata": {},
   "source": [
    "### 5. Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e8369-2d17-4de0-a6c7-7f71bea3005f",
   "metadata": {},
   "source": [
    "#### Building Default Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e84fc9b-7cee-4429-ac6c-201a846b760f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Model: 0.8783333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "MODEL2 = rf.fit(X_train,Y_train)\n",
    "rf_predict = MODEL2.predict(X_validation)\n",
    "accuracy_rf = accuracy_score(Y_validation,rf_predict)\n",
    "print(\"Accuracy of Random Forest Model:\",accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2ab59-6911-426d-95e7-823e82e4dc18",
   "metadata": {},
   "source": [
    "#### HyperTuning Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c86951f-e6bb-4658-a5aa-89335efae97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "params = {'criterion':['gini','entropy'],\n",
    "         'n_estimators':[100,300,500,800,1200],\n",
    "          'min_samples_leaf':[1,2,3],\n",
    "          'min_samples_split':[3,4,5,6,7],\n",
    "          'random_state':[123]}\n",
    "rf_gridS = GridSearchCV(model,param_grid=params,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f079bcb-bfca-43f9-a7e6-8e068348518a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'min_samples_split': [3, 4, 5, 6, 7],\n",
       "                         'n_estimators': [100, 300, 500, 800, 1200],\n",
       "                         'random_state': [123]})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gridS.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e51d2c3-0f5d-4cff-b400-73821a4e8e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 1200, 'random_state': 123}\n"
     ]
    }
   ],
   "source": [
    "# Print the parameters that gave us the best result!\n",
    "print(rf_gridS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0da9d91-9931-4187-8631-28a9efd5d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model = RandomForestClassifier(\n",
    "    criterion = \"entropy\",\n",
    "    min_samples_leaf = 1,\n",
    "    min_samples_split = 6,\n",
    "    n_estimators = 1200,\n",
    "    random_state = 123,\n",
    ")\n",
    "updated_rf_model = updated_model.fit(X_train,Y_train)\n",
    "rf_predict = updated_rf_model.predict(X_validation)\n",
    "accuracy_rf2 = accuracy_score(Y_validation,rf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e98b0731-203c-4e77-bddd-09b2dc0b1773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Model: 0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Random Forest Model:\",accuracy_rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8241f-92b5-4a82-aa11-d5f9b318f346",
   "metadata": {},
   "source": [
    "### 6. Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb6cf3-0a25-4339-81e9-fd98948421d4",
   "metadata": {},
   "source": [
    "#### Building Default Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a816433a-51e9-41ef-bc65-787dba99a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuravy of the decision tree classifier model:  82.33333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/trained_mobile_decisionTreeClassifier.pkl']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib\n",
    "dt_classfier = DecisionTreeClassifier()\n",
    "model_dt = dt_classfier.fit(X_train,Y_train)\n",
    "dt_classi_predict = model_dt.predict(X_validation)\n",
    "accuravy_DT = accuracy_score(Y_validation,dt_classi_predict)\n",
    "print(\"Accuravy of the decision tree classifier model: \", accuravy_DT * 100)\n",
    "joblib.dump(model_dt,'models/trained_mobile_decisionTreeClassifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe3e68-9125-4bca-83a4-2b96c72b4bcf",
   "metadata": {},
   "source": [
    "#### Hypertuning Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c167921-96f1-4d39-a25b-4ac415fa6b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "touch_screen - 0.00%\n",
      "m_dep - 0.35%\n",
      "sc_h - 0.40%\n",
      "fc - 0.58%\n",
      "mobile_wt - 0.82%\n",
      "sc_w - 0.85%\n",
      "talk_time - 0.91%\n",
      "n_cores - 0.97%\n",
      "pc - 1.29%\n",
      "int_memory - 1.67%\n",
      "px_width - 6.66%\n",
      "px_height - 10.37%\n",
      "battery_power - 14.20%\n",
      "ram - 60.93%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "feature_labels = np.array(['battery_power','fc','int_memory','m_dep','mobile_wt','n_cores','pc','px_height','px_width','ram','sc_h','sc_w','talk_time','touch_screen'])\n",
    "model = joblib.load('models/trained_mobile_decisionTreeClassifier.pkl')\n",
    "#create a numpy array based on the models feature importance\n",
    "importance = model.feature_importances_\n",
    "feature_indexes_by_importance = importance.argsort()\n",
    "\n",
    "for index in feature_indexes_by_importance:\n",
    "    print(\"{} - {:.2f}%\".format(feature_labels[index],(importance[index] * 100.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "16bcd42e-f21a-480e-8484-ba4238ee547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataTable_dt = data_table.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4fb46ee-bb4a-492b-901f-795412f1608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>fc</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>touch_screen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>794</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>106</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>1222</td>\n",
       "      <td>1890</td>\n",
       "      <td>668</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>915</td>\n",
       "      <td>1965</td>\n",
       "      <td>2032</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1911</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.7</td>\n",
       "      <td>108</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>868</td>\n",
       "      <td>1632</td>\n",
       "      <td>3057</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1512</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>0.1</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>336</td>\n",
       "      <td>670</td>\n",
       "      <td>869</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>510</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0.9</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>483</td>\n",
       "      <td>754</td>\n",
       "      <td>3919</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  fc  int_memory  m_dep  mobile_wt  n_cores  pc  px_height  \\\n",
       "0               842   1           7    0.6        188        2   2         20   \n",
       "1              1021   0          53    0.7        136        3   6        905   \n",
       "2               563   2          41    0.9        145        5   6       1263   \n",
       "3               615   0          10    0.8        131        6   9       1216   \n",
       "4              1821  13          44    0.6        141        2  14       1208   \n",
       "...             ...  ..         ...    ...        ...      ...  ..        ...   \n",
       "1995            794   0           2    0.8        106        6  14       1222   \n",
       "1996           1965   0          39    0.2        187        4   3        915   \n",
       "1997           1911   1          36    0.7        108        8   3        868   \n",
       "1998           1512   4          46    0.1        145        5   5        336   \n",
       "1999            510   5          45    0.9        168        6  16        483   \n",
       "\n",
       "      px_width   ram  sc_h  sc_w  talk_time  touch_screen  \n",
       "0          756  2549     9     7         19             0  \n",
       "1         1988  2631    17     3          7             1  \n",
       "2         1716  2603    11     2          9             1  \n",
       "3         1786  2769    16     8         11             0  \n",
       "4         1212  1411     8     2         15             1  \n",
       "...        ...   ...   ...   ...        ...           ...  \n",
       "1995      1890   668    13     4         19             1  \n",
       "1996      1965  2032    11    10         16             1  \n",
       "1997      1632  3057     9     1          5             1  \n",
       "1998       670   869    18    10         19             1  \n",
       "1999       754  3919    19     4          2             1  \n",
       "\n",
       "[2000 rows x 14 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a33f4726-58ff-4c4b-825f-d51c5607a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_dataTable_dt['touch_screen']\n",
    "del new_dataTable_dt['m_dep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfc83fa7-9ee4-4389-9c52-3b57da60bf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>fc</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>794</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>1222</td>\n",
       "      <td>1890</td>\n",
       "      <td>668</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>915</td>\n",
       "      <td>1965</td>\n",
       "      <td>2032</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1911</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>108</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>868</td>\n",
       "      <td>1632</td>\n",
       "      <td>3057</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1512</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>336</td>\n",
       "      <td>670</td>\n",
       "      <td>869</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>510</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>483</td>\n",
       "      <td>754</td>\n",
       "      <td>3919</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  fc  int_memory  mobile_wt  n_cores  pc  px_height  \\\n",
       "0               842   1           7        188        2   2         20   \n",
       "1              1021   0          53        136        3   6        905   \n",
       "2               563   2          41        145        5   6       1263   \n",
       "3               615   0          10        131        6   9       1216   \n",
       "4              1821  13          44        141        2  14       1208   \n",
       "...             ...  ..         ...        ...      ...  ..        ...   \n",
       "1995            794   0           2        106        6  14       1222   \n",
       "1996           1965   0          39        187        4   3        915   \n",
       "1997           1911   1          36        108        8   3        868   \n",
       "1998           1512   4          46        145        5   5        336   \n",
       "1999            510   5          45        168        6  16        483   \n",
       "\n",
       "      px_width   ram  sc_h  sc_w  talk_time  \n",
       "0          756  2549     9     7         19  \n",
       "1         1988  2631    17     3          7  \n",
       "2         1716  2603    11     2          9  \n",
       "3         1786  2769    16     8         11  \n",
       "4         1212  1411     8     2         15  \n",
       "...        ...   ...   ...   ...        ...  \n",
       "1995      1890   668    13     4         19  \n",
       "1996      1965  2032    11    10         16  \n",
       "1997      1632  3057     9     1          5  \n",
       "1998       670   869    18    10         19  \n",
       "1999       754  3919    19     4          2  \n",
       "\n",
       "[2000 rows x 12 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataTable_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e41089db-842c-429a-987a-4626a75f4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_dataTable_dt.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6673d283-9c51-47ce-8174-c73611c64d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "validation_size = 0.30\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0eb49b67-932e-4352-9e97-403c182c09fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuravy of the decision tree classifier model:  82.33333333333334\n",
      "DIFFERENCE of the decision tree classifier model:  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/trained_mobile_decisionTreeClassifier.pkl']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classfier2 = DecisionTreeClassifier()\n",
    "model_dt2 = dt_classfier2.fit(X_train,Y_train)\n",
    "dt_classi_predict2 = model_dt2.predict(X_validation)\n",
    "accuravy_DT2 = accuracy_score(Y_validation,dt_classi_predict)\n",
    "print(\"Accuravy of the decision tree classifier model: \", accuravy_DT2 * 100)\n",
    "\n",
    "Accuracy_changes_dt = accuravy_DT2 - accuravy_DT\n",
    "print(\"DIFFERENCE of the decision tree classifier model: \",Accuracy_changes_dt *100)\n",
    "joblib.dump(model_dt2,'models/trained_mobile_decisionTreeClassifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a18a0226-1d2c-44e6-89b7-7a7bf4395c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypertuning model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tree_para = {'criterion':['gini','entropy'],\n",
    "             'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n",
    "             'splitter' :['best', 'random'],\n",
    "             \n",
    "            }\n",
    "clf = GridSearchCV(dt_classfier2, tree_para, cv=5)\n",
    "model_dt3 = clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76d329d7-bb11-4838-97c5-fc746d34790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 11, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3cb0a6fc-b33e-4a88-999b-6ab217020c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classfier2 = DecisionTreeClassifier(\n",
    "     criterion = 'entropy',\n",
    "     max_depth = 11,\n",
    "    splitter = 'best'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a1784f0-3e98-4e92-8a5a-4bf5aab0e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dt_model = dt_classfier2.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c2428b6-4d42-48df-ab44-fc7411b45aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuravy of the decision tree classifier model:  85.5\n"
     ]
    }
   ],
   "source": [
    "dt_classi_predict2 = updated_dt_model.predict(X_validation)\n",
    "accuravy_DT2 = accuracy_score(Y_validation,dt_classi_predict2)\n",
    "print(\"Accuravy of the decision tree classifier model: \", accuravy_DT2 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbbcd2-c73c-4ab3-9f92-df276e86f6f2",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkblue\"> Choosing the best model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2a97ddb-6fe9-4b97-9b38-a6eedc1aa98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_LR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b71991a-487a-460c-9b6e-60d23127b13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuravy_DT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf2227b0-cfe8-4f21-8d28-91926e2f2def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_KN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ab3046c-6ab6-4f60-bffc-fa72bbfe16c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766666666666667"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "827ac140-b26e-456e-b284-e46343fb3e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8983333333333333"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_GT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "59582f74-455f-476e-8ef4-b5ff1805c7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ea4af9a-aaa7-465c-b9e9-b33fc1f17f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_table.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cce77216-ae8d-44ef-9596-3dfe36120aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "validation_size = 0.30\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0813f487-9309-4109-94e0-0262c73d1882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hafsah/opt/anaconda3/envs/ai/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of  LogisticR : 96.0\n",
      "Accuracy of  KNN : 92.5\n",
      "Accuracy of  SVM : 97.66666666666667\n",
      "Accuracy of  GTB : 89.5\n",
      "Accuracy of  RF : 88.0\n",
      "Accuracy of  DTC : 85.5\n",
      "There are 6 models we have tried out!! \n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "up_models = []\n",
    "up_models.append(('LogisticR', LogisticRegression(solver='newton-cg')))\n",
    "up_models.append(('KNN', KNeighborsClassifier(leaf_size=1, n_neighbors=16, weights='distance')))\n",
    "up_models.append(('SVM', SVC(gamma = 0.1,kernel = 'linear')))\n",
    "up_models.append(('GTB', ensemble.GradientBoostingClassifier(criterion = 'squared_error',learning_rate = 0.15,loss = 'deviance',max_features = 'sqrt',n_estimators = 3000,subsample = 0.5)))\n",
    "up_models.append(('RF', RandomForestClassifier(criterion = \"entropy\",min_samples_leaf = 1,min_samples_split = 6,n_estimators = 1200,random_state = 123,)))\n",
    "up_models.append(('DTC', DecisionTreeClassifier(criterion = 'entropy',max_depth = 11,splitter = 'best')))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in up_models:\n",
    "    model.fit(X_train,Y_train)\n",
    "    pred = model.predict(X_validation)\n",
    "    accuracy_model = accuracy_score(Y_validation,pred)\n",
    "    print(\"Accuracy of \" ,name, \":\", accuracy_model * 100)\n",
    "print(\"There are\" , len(models) , \"models we have tried out!! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f0b01-9f84-441f-b9f2-fd00f1f11322",
   "metadata": {},
   "source": [
    "#### Create a graph to image out the accuracy of all models and then choose the best accurate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d852416-690a-4412-9b89-8f6d6906380c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ['lr', 'dt', 'knn','svm','gt','rf']\n",
    "acc_scores = [accuracy_LR2, accuravy_DT2, accuracy_KN2,accuracy_svm,accuracy_GT2,accuracy_rf2]\n",
    "\n",
    "plt.bar(models, acc_scores, color=['lightgrey', 'pink', 'green', 'lightblue','black','orange'])\n",
    "plt.ylabel(\"accuracy scores\")\n",
    "plt.title(\"Which model is the most accurate?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ae313-8703-4196-ae4f-824700ea40e5",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\"> Conclusion : Best model is *Support Vector Machine*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2766eb-f235-40a7-87d8-c91a2a048c23",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df11bda-3224-4d4f-bfdb-b24d7df4be9a",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkblue\"> Save Model & Deploy on Web Application </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a529c9e5-051c-47fb-b970-45c7c98cde31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/trained_mobile_svm_updated_final.pkl']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAVE MODEL\n",
    "joblib.dump(model_svm2,'models/trained_mobile_svm_updated_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0b8884fe-754f-408d-8b10-4dfe69177a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>fc</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>touch_screen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>794</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>106</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>1222</td>\n",
       "      <td>1890</td>\n",
       "      <td>668</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>915</td>\n",
       "      <td>1965</td>\n",
       "      <td>2032</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1911</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.7</td>\n",
       "      <td>108</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>868</td>\n",
       "      <td>1632</td>\n",
       "      <td>3057</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1512</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>0.1</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>336</td>\n",
       "      <td>670</td>\n",
       "      <td>869</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>510</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0.9</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>483</td>\n",
       "      <td>754</td>\n",
       "      <td>3919</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  fc  int_memory  m_dep  mobile_wt  n_cores  pc  px_height  \\\n",
       "0               842   1           7    0.6        188        2   2         20   \n",
       "1              1021   0          53    0.7        136        3   6        905   \n",
       "2               563   2          41    0.9        145        5   6       1263   \n",
       "3               615   0          10    0.8        131        6   9       1216   \n",
       "4              1821  13          44    0.6        141        2  14       1208   \n",
       "...             ...  ..         ...    ...        ...      ...  ..        ...   \n",
       "1995            794   0           2    0.8        106        6  14       1222   \n",
       "1996           1965   0          39    0.2        187        4   3        915   \n",
       "1997           1911   1          36    0.7        108        8   3        868   \n",
       "1998           1512   4          46    0.1        145        5   5        336   \n",
       "1999            510   5          45    0.9        168        6  16        483   \n",
       "\n",
       "      px_width   ram  sc_h  sc_w  talk_time  touch_screen  \n",
       "0          756  2549     9     7         19             0  \n",
       "1         1988  2631    17     3          7             1  \n",
       "2         1716  2603    11     2          9             1  \n",
       "3         1786  2769    16     8         11             0  \n",
       "4         1212  1411     8     2         15             1  \n",
       "...        ...   ...   ...   ...        ...           ...  \n",
       "1995      1890   668    13     4         19             1  \n",
       "1996      1965  2032    11    10         16             1  \n",
       "1997      1632  3057     9     1          5             1  \n",
       "1998       670   869    18    10         19             1  \n",
       "1999       754  3919    19     4          2             1  \n",
       "\n",
       "[2000 rows x 14 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a8217f2e-2479-4156-a64e-38dcce3eec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "##done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be54d8-be21-4f3c-884a-98a200d8089d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
